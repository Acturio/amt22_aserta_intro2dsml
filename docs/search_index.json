[["index.html", "Ciencia de Datos y Machine Learning BIENVENIDA Objetivo Alcances del curso Instructores Temario Duración y evaluación del curso Recursos y dinámica de clase", " Ciencia de Datos y Machine Learning BIENVENIDA Objetivo Capacitar y brindar acompañamiento al equipo de analítica de ASERTA en temas relacionados con Ciencia de Datos para la correcta implementación de proyectos y toma de decisiones basadas en la evidencia de datos internos y externos a la empresa para lograr un beneficio operativo y económico. Alcances del curso El participante conocerá los conceptos teóricos alrededor de esta ciencia y sabrá implementar correctamente un análisis exploratorio estadístico y gráfico que le permita conocer a mayor profundidad los datos a usar. Conocerá y sabrá implementar los modelos predictivos de Machine Learning más usados y de mayor impacto en la industria de seguros y fianzas. Finalmente, sabrá tomar decisiones sobre el correcto uso e implementación de los modelos para aumentar el beneficio comercial dentro de la institución. Instructores ACT. ARTURO BRINGAS LinkedIn: arturo-bringas Email: act.arturo.b@ciencias.unam.mx Actuario egresado de la Facultad de Ciencias con maestría en Ciencia de Datos por el ITAM. Se especializa en modelos predictivos y de clasificación de machine learning aplicado a seguros, marketing, deportes, e-commerce y movilidad internacional. Ha sido consultor Senior Data Scientist para empresas y organizaciones como GNP, El Universal, UNAM, la Organización de las Naciones Unidas Contra la Droga y el Delito (UNODC), Sinnia, entre otros. Actualmente es profesor de Ciencia de datos y Machine Learning en AMAT. Es colaborador del Laboratorio Nacional de Observación de la Tierra (LANOT) y jefe de estadística en el Departamento de Investigación Aplicada y Opinión de la UNAM, donde realiza estudios nacionales de impacto social. ACT. KARINA LIZETTE GAMBOA LinkedIn: KaLizzyGam Email: lizzygamboa@ciencias.unam.mx Actuaria egresada de la Facultad de Ciencias y candidata a Maestra en Ciencia de Datos por el ITAM. Experiencia en áreas de analítica predictiva e inteligencia del negocio. Lead y Senior Data Scientist en consultoría en diferentes sectores como tecnología, asegurador, financiero y bancario. Es experta en entendimiento de negocio para la correcta implementación de algoritmos de inteligencia y explotación de datos. Actualmente se desarrolla como Arquitecta de Soluciones Analíticas en Merama, startup mexicana clasificada como uno de los nuevos unicornios de Latinoamérica. Senior Data Science en CLOSTER y como profesora del diplomado de Metodología de la Investigación Social por la UNAM así como instructora de cursos de Ciencia de Datos en AMAT. Empresas anteriores: GNP, Actinver Banco y Casa de Bolsa, PlayCity Casinos, RakenDataGroup Consulting, entre otros. Temario Módulo 1: Introducción a R (22 hrs) Objetivo: A través de este módulo se adquirirán los conocimientos necesarios para la operación del software estadístico y la manipulación ágil de datos. Al finalizar, el participante desarrollará análisis exploratorios y reportes automatizados. Estructuras de almacenamiento de datos Almacenamiento Vectores Matrices Listas DataFrames Funciones y estructuras de control Librerías y funciones Operaciones vectoriales Condicionamiento Ciclos Guía de estilo Manipulación de estructuras de datos Importación de tablas Consultas y transformación de estructuras Iteraciones Manipulación de texto y datos temporales Análisis exploratorio y visualización de datos Guía de visualización Análisis Exploratorio de Datos (EDA) Análisis Gráfico Exploratorio de Datos (GEDA) Reportes con markdown Consultoría y aplicaciones con datos institucionales Módulo 2: Introducción a Ciencia de Datos (18 hrs) Objetivo: Este módulo presenta los conceptos teóricos clave para conocer los términos, objetivo y alcances de proyectos con enfoque en ciencia de datos. Se presenta el flujo de trabajo y organización que deberá seguir un equipo para obtener el mayor beneficio posible. Adicionalmente, se propone presentar el software git y github para implementar correctamente el trabajo en equipo que garantice la reproducibilidad y seguridad del desarrollo realizado. Introducción a ciencia de datos ¿Qué es la ciencia de datos? Objetivo de ciencia de datos Requisitos y aplicaciones Tipos de algoritmos Perfiles de un equipo de ciencia de datos Ciclo de vida de un proyecto Taller de scoping Concepto de Ciencia de Datos Machine learning Análisis supervisado Análisis no supervisado Sesgo y varianza Pre-procesamiento e ingeniería de datos Partición de datos Colaboración y reproducibilidad Git &amp; Github Ambiente de desarrollo Consultoría y aplicaciones con datos institucionales Módulo 3: Machine Learning: Supervisado (38 hrs) Objetivo: Este módulo está diseñado para adquirir los conocimientos técnicos para conocer e implementar los distintos modelos de aprendizaje supervisado que son aplicados en ciencia de datos a la industria de los seguros y fianzas. Modelos de aprendizaje Supervisado Regresión Lineal Regresión logística Regularización Ridge &amp; Lasso Elasticnet KNN Árbol de decisión Bagging Random Forest Boosting Stacking Toma de decisiones enfocadas a negocio Comparación de modelos Balance entre precisión y cobertura Cuantificación de sesgo e inequidad Cuantificación de ganancia comercial Diseño experimental Consultoría y aplicaciones con datos institucionales Módulo 4: Machine Learning: No Supervisado (12 hrs) Objetivo: Este módulo permite al participante conocer técnicas de clustering para clasificar clientes de acuerdo con la utilidad y riesgo para la empresa. Adicionalmente, se presentan aplicaciones de clustering enfocadas a la estratificación de acuerdo con el riesgo geográfico. Técnicas de reducción de dimensión Análisis de componentes principales Creación de índices Clustering Liga simple, compleja y promedio Dendogramas &amp; heatmaps Kmeans &amp; Kmedoids DBSCAN Consultoría y aplicaciones con datos institucionales Requisitos Computadora con al menos 4Gb Ram. Instalación de R con versión &gt;= 4.1.0 Instalación de Rstudio con versión &gt;= 1.4.17 Conocimientos generales de probabilidad, estadística y álgebra lineal Duración y evaluación del curso El programa tiene una duración de 90 hrs. Las clases serán impartidas los días lunes a viernes, de 7:30 am a 9:30 am Serán asignados ejercicios que el participante deberá resolver entre una semana y otra. Al final del curso se solicitará un proyecto final, el cual deberá ser entregado para ser acreedor a la constancia de participación. “La gota abre la piedra, no por su fuerza sino por su constancia” - Ovidio Recursos y dinámica de clase En esta clase estaremos usando: R (descargar) RStudio (descargar) Zoom Clases Pulgar arriba: Voy bien, estoy entendiendo! Pulgar abajo: Eso no quedó muy claro Mano arriba: Quiero participar/preguntar o Ya estoy listo para iniciar One Drive Notas de clase Asistencias y rescates vía gpo whatsapp: https://chat.whatsapp.com/InAbwYRJ7njGbtz9V2xxrN FINALMENTE… se dejarán ejercicios que serán clave para el éxito del aprendizaje de los capítulos, por lo que se trabajará en equipo para lograr adquirir el mayor aprendizaje. "],["conceptos-de-ciencia-de-datos.html", "Capítulo 1 Conceptos de Ciencia de Datos 1.1 ¿Qué es Ciencia de Datos? 1.2 Objetivos 1.3 Requisitos 1.4 Aplicaciones 1.5 Tipos de algoritmos 1.6 Perfiles de un equipo 1.7 Flujo de trabajo en ML 1.8 Ciclo de un proyecto 1.9 Taller de Scoping", " Capítulo 1 Conceptos de Ciencia de Datos 1.1 ¿Qué es Ciencia de Datos? Definiendo conceptos: Estadística Disciplina que recolecta, organiza, analiza e interpreta datos. Lo hace a través de una población muestral generando estadística descriptiva y estadística inferencial. La estadística descriptiva, como su nombre lo indica, se encarga de describir datos y obtener conclusiones. Se utilizan números (media, mediana, moda, mínimo, máximo, etc) para analizar datos y llegar a conclusiones de acuerdo a ellos. La estadística inferencial argumenta o infiere sus resultados a partir de las muestras de una población. Se intenta conseguir información al utilizar un procedimiento ordenado en el manejo de los datos de la muestra. La estadística predictiva busca estimar valores y escenarios futuros más probables de ocurrir a partir de referencias históricas previas. Se suelen ocupar como apoyo características y factores áltamente asociados al fenómeno que se desea predecir. Business Intelligence: BI aprovecha el software y los servicios para transformar los datos en conocimientos prácticos que informan las decisiones empresariales estratégicas y tácticas de una organización. Las herramientas de BI acceden y analizan conjuntos de datos y presentan hallazgos analíticos en informes, resúmenes, tableros, gráficos, cuadros, -indicadores- o KPI’s y mapas para proporcionar a los usuarios inteligencia detallada sobre el estado del negocio. BI esta enfocado en analizar la historia pasada para tomar decisiones hacia el futuro. ¿Qué características tiene un KPI? Específicos Continuos y periódicos Objetivos Cuantificables Medibles Realistas Concisos Coherentes Relevantes Machine Learning: Machine learning –aprendizaje de máquina– es una rama de la inteligencia artificial que permite que las máquinas aprendan de los patrones existentes en los datos. Se usan métodos computacionales para aprender de datos con el fin de producir reglas para mejorar el desempeño en alguna tarea o toma de decisión. (Está enfocado en la programación de máquinas para aprender de los patrones existentes en datos principalmente estructurados y anticiparse al futuro) Deep Learning: El aprendizaje profundo es un subcampo del aprendizaje automático que se ocupa de los algoritmos inspirados en la estructura y función del cerebro llamados redes neuronales artificiales. En Deep Learning, un modelo de computadora aprende a realizar tareas de clasificación directamente a partir de imágenes, texto o sonido. Los modelos de aprendizaje profundo pueden lograr una precisión de vanguardia, a veces superando el rendimiento a nivel humano. Los modelos se entrenan mediante el uso de un gran conjunto de datos etiquetados y arquitecturas de redes neuronales que contienen muchas capas. (Está enfocado en la programación de máquinas para el reconocimiento de imágenes y audio (datos no estructurados)) Big data se refiere a los grandes y diversos conjuntos de información que crecen a un ritmo cada vez mayor. Abarca el volumen de información, la velocidad a la que se crea y recopila, y la variedad o alcance de los puntos de datos que se cubren. Los macrodatos a menudo provienen de la minería de datos y llegan en múltiples formatos. Es común que se confunda los conceptos de Big Data y Big Compute, como se mencionó, Big Data se refiere al procesamiento de conjuntos de datos que son más voluminosos y complejos que los tradicionales y Big Compute a herramientas y enfoques que utilizan una gran cantidad de recursos de CPU y memoria de forma coordinada para resolver problemas que usan algoritmos muy complejos. Curiosidad: Servidores en líquido para ser enfriados Curiosidad 2: Centro de datos en el océano Entonces, ¿qué NO es ciencia de datos? No es una tecnología No es una herramienta No es desarrollo de software No es Business Intelligence* No es Big Data* No es Inteligencia Artificial* No es (solo) machine learning No es (solo) deep learning No es (solo) visualización No es (solo) hacer modelos 1.2 Objetivos Los científicos de datos analizan qué preguntas necesitan respuesta y dónde encontrar los datos relacionados. Tienen conocimiento de negocio y habilidades analíticas, así como la capacidad de extraer, limpiar y presentar datos. Las empresas utilizan científicos de datos para obtener, administrar y analizar grandes cantidades de datos no estructurados. Luego, los resultados se sintetizan y comunican a las partes interesadas clave para impulsar la toma de decisiones estratégicas en la organización. Fuente: Blog post de Drew Conway Más sobre Conway: Forbes 2016 1.3 Requisitos Background científico: Conocimientos generales de probabilidad, estadística, álgebra lineal, cálculo, geometría analítica, programación, conocimientos computacionales… etc Datos relevantes y suficientes: Es indispensable saber si los datos con los que se trabajará son relevantes y suficientes, debemos evaluar qué preguntas podemos responder con los datos con los que contamos. Suficiencia: Los datos con los que trabajamos tienen que ser representativos de la población en general, necesitamos que las características representadas en la información sean suficientes para aproximar a la población objetivo. Relevancia: De igual manera los datos tienen que tener relevancia para la tarea que queremos resolver, por ejemplo, es probable que información sobre gusto en alimentos sea irrelevante para predecir número de hijos. Etiquetas: Se necesita la intervención humana para etiquetar, clasificar e introducir los datos en el algoritmo. Software: Existen distintos lenguajes de programación para realizar ciencia de datos 1.4 Aplicaciones Dependiendo de la industria en la que se quiera aplicar Machine Learning, podemos pensar en distintos enfoques, en la siguiente imagen se muestran algunos ejemplos: Podemos pensar en una infinidad de aplicaciones comerciales basadas en el análisis de datos. Con la intención de estructurar las posibles aplicaciones, se ofrece a continuación una categorización que, aunque no es suficiente para englobar todos los posibles casos de uso, sí es sorprendente la cantidad de aplicaciones que abarca. 1. Aplicaciones centradas en los clientes Incrementar beneficio al mejorar recomendaciones de productos Up-selling Cross-selling Reducir tasas de cancelación y mejorar tasas de retención Personalizar experiencia de usuario Mejorar el marketing dirigido Análisis de sentimientos Personalización de productos o servicios 2. Optimización de problemas Optimización de precios Ubicación de nuevas sucursales Maximización de ganancias mediante producción de materias primas Construcción de portafolios de inversión 3. Predicción de demanda Número futuro de clientes Número esperado de viajes en avión / camión / bicis Número de contagios por un virus (demanda médica / medicamentos / etc) Predicción de uso de recursos (luz / agua / gas) 4. Análisis de detección de fraudes Detección de robo de identidad Detección de transacciones ilícitas Detección de servicios fraudulentos Detección de zonas geográficas con actividades ilícitas 1.5 Tipos de algoritmos Los algoritmos de Machine Learning se dividen en tres categorías, siendo las dos primeras las más comunes: La diferencia entre el análisis supervisado y el no supervisado es la etiqueta, es decir, en el análisis supervisado tenemos una etiqueta “correcta” y el objetivo de los algoritmos es predecir esta etiqueta. 1.5.1 Aprendizaje supervisado En el aprendizaje supervisado, la idea principal es aprender bajo supervisión, donde la señal de supervisión se nombra como valor objetivo o etiqueta. Estos algoritmos cuentan con un aprendizaje previo basado en un sistema de etiquetas asociadas a unos datos que les permiten tomar decisiones o hacer predicciones. Conocemos la respuesta correcta de antemano. Esta respuesta correcta fue “etiquetada” por un humano (la mayoría de las veces, en algunas circunstancias puede ser generada por otro algoritmo). Debido a que conocemos la respuesta correcta, existen muchas métricas de desempeño del modelo para verificar que nuestro algoritmo está haciendo las cosas “bien”. Algunos ejemplos son: - Un detector de spam que etiqueta un e-mail como spam o no. - Predecir precios de casas - Clasificación de imagenes - Predecir el clima - ¿Quiénes son los clientes descontentos? Tipos de aprendizaje supervisado (Regresión vs clasificación) Existen dos tipos principales de aprendizaje supervisado, esto depende del tipo de la variable respuesta: Clasificación En el aprendizaje supervisado, los algoritmos de clasificación se usan cuando el resultado es una etiqueta discreta. Esto quiere decir que se utilizan cuando la respuesta se fundamenta en conjunto finito de resultados. Regresión El análisis de regresión es un subcampo del aprendizaje automático supervisado cuyo objetivo es establecer un método para la relación entre un cierto número de características y una variable objetivo continua. 1.5.2 Aprendizaje no supervisado En el aprendizaje no supervisado, carecemos de etiquetas. Por lo tanto, necesitamos encontrar nuestro camino sin ninguna supervisión ni guía. Esto simplemente significa que necesitamos descubrir ¿qué es qué? por nosotros mismos. Aquí no tenemos la respuesta correcta de antemano ¿cómo podemos saber que el algoritmo está bien o mal? Estadísticamente podemos verificar que el algoritmo está bien Siempre tenemos que verificar con el cliente si los resultados que estamos obteniendo tienen sentido de negocio. Por ejemplo, número de grupos y características Algunos ejemplos son: - Encontrar segmentos de clientes. - Reducir la complejidad de un problema - Selección de variables - Encontrar grupos - Reducción de dimensionalidad 1.5.3 Aprendizaje por refuerzo Su objetivo es que un algoritmo aprenda a partir de la propia experiencia. Esto es, que sea capaz de tomar la mejor decisión ante diferentes situaciones de acuerdo a un proceso de prueba y error en el que se recompensan las decisiones correctas. Algunos ejemplos son: - Optimización de campañas de marketing - Reconocimiento facial - Diagnósticos médicos - Clasificar secuencias de ADN Ejemplo: Mario Bros 1.6 Perfiles de un equipo La tecnología crece día con día y resulta imposible abarcar todos los conocimientos y especialidades, por lo que resulta indispensable tener un claro entendimiento de los diferentes roles o perfiles de profesionistas que lograrán colaborar en el alcance de objetivos de negocio basados en el análisis y explotación de datos. A continuación se presenta un esquema global sobre el crecimiento de un equipo con enfoque de ingeniería de inteligencia de decisiones basada en análisis de datos: 1) Ingeniero de datos Es fundamental tener la capacidad de obtener datos antes de que tenga sentido hablar sobre el análisis de datos. Si estás trabajando con conjuntos de datos pequeños, la ingeniería de datos consistirá básicamente en ingresar algunos números en una hoja de cálculo. Cuando se opera en una escala más grande, la ingeniería de datos se convierte en una disciplina sofisticada por derecho propio. Alguien del equipo deberá asumir la responsabilidad de lidiar con los complicados aspectos de ingeniería para poder entregar los datos con los que el resto del equipo pueda trabajar. 2) Decisor Antes de contratar a un científico de datos con un master o PhD, hay que asegurarse de que quien toma las decisiones entiende el arte y la ciencia de la toma de decisiones basada en datos. \\[\\text{Las capacidades de toma de decisiones deben estar establecidas, antes de que un equipo}\\] \\[\\text{pueda obtener valor de los datos}\\] Este individuo es responsable de identificar las decisiones que valen la pena tomar con los datos, plasmarlas en un modelo (desde el diseño de métricas, hasta las inyecciones sobre supuestos estadísticos) y determinar el nivel requerido de rigor analítico, basado en el impacto potencial en el negocio. 3) Analista En este punto, todos los que ya están trabajando contigo en el equipo están calificados para mirar los datos y sentirse inspirados, lo único que puede estar faltando es un poco de familiaridad con el software que sea adecuado para hacer el análisis. Si toda la fuerza laboral está empoderada para hacerlo, se tendrá un mucho mejor pulso de negocio que si nadie está mirando ningún dato en lo absoluto. Lo importante a recordar, es que no se debe llegar a conclusiones más allá de los datos. Eso requiere entrenamiento especializado. Al igual que con la foto de arriba, esto es todo lo que puedes decir al respecto: “Esto es lo que hay en mi conjunto de datos”. Por favor, no usarlo para concluir que el Monstruo de Lago Ness es real. 4) Analista experto ¡Entra la versión ultra rápida! Esta persona puede ver más datos más rápido. El juego aquí es velocidad, exploración, descubrimiento… ¡diversión!. Esta es la persona que ayuda a tu equipo a ver la mayor cantidad de datos posible para que el responsable de la toma de decisiones pueda tener una idea de lo que vale la pena obtener con más detalle. \\[\\text{El trabajo aquí es la velocidad, encontrando potenciales “insights” lo más rápido posible.}\\] Esto puede ser contrario a la intuición, pero no es conveniente asignar esta función a los mejores ingenieros que escriben un magnífico y sólido código de software. El trabajo aquí es velocidad, encontrando potenciales “insights” o revelaciones lo más rápido posible. \\[\\text{Aquellos que se obsesionan con la calidad del código pueden}\\] \\[\\text{encontrar difícil ser útiles en este rol.}\\] 5) Estadístico Ahora que tenemos a todas estas personas contentas explorando los datos, es mejor tener a alguien cerca para controlar tanta exaltación. Podría ser una buena idea tener a alguien cerca que pueda evitar que el equipo saque conclusiones infundadas. \\[\\text{La inspiración es barata, pero el rigor es caro.}\\] Los Estadísticos ayudan a quienes toman las decisiones a llegar a conclusiones seguras más allá de los datos. Por ejemplo, si el sistema de Machine Learning funcionó para un conjunto de datos, todo lo que se puede concluir es que funcionó en ese conjunto de datos. ¿Funcionará cuando se está ejecutando en producción con otros datos? ¿Debería ser lanzado a producción? Necesita algunas habilidades adicionales para lidiar con esas preguntas. Habilidades estadísticas. Si queremos tomar decisiones serias cuando no tenemos datos perfectos, vayamos más despacio y tomemos un enfoque cuidadoso. Los estadísticos ayudan a quienes toman las decisiones a llegar a conclusiones más seguras, más allá de los datos analizados. 6) Ingeniero de Machine Learning El mejor atributo de un ingeniero de Machine Learning / Inteligencia Artificial aplicada es diseñar, crear, evaluar y producir modelos para resolver problemas de la vida real. Lo que estás buscando es experiencia en transformar código para hacer que los algoritmos existentes acepten y revuelvan tus conjuntos de datos. Se busca una personalidad que tenga tolerancia frente al fracaso. Ejecuta los datos a través de un grupo de algoritmos lo más rápido posible y ve sí está funcionando. Una gran parte del trabajo es ir tanteando a ciegas, y se necesita un tipo especial de personalidad para disfrutar eso. \\[\\text{Los perfeccionistas tienden a tener problemas como ingenieros de ML.}\\] Como el problema de negocio no está en un libro de texto, no puede saberse de antemano qué funcionará, por lo que no puede esperarse obtener un resultado perfecto la primera vez. ¡Eso bien!, solo hay que intentar muchos enfoques lo más rápido posible e intentar encontrar una solución. Es importante que el ingeniero de Machine Learning tenga un profundo respeto por la parte del proceso donde el rigor es vital: la evaluación. ¿Funcionó realmente la solución con nuevos datos? 7) Científico de Datos Un científico de datos es alguien que es un experto en los tres roles anteriores. No todos usan esta definición: verás las solicitudes de empleo por ahí con personas que se autodenominan “científicos de datos” cuando sólo dominan realmente uno de los tres, así que vale la pena comprobarlo. Este rol está en la posición # 7 porque contratar a los verdaderos tres-en-uno es una opción costosa. Antes de contratar a un nuevo especialista 3 en 1, considera hacer crecer a tu equipo actual. 8) Gerente de Análisis / Líder de Ciencia de Datos El Gerente de Analítica es la gallina de los huevos de oro: son un híbrido entre el científico de datos y el que toma las decisiones. Su presencia en el equipo actúa como un multiplicador de fuerzas, lo que garantiza que tu equipo de ciencia de datos no quede en fuera de juego en lugar de agregar valor a tu negocio. Esta persona se queda despierta por la noche pensando preguntas como: “¿Cómo diseñamos las preguntas correctas? ¿Cómo tomamos decisiones? ¿Cómo podemos asignar mejor a nuestros expertos? ¿Qué vale la pena hacer? ¿Las habilidades y los datos coincidirán con los requerimientos? ¿Cómo aseguramos buenos datos de entrada? 9) Experto Cualitativo / Científico Social A veces, quien toma las decisiones es un brillante líder, gerente, motivador, influyente o navegante de la política organizacional, pero no siempre tiene experiencia en el arte y la ciencia de la toma de decisiones. El experto cualitativo está para complementar las habilidades del equipo técnico en matemáticas y programación. Esta persona generalmente tiene formación en ciencias sociales y datos: Los economistas de comportamiento, neuroeconomistas y psicólogos reciben la capacitación más especializada, pero la gente autodidacta también puede ser buena en esto. El trabajo consiste en ayudar al responsable de la toma de decisiones a aclarar ideas, examinar todos los ángulos y convertir las intuiciones ambiguas en instrucciones bien pensadas en un lenguaje que facilite la ejecución al resto del equipo. Por lo general, los científicos sociales están mejor equipados que los científicos de datos para traducir en métricas concretas, las intuiciones e intenciones de quienes toman las decisiones. Se aseguran de que el responsable de la toma de decisiones haya captado completamente la información disponible para poderla tomar. También son un asesor de confianza, un compañero de intercambio de ideas y una caja de resonancia para quien toma las decisiones. 10) Investigador Se trata de profesionistas de larga trayectoria tanto académica como práctica. Son profesionistas con PhD. Su gran experiencia y conocimientos los hacen ser capaces de construir nuevas herramientas hechas a la medida que no existen en el mercado. Este perfil es de gran beneficio cuando ya se cuenta con las posiciones anteriores. Si el investigador es el primer empleado, es probable que no se tenga el entorno adecuado para hacer un buen uso y aprovechamiento de sus conocimientos y habilidades. \\[\\text{&quot;Antes de construir ese bolígrafo espacial para viajar a la Luna, }\\] \\[\\text{comprueba primero si un lápiz puede hacer el trabajo.&quot;}\\] Es conveniente esperar hasta que el equipo se haya desarrollado lo suficiente como para haber averiguado para qué específicamente necesitan un investigador. Extra) Personal Adicional Además de los roles que vimos, estas son algunas de las personas que podrían para participar en un proyecto de inteligencia de decisiones: Experto en áreas específicas de negocio Ética Ingeniero de software Ingeniero de Confiabilidad Diseñador de Experiencia de Usuario Visualizador interactivo / diseñador gráfico. Especialista en recolección de datos Gerente de producto de datos Gerente de proyectos / programas Muchos proyectos no pueden prescindir de ellos. La única razón por la que no figuran en el top 10 es que la inteligencia de decisiones basadas en la explotación de datos no es su negocio principal. En su lugar, son genios en su propia área y han aprendido lo suficiente sobre datos y la toma de decisiones para ser muy útiles en el proyecto. Piensa en ellos como si tuvieran su propia carrera universitaria, pero con suficiente amor por la inteligencia de decisiones que eligieron estudiarla como una especialización. Fuente: Medium Google Data Career Path Google es una compañía bastante especializada en procesos de desarrollo de tecnología con grandes aplicaciones comerciales. Cada compañía puede armar su propio equipo de acuerdo con la estrategia que más se apegue a sus objetivos, no obstante, siempre vale la pena conocer los perfiles que Google define y sugiere. Fuente: Data Science On Google Cloud Platform Esta definición de roles es particularmente técnica, por lo que es importante añadir los perfiles de expertos de negocio. 1.7 Flujo de trabajo en ML Al conocer el camino antes de atravesarlo habrá mucho más probabilidad de que sea alcanzado de manera exitosa. Como un preciado extra, además podrá lograrse en una cantidad significativamente menor de tiempo a lo que llevaría avanzar sin un esquema de trabajo. En esta sección se muestra el flujo de trabajo general que todo científico de datos debe implementar para llevar a cabo un exitoso proyecto basado en análisis de datos. Existen múltiples propuestas sobre el flujo de trabajo a implementar, sin embargo, la mayoría son muy similares. La diferencia estas propuestas son aspectos triviales que no se ven reflejados de una manera importante en la estructura del flujo de trabajo. El diagrama anterior muestra los elementos que pueden encontrarse en todo proyecto de analítica. 1. Business Understanding: La ciencia y tecnología aplicada a un problema es peligrosa si no se cuenta con el conocimiento de la naturaleza del problema y los factores que los afectan. El entendimiento del negocio es vital para lograr resolver el problema y solo mediante el profundo entendimiento es que se logra encontrar nuevas aportaciones para mejorar la calidad de la solución. 2. Data Mining: Es indispensable contar con una fuente de datos que provea a todo el equipo la información necesaria para el desarrollo del proyecto. Recolectar datos de diversas fuentes y centralizarlos para el consumo de todos aquellos autorizados a consumirlos, es la primer tarea técnica a satisfacer. En esta etapa se deberán separar los datos que serán usados en las siguientes etapas para: Creación del modelo Pruebas de calidad Validación de resultados 3. Data Cleaning: Los datos suelen no estar listos para ser usados. Suelen existir datos faltantes o equivocados que en caso de usarse directamente provocarán resultados de baja calidad. Es indispensable limpiar estos datos para que los modelos tengan resultados con la mejor calidad posible. 4. Data Exploration: La exploración es necesaria para conocer el contenido, estructura, distribución y relación que existe entre los datos. Los modelos suelen tener distintos supuestos del comportamiento de la información para poder ser implementados. En la medida en que se conocen estos atributos, será posible proponer alternativas y soluciones adicionales que aporten valor. 5. Feature Engineering: La ingeniería de variables es el proceso en el que se manufacturan nuevas variables que aportan valor adicional al que ofrecen las variables originales. Esta etapa es posible realizarse con éxito cuando se conoce el negocio y se ha realizado una exploración de datos profunda. 6. Predictive Modeling Una vez que se ha creado el conjunto de datos que sirve de insumo para el análisis, se procede a la creación del modelo. En esta etapa se ponen a prueba múltiples modelos hasta elegir el que aporta la mejor solución de acuerdo con el objetivo de negocio. 7. Data Visualization Todos los modelos tienen un tiempo de vida finita. Con el paso del tiempo la calidad va disminuyendo debido a cambios en el comportamiento de la población o fenómenos involucrados, por lo que los resultados y la calidad del modelo deben ser monitoreados constantemente para conocer el momento en que dejan de ser útiles y debe darse mantenimiento al proyecto. En R, existen librerías que ayudan a llevar a cabo todo el proceso de principio a fin. En las siguientes sesiones se llevará a cabo la explicación de todo el proceso y su implementación. 1.8 Ciclo de un proyecto Identificación del problema Debemos conocer si el problema es significativo, si el problema se puede resolver con ciencia de datos, y si habrá un compromiso real del lado de cliente/usuario/partner para implementar la solución con todas sus implicaciones: recursos físicos y humanos. Scoping El objetivo es definir el alcance del proyecto y por lo tanto definir claramente los objetivos. Conocer las acciones que se llevarán a cabo para cada objetivo. Estas definirán las soluciones analíticas a hacer. Queremos saber si los datos con los que contamos son relevantes y suficientes. Hacer visible los posibles conflictos éticos que se pueden tener en esta fase. Debemos definir el cómo evaluaremos que el análisis de esos datos será balanceada entre eficiencia, efectividad y equidad. Adquisición de datos Adquisición, almacenamiento, entendimiento y preparación de los datos para después poder hacer analítica sober ellos. Asegurar que en la transferencia estamos cumpliendo con el manejo adecuado de datos sensibles y privados. EDA El objetivo en esta fase es conocer los datos con los que contamos y contexto de negocio explicado a través de los mismos. Identificamos datos faltantes, sugerimos cómo imputarlos. Altamente apoyado de visualización y procesos de adquisición y limpieza de datos. Formulación analítica Esta fase incluye empezar a formular nuestro problema como uno de ciencia de datos, el conocimiento adquirido en la fase de exploración nos permite conocer a mayor detalle del problema y por lo tanto de la solución adecuada. Modelado Proceso iterativo para desarrollar diferentes “experimentos”. Mismo algoritmo/método diferentes hiperparámetros (grid search). Diferentes algortimos. Selección de un muy pequeño conjunto de modelos tomando en cuenta un balance entre interpretabilidad, complejidad, desempeño, fairness. Correcta interpretación de los resultados de desempeño de cada modelo. Validación Es muy importante poner a prueba el/los modelo/modelos seleccionados en la fase anterior. Esta prueba es en campo con datos reales, le llamamos prueba piloto. Debemos medir el impacto causal que nuestro modelo tuvo en un ambiente real. Acciones a realizar Finalmente esta etapa corresponde a compartir con los tomadores de decisiones/stakeholders/creadores de política pública los resultados obtenidos y la recomendación de acciones a llevar a cabo -menú de opciones-. Las implicaciones éticas de esta fase consisten en hacer conciente el impacto social de nuestro trabajo. 1.9 Taller de Scoping El scoping es uno de los pasos más importante en los proyectos de ciencia de datos, es ideal realizarlo con ayuda del cliente, tiene como objetivo definir el alcance del proyecto, definir los objetivos, conocer las acciones que se llevaran acabo, conocer si los datos son relevantes y suficientes, proponer soluciones analíticas, entre otros puntos que se tocaran a continuación. 1.9.1 Data Maturity Framework Antes de iniciar con el scoping, queremos conocer si los interesados están listos para realizar un proyecto de ciencia de datos. Para ello, una opción es usar el Data Maturity Framework desarrollado en la Universidad de Chicago. El Data Maturity Framework nos sirve para ver dónde se encuentra la organización en el marco de madurez de datos y cómo mejorar su organización, tecnología y preparación de datos. Tiene tres áreas de contenido: Definición del problema Disponibilidad de datos y tecnología Preparación organizacional Esta dividido en tres partes: Un cuestionario y una encuesta para evaluar la preparación de la organización. Matriz de preparación de datos y tecnología Matriz de preparación organizacional 1.9.2 Scoping Para realizar el scoping podemos apoyarnos del siguiente documento. Ya que sabemos que la organización esta preparada para realizar un proyecto de ciencia de datos, podemos iniciar el scoping. El proceso a seguir es el siguiente: Definir objetivo(s) Considerado el paso más importante del proceso, los stakeholders iniciaran con un planteamiento del problema de manera muy general, nuestra responsabilidad será ir aterrizando ideas y definir el problema de manera más concreta, esta parte del scoping puede ocurrir en distintas iteraciones. Necesitamos hacer que el objetivo sea concreto, medible y optimizable. Cuando se van refinando objetivos, es común que se vaya priorizando por lo que tendremos tradeoffs que irán ligados a las acciones y al contexto del negocio. ¿Qué acciones o intervenciones existen que serán mejoradas a través de este proyecto? Debemos definir acciones concretas, si esto no ocurre es muy probable que la solución no sea implementada por lo que el proyecto no tendrá uso y no se estará haciendo ciencia de datos. La implementación del proyecto debería ayudar a tener mejor información para llevar acabo estas acciones, es decir, el proyecto mejorará la toma de decisiones basadas en la evidencia de los datos. Hacer una lista con las acciones ayuda a que el proyecto sea accionable, es posible que estas acciones no existan aún en la organización, por lo que el proyecto puede ayudar a generar nuevas acciones. Es muy común que la acción definida por el stakeholder sea de muy alto nivel, en ese caso podemos tomar 2 caminos en el scoping: Proponer en el scoping que el proyecto informe a esa acción general. Generar a partir de esa acción general acciones más pequeñas. ¿Qué datos tenemos y cuáles necesitamos? Primero observemos que no se había hablado de los datos hasta este punto, lo anterior porque debemos primero pensar en el problema, entenderlo y luego ver con qué datos contamos para resolverlo. Si hacemos esto primero seguramente acabaremos desarrollando productos de datos “muertos” y no accionables. En este paso se le dará uso al Data Maturity Framework, queremos conocer cómo se guardan los datos, con qué frecuencia, en qué formato, en qué estructura, qué granularidad tiene, desde cuándo tenemos historia de estos datos, si existe un sesgo en su recolección, con qué frecuencia recolectan nueva información, sobrescribe la ya existente? Uno de los objetivos consiste en identificar si la granularidad, frecuencia y horizonte de tiempo en los datos corresponde a la granularidad, frecuencia y horizonte de tiempo de las acciones. ¿Cuál es el análisis que necesitamos hacer? En esta sección del scoping queremos definir qué tipo de análisis necesitamos hacer con los datos con los que contamos para cumplir con los objetivos definidos y generar las acciones identificadas. El análisis puede incluir métodos y herramientas de diferentes disciplinas: ciencias computacionales, ciencia de datos, machine learning, estadística, ciencias sociales. Existen distintos tipos de análisis, los 4 más comunes son: Descripción: Centrado en entender eventos y comportamientos del pasado. Aunque puede confundirse con business intelligence, debido a que ya definimos objetivos y acciones vamos a desarrollar un producto de datos. Para este tipo de análisis podemos ocupar métodos de aprendizaje no supervisado: clustering. Detección: Más concentrado en los eventos que están sucediendo. Detección de anomalías. Predicción: Concentrado en el futuro, prediciendo futuros eventos o comportamientos. Cambio en comportamiento: Concentrado en entender las causas de cambios en comportamientos de personas eventos, organizaciones, vecindarios, etc. En esta fase tenemos que responder las siguientes preguntas: ¿Qué tipo de análisis necesitaremos? Puede ser más de uno. ¿Cómo vamos a validar el análisis? ¿Qué validaciones se pueden hacer con los datos existentes? ¿Cómo podemos diseñar una prueba en campo para validar el análisis antes de que pongamos el producto en producción. Identificar qué acciones se cubren con cada análisis, debemos tener todas las acciones cubiertas. Ejemplos Los siguientes ejemplos forman parte del trabajo de DSSG, en cada uno de estos planteamientos intentaremos responder las siguientes preguntas: ¿Cuál es el objetivo? ¿Cómo se mide el objetivo? ¿Qué se optimiza? ¿Se puede optimizar? ¿Cuáles son los tradeoffs? ¿Que implicaciones éticas identificas? Envenenamiento por plomo: Hace unos años, comenzamos a trabajar con el Departamento de Salud Pública de Chicago para prevenir el envenenamiento por plomo. El objetivo inicial era aumentar la eficacia de sus inspecciones de peligro de plomo. Una forma de lograr ese objetivo sería concentrarse en los hogares que tienen peligros de plomo. Aunque fue útil, este enfoque no lograría su objetivo real, que era evitar que los niños se intoxicaran con plomo. Encontrar un hogar con peligros de plomo y repararlo solo es beneficioso si existe una alta probabilidad de que un niño presente (actualmente o en el futuro) se exponga al plomo. La siguiente iteración del objetivo fue maximizar la cantidad de inspecciones que detectan peligros de plomo en hogares donde hay un niño en riesgo (antes de que el niño se exponga al plomo). Finalmente, llegamos al objetivo final: identificar qué niños corren un alto riesgo de intoxicación por plomo en el futuro y luego dirigir las intervenciones a los hogares de esos niños.. High School Graduation: Uno de los mayores desafíos que enfrentan las escuelas hoy en día es ayudar a sus estudiantes a graduarse (a tiempo). Las tasas de graduación en los EE. UU. Son ~65%. Todos están interesados en identificar a los estudiantes que corren el riesgo de no graduarse a tiempo. Al hablar inicialmente con la mayoría de los distritos escolares, comienzan con un objetivo muy limitado de predecir qué niños es poco probable que se gradúen a tiempo. El primer paso es volver al objetivo de aumentar las tasas de graduación y preguntar si hay un subconjunto específico de estudiantes en riesgo que quieran identificar. ¿Qué pasaría si pudiéramos identificar a los estudiantes que tienen solo un 5% de probabilidades de estar en riesgo frente a los estudiantes que tienen un 95% de probabilidades de no graduarse a tiempo sin apoyo adicional? Si el objetivo es simplemente aumentar las tasas de graduación, es (probablemente) más fácil intervenir e influir en el primer grupo, mientras que el segundo grupo puede ser más desafiante debido a los recursos que necesita. ¿El objetivo es maximizar la probabilidad promedio/media/mediana de graduarse para una clase/escuela o es el objetivo enfocarse en los niños con mayor riesgo y maximizar la probabilidad de graduación del 10% inferior de los estudiantes? ¿O el objetivo es crear más equidad y disminuir la diferencia en la probabilidad de graduación a tiempo entre el cuartil superior y el cuartil inferior? Todos estos son objetivos razonables, pero las escuelas deben comprender, evaluar y decidir qué objetivos les interesan. Esta conversación a menudo los hace pensar más en definir analíticamente cuáles son sus objetivos organizacionales, así como las compensaciones.. Inspecciones: Hemos trabajado en varios proyectos que involucraron inspecciones, como con la EPA (Agencia de Protección Ambiental) y el Departamento de Conservación Ambiental del Estado de Nueva York para ayudarlos a priorizar qué instalaciones inspeccionar para detectar infracciones de eliminación de desechos, con la ciudad de Cincinnati para ayudar a identificar las propiedades en riesgo de violaciones del código para prevenir el deterioro -el proceso a través del cual una ciudad que funcionaba anteriormente, o parte de ella, cae en deterioro y decrepitud-, y con el Grupo del Banco Mundial para ayudarlos a priorizar qué denuncias de fraude y colusión investigar. En la mayoría de los problemas de inspección/investigación, hay muchas más entidades (viviendas, edificios, instalaciones, negocios, contratos) para inspeccionar que los recursos disponibles necesarios para realizar esas inspecciones. El objetivo con el que comienzan la mayoría de estas organizaciones es dirigir sus inspecciones a las entidades que tienen más probabilidades de violar las regulaciones existentes. Ese es un buen comienzo, pero la mayoría de estas organizaciones nunca pueden inspeccionar todas las instalaciones/hogares que pueden no cumplir con las normas, por lo que el objetivo que realmente buscan es la disuasión: reducir la cantidad total de instalaciones que estarán en violación. Un proceso de inspección ideal resultaría entonces en la reducción del número real de violaciones (encontradas o no), lo cual puede no ser lo mismo que un proceso de inspección que tiene como objetivo ser eficiente y aumentar la tasa de aciertos (% de inspección que resulta en violaciones). Programación de la recolección de residuos: Recientemente comenzamos a trabajar con Sanergy, una empresa social con sede en Kenia. Implementan inodoros portátiles en asentamientos urbanos informales y uno de sus mayores costos es contratar personas para vaciar los inodoros. Hoy en día, todos los inodoros se vacían todos los días, aunque existe una variación en cuánto se usan y cuánto se llenan. Para que puedan crecer y mantener bajos los costos, necesitan un enfoque más adaptable que pueda optimizar el cronograma de vaciado de los inodoros. El objetivo en este caso es asegurarse de no vaciar demasiado el inodoro cuando no está lleno, pero tampoco dejar que permanezca lleno porque entonces no se puede usar. Esto se traduce en una formulación que presiona para vaciar el inodoro lo más cerca posible de estar lleno al 100% sin llegar al 100%. "],["introducción-a-r.html", "Capítulo 2 Introducción a R 2.1 ¿Cómo obtener R? 2.2 ¿Qué es RStudio? 2.3 R como lenguaje orientado a objetos 2.4 Estructuras de almacenamiento 2.5 Funciones básicas de R 2.6 Estructuras de control 2.7 Guía de estilo", " Capítulo 2 Introducción a R R (R Core Team) es un entorno y lenguaje de programación que permite el análisis estadístico de información y reportes gráficos. Es ampliamente usado en investigación por la comunidad estadística en campos como la biomedicina, minería de datos, finanzas, seguros, entre otros. Ha ganado mucha popularidad en los últimos años al ser un software libre que está en constante crecimiento por las aportaciones de otros usuarios y que permite la interacción con software estadísticos como STATA, SAS, SPSS, etc. R permite la incorporación de librerías y paqueterías con funcionalidades específicas, por lo que es un lenguaje de programación muy completo y fácil de usar. 2.1 ¿Cómo obtener R? R puede ser fácilmente descargado de forma gratuita desde el sitio oficial http://www.r-project.org/. R está disponible para las plataformas Windows, Mac y Linux. 2.2 ¿Qué es RStudio? RStudio es un Entorno de Desarrollo Integrado (IDE, por sus siglas en inglés) para R. Este permite y facilita el desarrollo y ejecución de sintaxis para código en R, incluye una consola y proporciona herramientas para la gestión del espacio de trabajo. RStudio está disponible para Windows, Mac y Linux o para navegadores conectados a RStudio Server o RStudio Server Pro. Algunas de las principales características de Rstudio que lo hacen una gran herramienta para trabajar en R, son: Auto completado de código Sangría inteligente Resaltado de sintaxis Facilidad para definir funciones Soporte integrado Documentación integrada Administración de directorios y proyectos Visor de datos Depurador interactivo para corregir errores Conexión con Rmarkwon y Sweave La siguiente imagen muestra la forma en la que está estructurado RStudio. El orden de las ventanas puede ser elegido por el usuario, así como las características de tipo de letra, tamaño y color de fondo, entre otras características. Figure 2.1: Páneles de trabajo de Rstudio 2.3 R como lenguaje orientado a objetos R es un lenguaje de programación orientado a objetos (POO). Un objeto es “cualquier cosa con significado para el problema que se trata de resolver”. Los objetos tienen características fundamentales que permiten identificarlos, conocerlos y entender su comportamiento. De acuerdo con (Schildt 2009), estas características son: Identidad: Esta es la propiedad que da nombre a cada uno de los objetos y que permite declararlos, distinguirlos de manera única, usarlos y llamarlos para la representación de su contenido. Comportamiento: Esta es la propiedad que determina las operaciones que puede realizar el objeto, es decir, permite conocer las capacidades y alcances de la funcionalidad de cada objeto. El comportamiento permite conocer la interacción que puede existir con otros objetos y los resultados que generarán. Estructura: El estado se refiere a un conjunto de características o atributos específicos del objeto dados en un momento determinado, y que pueden cambiar en un instante de tiempo. En la programación orientada a objetos, un programa recolecta muchos objetos para ser tratado como un conjunto dinámico de objetos interactuando entre sí. Los objetos están definidos por: Atributos: Son las propiedades o características de los datos contenidos en un objeto. Los valores asociados a un objeto en un momento determinado del tiempo determinan su estado. Métodos: Acceden a los atributos de los objetos y determinan el comportamiento de los datos contenidos. 2.4 Estructuras de almacenamiento En R existen varios tipos de objectos que permiten que el usuario pueda almacenar la información para realizar procedimientos estadísticos y gráficos. Los principales objetos en R son vectores, matrices, arreglos, marcos de datos y listas. A continuación se presentan las características de estos objetos y la forma para crearlos. 2.4.1 Operadores de asignación En R se pueden hacer asignación de varias formas, a continuación se presentan los operadores disponibles para tal fin. &lt;- este es el operador de asignación a izquierda, es el más usado y recomendado. -&gt; este es el operador de asignación a derecha, no es frecuente su uso. = el símbolo igual sirve para hacer asignaciones pero NO se recomienda usarlo. &lt;&lt;- este es un operador de asignación global y sólo debe ser usado por usuarios avanzados. Ejemplo Almacene los valores 5.3, 4.6 y 25 en los objetos a, b y age respectivamente, use diferentes símbolos de asignación. Para hacer lo solicitado se podría usar el siguiente código. a &lt;- 5.3 # Recomendado 4.6 -&gt; b # No es usual age = 25 # No recomendado Aunque una asignación se puede hacer de tres formas diferentes, se recomienda sólo usar el símbolo &lt;-. 2.4.2 Variables Las variables sirven para almacenar un valor que luego vamos a utilizar en algún procedimiento. Para hacer la asignación de un valor a alguna variable se utiliza el operador &lt;- entre el valor y el nombre de la variable. A continuación un ejemplo sencillo. x &lt;- 5 (2 * x) + 3 ## [1] 13 En el siguiente ejemplo se crea la variable país y se almacena el nombre Colombia, luego se averigua el número de caracteres de la variable país. pais &lt;- &quot;México&quot; nchar(pais) ## [1] 6 También existen variables lógicas y estas toman los valores verdadero (TRUE) o falso (FALSE) dependiendo del resultado lógico puesto a prueba. Ejemplo: y &lt;- 10 y == (5 + 3 + 2) ## [1] TRUE y != 5 + 5 ## [1] FALSE 2.4.3 Vectores Los vectores vectores son arreglos ordenados en los cuales se puede almacenar información de tipo numérico (variable cuantitativa), alfanumérico (variable cualitativa) o lógico (TRUE o FALSE), pero no mezclas de éstos. La función de R para crear un vector es c() y que significa concatenar; dentro de los paréntesis de esta función se ubica la información a almacenar. Una vez construido el vector se acostumbra a etiquetarlo con un nombre corto y representativo de la información que almacena, la asignación se hace por medio del operador &lt;- entre el nombre y el vector. A continuación se presenta un ejemplo de cómo crear tres vectores que contienen las respuestas de cinco personas a tres preguntas que se les realizaron. edad &lt;- c(15, 19, 13, NA, 20) deporte &lt;- c(TRUE, TRUE, NA, FALSE, TRUE) sexo &lt;- c(&quot;Hombre&quot;, &quot;Mujer&quot;, &quot;Hombre&quot;, &quot;Hombre&quot;, &quot;Mujer&quot;) El vector edad es un vector cuantitativo y contiene las edades de las 5 personas. En la cuarta posición del vector se colocó el símbolo NA que significa Not Available debido a que no se registró la edad para esa persona. Al hacer una asignación se acostumbra a dejar un espacio antes y después del operador &lt;- de asignación. El segundo vector es llamado deporte y es un vector lógico que almacena las respuestas a la pregunta de si la persona practica deporte, nuevamente aquí hay un NA para la tercera persona. El último vector sexo contiene la información del sexo de cada persona, como esta variable es cualitativa es necesario usar las comillas ” ” para encerrar las respuestas. ¡¡ RECORDAR !! Cuando se usa NA para representar una información Not Available no se deben usar comillas. Es posible usar comillas ‘sencillas’ o comillas “dobles” para ingresar valores de una variable cualitativa. Si se desea ver lo que está almacenado en cada uno de estos vectores, se debe escribir en la consola de R el nombre de uno de los objetos y luego se presiona la tecla enter o intro, al realizar esto lo que se obtiene se muestra a continuación. edad ## [1] 15 19 13 NA 20 deporte ## [1] TRUE TRUE NA FALSE TRUE sexo ## [1] &quot;Hombre&quot; &quot;Mujer&quot; &quot;Hombre&quot; &quot;Hombre&quot; &quot;Mujer&quot; 2.4.3.1 ¿Cómo extraer elementos de un vector? Para extraer un elemento almacenado dentro un vector se usan los corchetes [] y dentro de ellos la posición o posiciones que interesan. Ejemplo Si queremos extraer la edad de la tercera persona escribimos el nombre del vector y luego \\[3\\] para indicar la tercera posición de edad, a continuación el código. edad[3] ## [1] 13 Si queremos conocer el sexo de la segunda y quinta persona, escribimos el nombre del vector y luego, dentro de los corchetes, escribimos otro vector con las posiciones 2 y 5 que nos interesan así: \\(c(2, 5)\\), a continuación el código. sexo[c(2, 5)] ## [1] &quot;Mujer&quot; &quot;Mujer&quot; Si nos interesan las respuestas de la práctica de deporte, excepto la de la persona 3, usamos \\[-3\\] luego del nombre del vector para obtener todo, excepto la tercera posición. deporte[-3] ## [1] TRUE TRUE FALSE TRUE ¡¡ RECORDAR !! Si desea extraer varios posiciones de un vector NUNCA escriba esto: mi_vector[2, 5, 7]. Tiene que crear un vector con las posiciones y luego colocarlo dentro de los corchetes así: \\[mi\\_vector[c(2, 5, 7)]\\] 2.4.4 Matrices Las matrices son arreglos rectangulares de filas y columnas con información numérica, alfanumérica o lógica. Para construir una matriz se usa la función matrix( ). Por ejemplo, para crear una matriz de 4 filas y 5 columnas (de dimensión 4×5) con los primeros 20 números positivos se escribe el código siguiente en la consola. mimatriz &lt;- matrix(data = 1:20, nrow = 4, ncol = 5, byrow = FALSE) El argumento data de la función sirve para indicar los datos que se van a almacenar en la matriz, los argumentos nrow y ncol sirven para definir la dimensión de la matriz y por último el argumento byrow sirve para indicar si la información contenida en data se debe ingresar por filas o no. Para observar lo que quedó almacenado en el objeto mimatriz se escribe en la consola el nombre del objeto seguido de la tecla enter o intro. mimatriz ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 5 9 13 17 ## [2,] 2 6 10 14 18 ## [3,] 3 7 11 15 19 ## [4,] 4 8 12 16 20 2.4.4.1 ¿Cómo extraer elementos de una matriz? Al igual que en el caso de los vectores, para extraer elementos almacenados dentro de una matriz se usan los corchetes [ , ] y dentro, separado por una coma, el número de fila(s) y el número de columna(s) que nos interesan. Ejemplo Si queremos extraer el valor almacenado en la fila 3 y columna 4 usamos el siguiente código. mimatriz[3, 4] ## [1] 15 Si queremos recuperar toda la fila 2 usamos el siguiente código. mimatriz[2, ] # No se escribe nada luego de la coma ## [1] 2 6 10 14 18 Si queremos recuperar toda la columna 5 usamos el siguiente código. mimatriz[, 5] # No se escribe nada antes de la coma ## [1] 17 18 19 20 Si queremos recuperar la matriz original sin las columnas 2 y 4 usamos el siguiente código. mimatriz[, -c(2, 4)] # Las columnas como vector ## [,1] [,2] [,3] ## [1,] 1 9 17 ## [2,] 2 10 18 ## [3,] 3 11 19 ## [4,] 4 12 20 Si queremos recuperar la matriz original sin la fila 1 ni columna 3 usamos el siguiente código. mimatriz[-1, -3] # Signo de menos para eliminar ## [,1] [,2] [,3] [,4] ## [1,] 2 6 14 18 ## [2,] 3 7 15 19 ## [3,] 4 8 16 20 2.4.5 Arreglos Un arreglo es una matriz de varias dimensiones con información numérica, alfanumérica o lógica. Para construir una arreglo se usa la función array( ). Por ejemplo, para crear un arreglo de 3 × 4 × 2 con las primeras 24 letras minúsculas del alfabeto se escribe el siguiente código. miarray &lt;- array(data = letters[1:24], dim=c(3, 4, 2)) El argumento data de la función sirve para indicar los datos que se van a almacenar en el arreglo y el argumento dim sirve para indicar las dimensiones del arreglo. Para observar lo que quedó almacenado en el objeto miarray se escribe en la consola lo siguiente. miarray ## , , 1 ## ## [,1] [,2] [,3] [,4] ## [1,] &quot;a&quot; &quot;d&quot; &quot;g&quot; &quot;j&quot; ## [2,] &quot;b&quot; &quot;e&quot; &quot;h&quot; &quot;k&quot; ## [3,] &quot;c&quot; &quot;f&quot; &quot;i&quot; &quot;l&quot; ## ## , , 2 ## ## [,1] [,2] [,3] [,4] ## [1,] &quot;m&quot; &quot;p&quot; &quot;s&quot; &quot;v&quot; ## [2,] &quot;n&quot; &quot;q&quot; &quot;t&quot; &quot;w&quot; ## [3,] &quot;o&quot; &quot;r&quot; &quot;u&quot; &quot;x&quot; 2.4.5.1 ¿Cómo extraer elementos de un arreglo? Para recuperar elementos almacenados en un arreglo se usan también corchetes, y dentro de los corchetes, las coordenadas del objeto de interés. Ejemplo Si queremos extraer la letra almacenada en la fila 1 y columna 3 de la segunda capa de miarray usamos el siguiente código. miarray[1, 3, 2] # El orden es importante ## [1] &quot;s&quot; Si queremos extraer la segunda capa completa usamos el siguiente código. miarray[,, 2] # No se coloca nada en las primeras posiciones ## [,1] [,2] [,3] [,4] ## [1,] &quot;m&quot; &quot;p&quot; &quot;s&quot; &quot;v&quot; ## [2,] &quot;n&quot; &quot;q&quot; &quot;t&quot; &quot;w&quot; ## [3,] &quot;o&quot; &quot;r&quot; &quot;u&quot; &quot;x&quot; Si queremos extraer la tercera columna de todas las capas usamos el siguiente código. miarray[, 3,] # No se coloca nada en las primeras posiciones ## [,1] [,2] ## [1,] &quot;g&quot; &quot;s&quot; ## [2,] &quot;h&quot; &quot;t&quot; ## [3,] &quot;i&quot; &quot;u&quot; 2.4.6 Data Frames El marco de datos marco de datos o data frame es uno de los objetos más utilizados porque permite agrupar vectores con información de diferente tipo (numérica, alfanumérica o lógica) en un mismo objeto, la única restricción es que los vectores deben tener la misma longitud. Para crear un marco de datos se usa la función data.frame( ), como ejemplo vamos a crear un marco de datos con los vectores edad, deporte y sexo definidos anteriormente. mi_data_frame &lt;- data.frame(edad, deporte, sexo) Una vez creado el objeto mi_data_frame podemos ver el objeto escribiendo su nombre en la consola, a continuación se muestra lo que se obtiene. mi_data_frame ## edad deporte sexo ## 1 15 TRUE Hombre ## 2 19 TRUE Mujer ## 3 13 NA Hombre ## 4 NA FALSE Hombre ## 5 20 TRUE Mujer De la salida anterior vemos que el marco de datos tiene 3 variables (columnas) cuyos nombres coinciden con los nombres de los vectores creados anteriormente, los números consecutivos al lado izquierdo son sólo de referencia y permiten identificar la información para cada persona en el conjunto de datos. Ejercicios: Use funciones o procedimientos (varias líneas) de R para responder cada una de las siguientes preguntas. Construya un vector con 5 nombres de personas. Construya un vector con las edades de las 5 personas anteriores. Construya un marco de datos o data frame con las respuestas de 5 personas a las preguntas: ¿Cuál es su nombre? Sexo de la persona ¿Cuál es su edad en años? ¿En qué alcaldía vive? ¿En qué alcaldía trabaja? 2.4.7 Listas Las listas son otro tipo de objeto muy usado para almacenar objetos de diferente tipo. La instrucción para crear una lista es list( ). A continuación vamos a crear una lista que contiene tres objetos: un vector con 5 números aleatorios llamado mivector, una matriz de dimensión 6×2 con los primeros doce números enteros positivos llamada matriz2 y el tercer objeto será el marco de datos mi_data_frame creado en el apartado anterior. Las instrucciones para crear la lista requerida se muestran a continuación. set.seed(12345) mivector &lt;- runif(n=5) matriz2 &lt;- matrix(data=1:12, ncol=6) milista &lt;- list(E1=mivector, E2=matriz2, E3=mi_data_frame) La función set.seed de la línea número 1 sirve para fijar la semilla de tal manera que los números aleatorios generados en la segunda línea con la función runif sean siempre los mismos. En la última línea del código anterior se construye la lista, dentro de la función list se colocan los tres objetos mivector, matriz2 y mi_data_frame. Es posible colocarle un nombre especial a cada uno de los elementos de la lista, en este ejemplo se colocaron los nombres E1, E2 y E3 para cada uno de los tres elementos. Para observar lo que quedó almacenado en la lista se escribe milista en la consola y el resultado se muestra a continuación. milista ## $E1 ## [1] 0.7209039 0.8757732 0.7609823 0.8861246 0.4564810 ## ## $E2 ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 1 3 5 7 9 11 ## [2,] 2 4 6 8 10 12 ## ## $E3 ## edad deporte sexo ## 1 15 TRUE Hombre ## 2 19 TRUE Mujer ## 3 13 NA Hombre ## 4 NA FALSE Hombre ## 5 20 TRUE Mujer 2.4.7.1 ¿Cómo extraer elementos de una lista? Para recuperar los elementos almacenadas en una lista se usa el operador $, corchetes dobles [[]] o corchetes sencillos []. A continuación unos ejemplos para entender cómo extraer elementos de una lista. Ejemplos Si queremos la matriz almacenada con el nombre de E2 dentro del objeto milista se puede usar el siguiente código. milista$E2 ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 1 3 5 7 9 11 ## [2,] 2 4 6 8 10 12 Es posible indicar la posición del objeto en lugar del nombre, para eso se usan los corchetes dobles. milista[[2]] ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 1 3 5 7 9 11 ## [2,] 2 4 6 8 10 12 El resultado obtenido con milista$E2 y milista[[2]] es exactamente el mismo. Vamos ahora a solicitar la posición 2 pero usando corchetes sencillos. milista[2] ## $E2 ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 1 3 5 7 9 11 ## [2,] 2 4 6 8 10 12 La apariencia de este último resultado es similar, no igual, al encontrado al usar $ y [[]]. Para ver la diferencia vamos a pedir la clase a la que pertenecen los tres últimos objetos usando la función class. A continuación el código usado. class(milista$E2) ## [1] &quot;matrix&quot; &quot;array&quot; class(milista[[2]]) ## [1] &quot;matrix&quot; &quot;array&quot; class(milista[2]) ## [1] &quot;list&quot; De lo anterior se observa claramente que cuando usamos $ o [[]] el resultado es el objeto almacenado, una matriz. Cuando usamos [] el resultado es una lista cuyo contenido es el objeto almacenado. 2.4.8 Ejercicios Use funciones o procedimientos (varias líneas) de R para responder cada una de las siguientes preguntas. Construya un vector con la primeras 20 letras MAYÚSCULAS usando la función LETTERS. Construya una matriz de 10×10 con los primeros 100 números positivos pares. Construya una matriz identidad de dimensión 3×3. Recuerde que una matriz identidad tiene sólo unos en la diagonal principal y los demás elementos son cero. Construya una lista con los anteriores tres objetos creados. Construya un marco de datos o data frame con las respuestas de 5 personas de su trabajo a las preguntas: ¿Cuál es su nombre? ¿Cuál es su antigüedad en la empresa? ¿Cuál es su puesto? ¿Tiene usted algún producto contratado con la empresa? (Sí / No) ¿Cuál? ¿Cuál es el error al ejecutar el siguiente código? ¿A qué se debe? edad &lt;- c(15, 19, 13, NA, 20) deporte &lt;- c(TRUE, TRUE, NA, FALSE, TRUE) sexo &lt;- c(NA, &#39;Hombre&#39;, &#39;Hombre&#39;, NA, &#39;Mujer&#39;) matrix(edad, deporte, sexo) 2.5 Funciones básicas de R En este capítulo se presentará lo que es una función y se mostrarán varias funciones básicas que son útiles para realizar diversas tareas. 2.5.1 ¿Qué es una función de R? En la figura de abajo se muestra una ilustración de lo que es una función o máquina general. Hay unas entradas (inputs) que luego son procesadas dentro de la caja para generar unas salidas (outputs). Un ejemplo de una función o máquina muy común en nuestras casas es la licuadora. Si a una licuadora le ingresamos leche, fresas, azúcar y hielo, el resultado será un delicioso jugo de fresa. Las funciones en R se caracterizan por un nombre corto y que dé una idea de lo que hace la función. Los elementos que pueden ingresar (inputs) a la función se llaman parámetros o argumentos y se ubican dentro de paréntesis, el cuerpo de la función se ubica dentro de llaves y es ahí donde se procesan los inputs para convertirlos en outputs A continuación se muestra la estructura general para definir una función. nombre_de_funcion &lt;- function(parametro1, parametro2, ...) { tareas internas tareas internas tareas internas return(salida) } Cuando usamos una función sólo debemos escribir bien el nombre e ingresar correctamente los parámetros de la función, el cuerpo de la función ni lo vemos ni lo debemos modificar. A continuación se presenta un ejemplo de cómo usar la función mean para calcular un promedio. notas &lt;- c(4.0, 1.3, 3.8, 2.0) # Notas de un estudiante mean(notas) ## [1] 2.775 2.5.2 Operaciones básicas En R se pueden hacer diversas operaciones usando operadores binarios. Este tipo de operadores se denomina binarios porque actúan entre dos objetos, a continuación el listado. + operador binario para sumar. - operador binario para restar. * operador binario para multiplicar. / operador binario para dividir. ^ operador binario para potencia. %/% operador binario para obtener el cociente en una división (número entero). %% operador binario para obtener el residuo en una división. A continuación se presentan ejemplos de cómo usar las anteriores funciones. 6 + 4 # Para sumar dos números ## [1] 10 a &lt;- c(1, 3, 2) b &lt;- c(2, 0, 1) # a y b de la misma dimensión a + b # Para sumar los vectores a y b miembro a miembro ## [1] 3 3 3 a - b # Para restar dos vectores a y b miembro a miembro ## [1] -1 3 1 a * b # Para multiplicar ## [1] 2 0 2 a / b # Para dividir ## [1] 0.5 Inf 2.0 a ^ b # Para potencia ## [1] 1 1 2 7 %/% 3 # Para saber las veces que cabe 3 en 7 ## [1] 2 7 %% 3 # Para saber el residuo al dividir 7 entre 3 ## [1] 1 2.5.3 Pruebas lógicas En R se puede verificar si un objeto cumple una condición dada, a continuación el listado de las pruebas usuales. &lt; para saber si un número es menor que otro. &gt; para saber si un número es mayor que otro. == para saber si un número es igual que otro. &lt;= para saber si un número es menor o igual que otro. &gt;= para saber si un número es mayor o igual que otro. A continuación se presentan ejemplos de cómo usar las anteriores funciones. 5 &lt; 12 # ¿Será 5 menor que 12? ## [1] TRUE # Comparando objetos x &lt;- 5 y &lt;- 20 / 4 x == y # ¿Será x igual a y? ## [1] TRUE # Usando vectores a &lt;- c(1, 3, 2) b &lt;- c(2, 0, 1) a &gt; b # Comparación término a término ## [1] FALSE TRUE TRUE a == b # Comparación de igualdad término a término ## [1] FALSE FALSE FALSE 2.5.4 Operadores lógicos En R están disponibles los operadores lógicos negación, conjunción y disyunción. A continuación el listado de los operadores entre los elementos x e y. !x # Negación de x x &amp; y # Conjunción entre x e y x &amp;&amp; y x | y # Disyunción entre x e y x || y xor(x, y) A continuación se presentan ejemplos de cómo usar el símbolo de negación !. ans &lt;- c(TRUE, FALSE, TRUE) !ans # Negando las respuestas almacenadas en ans ## [1] FALSE TRUE FALSE x &lt;- c(5, 1.5, 2, 3, 2) !(x &lt; 2.5) # Negando los resultados de una prueba ## [1] TRUE FALSE FALSE TRUE FALSE A continuación se presentan ejemplos de cómo aplicar la conjunción &amp; y &amp;&amp;. x &lt;- c(5, 1.5, 2) # Se construyen dos vectores para la prueba y &lt;- c(4, 6, 3) x &lt; 4 # ¿Serán los elementos de x menores que 4? ## [1] FALSE TRUE TRUE y &gt; 5 # ¿Serán los elementos de y mayores que 5? ## [1] FALSE TRUE FALSE x &lt; 4 &amp; y &gt; 5 # Conjunción entre las pruebas anteriores. ## [1] FALSE TRUE FALSE x &lt; 4 &amp;&amp; y &gt; 5 # Conjunción vectorial ## [1] FALSE Note las diferencias entre los dos últimos ejemplos, cuando se usa &amp; se hace una prueba término a término y el resultado es un vector, cuando se usa &amp;&amp; se aplica la conjunción al vector de resultados obtenido con &amp;. 2.5.5 Funciones sobre vectores En R podemos destacar las siguientes funciones básicas sobre vectores numéricos. min: para obtener el mínimo de un vector. max: para obtener el máximo de un vector. length: para determinar la longitud de un vector. range: para obtener el rango de valores de un vector, entrega el mínimo y máximo. sum: entrega la suma de todos los elementos del vector. prod: multiplica todos los elementos del vector. which.min: nos entrega la posición en donde está el valor mínimo del vector. which.max: nos da la posición del valor máximo del vector. rev: invierte un vector. Ejemplo Construir en vector llamado myvec con los siguientes elementos: 5, 3, 2, 1, 2, 0, NA, 0, 9, 6. Luego aplicar todas las funciones anteriores para verificar el funcionamiento de las mismas. myvec &lt;- c(5, 3, 2, 1, 2, 0, NA, 0, 9, 6) myvec ## [1] 5 3 2 1 2 0 NA 0 9 6 min(myvec) # Oops, no aparece el mínimo que es Cero. ## [1] NA min(myvec, na.rm=TRUE) # Usamos na.rm = TRUE para remover el NA ## [1] 0 max(myvec, na.rm=T) # Para obtener el valor máximo ## [1] 9 range(myvec, na.rm=T) # Genera min y max simultáneamente ## [1] 0 9 sum(myvec, na.rm=T) # La suma de los valores internos ## [1] 28 prod(myvec, na.rm=T) # El productor de los valores internos ## [1] 0 which.min(myvec) # Posición del valor mínimo 0 en el vector ## [1] 6 which.max(myvec) # Posición del valor máximo 9 en el vector ## [1] 9 De las dos últimas líneas podemos destacar lo siguiente: NO es necesario usar na.rm = TRUE para remover el NA dentro de las funciones which.min ni which.max. El valor mínimo 0 aparece en las posicione 2.5.6 Función rep En R podemos crear repeticiones usando la función rep, la estructura de esta función es: rep(x, times=1, length.out=NA, each=1) Los argumentos de esta función son: x: vector con los elementos a repetir. times: número de veces que el vector x se debe repetir. length.out: longitud deseada para el vector resultante. each: número de veces que cada elemento de x se debe repetir. Ejemplo Construya las siguientes repeticiones usando la función rep, no lo haga ingresando número por número. 1 2 3 4 1 2 3 4 1 1 2 2 3 3 4 4 1 1 2 3 3 4 1 1 2 2 3 3 4 4 La clave para construir una repetición es, descubrir la semilla o elemento que se repite. Las instrucciones para obtener las repeticiones anteriores se muestra a continuación. rep(x=1:4, times=2) ## [1] 1 2 3 4 1 2 3 4 rep(x=1:4, times=c(2,2,2,2)) ## [1] 1 1 2 2 3 3 4 4 rep(x=1:4, times=c(2,1,2,1)) ## [1] 1 1 2 3 3 4 rep(x=1:4, each=2) ## [1] 1 1 2 2 3 3 4 4 2.5.7 Función seq En R podemos crear secuencias de números de una forma sencilla usando la función seq, la estructura de esta función es: seq(from=1, to=1, by, length.out) Los argumentos de esta función son: from: valor de inicio de la secuencia. to: valor de fin de la secuencia, no siempre se alcanza. by: incremento de la secuencia. length.out: longitud deseado de la secuencia. Ejemplo Construya las siguientes tres secuencias usando la función seq. Once valores igualmente espaciados desde 0 hasta 1. Una secuencia de dos en dos comenzando en 1. Una secuencia desde 1 con un salto de \\(\\pi\\) y sin pasar del número 9. El código necesario para obtener las secuencias se muestra a continuación. seq(from=0, to=1, length.out = 11) ## [1] 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 seq(from=1, to=9, by=2) # concuerda con final ## [1] 1 3 5 7 9 seq(from=1, to=9, by=pi) # se mantiene por debajo del final ## [1] 1.000000 4.141593 7.283185 En R existe el operador binario : que sirve para construir secuencias de uno en uno fácilmente. Revise los siguientes ejemplos para entender el funcionamiento del operador :. 2:8 ## [1] 2 3 4 5 6 7 8 3:-5 ## [1] 3 2 1 0 -1 -2 -3 -4 -5 pi:6 # secuencia real ## [1] 3.141593 4.141593 5.141593 6:pi # secuencia entera ## [1] 6 5 4 2.5.8 EJERCICIOS Use funciones o procedimientos (varias líneas) de R para responder (al menos) a 15 de las siguientes preguntas. ¿Qué cantidad de dinero sobra al repartir $10,000 entre 3 personas? ¿Es el número 4,560 divisible por 3? Construya un vector con los números enteros del 2 al 87. ¿Cuáles de esos números son divisibles por 7? Construya dos vectores, el primero con los números enteros desde 7 hasta 3, el segundo vector con los primeros cinco números positivos divisibles por 5. Sea A la condición de ser par en el primer vector. Sea B la condición de ser mayor que 10 en el segundo vector. ¿En cuál de las 5 posiciones se cumple A y B simultáneamente? Construya un vector con los siguientes elementos: 1, -4, 5, 9, -4. Escriba un procedimiento para extraer las posiciones donde está el valor mínimo en el vector. Calcular \\(8!\\) Evaluar la siguiente suma \\(\\sum_{i=3}^{i=7}e^i\\) Evaluar el siguiente producto \\(\\prod_{i=1}^{i=10}\\log\\sqrt{i}\\) Construya un vector cualquiera e inviértalo, es decir, que el primer elemento quede de último, el segundo de penúltimo y así sucesivamente. Compare su resultado con el de la función rev. Crear el vector: \\(1, 2, 3, \\ldots, 19, 20\\). Crear el vector: \\(20, 19, \\ldots , 2, 1\\). Crear el vector: \\(1, -2, 3, -4, 5, -6, \\ldots, 19, -20\\). Crear el vector: \\(0.1^3, 0.2^1, 0.1^6, 0.2^4, . . . , 0.1^{36}, 0.2^{34}\\). Calcular lo siguiente: \\(\\sum_{i=10}^{100}(i^3+4i^2)\\) y \\(\\sum_{i=1}^{25}\\left( \\frac{2^i}{i} + \\frac{3^i}{i^2} \\right)\\). En R hay unas bases de datos incluidas, una de ellas es la base de datos llamada mtcars. Para conocer las variables que están en mtcars usted puede escribir en la consola ?mtcars o también help(mtcars). De la base mtcars obtenga bases de datos que cumplan las siguientes condiciones. Autos que tengan un rendimiento menor a 18 millas por galón de combustible. Autos que tengan 4 cilindros. Autos que pesen más de 2500 libras y tengan transmisión manual. 2.6 Estructuras de control En R se disponen de varias instrucciones de control para facilitar los procedimientos que un usuario debe realizar. A continuación se explican esas instrucciones de control. 2.6.1 Instrucción if Esta instrucción sirve para realizar un conjunto de operaciones si se cumple cierta condición. A continuación se muestra la estructura básica de uso. if (condicion) { operación 1 operación 2 ... operación final } Ejemplo Una secretaria recibe la información del salario básico semanal de un empleado y las horas trabajadas durante la semana por ese empleado. El salario básico es la remuneración por 40 horas de labor por semana, las horas extra son pagadas a 150 pesos. Escriba el procedimiento en R que debe usar la secretaria para calcular el salario semanal de un empleado que trabajó 45 horas y tiene salario básico de 5 mil pesos. El código para calcular el salario final del empleado es el siguiente: sal &lt;- 5000 # Salario básico por semana hlab &lt;- 45 # Horas laboradas por semana if(hlab &gt; 40) { hext &lt;- hlab - 40 salext &lt;- hext * 150 sal &lt;- sal + salext } sal # Salario semanal ## [1] 5750 2.6.2 Instrucción if else Esta instrucción sirve para realizar un conjunto de operaciones cuando NO se cumple cierta condición evaluada por un if. A continuación se muestra la estructura básica de uso. if (condicion) { operación 1 operación 2 ... operación final } else { operación 1 operación 2 ... operación final } Ejemplo sal &lt;- 5000 # Salario básico por semana hlab &lt;- 40 # Horas laboradas por semana if (hlab &gt; 40) { hext &lt;- hlab - 40 salext &lt;- hext * 150 sal &lt;- sal + salext } else { h_faltantes &lt;- 40 - hlab sueldo_sobrante &lt;- h_faltantes * sal/40 sal &lt;- sal - sueldo_sobrante } # Cálculo de salario por horas trabajadas sal ## [1] 5000 2.6.3 Instrucción ifelse Se recomienda usar la instrucción ifelse cuando hay una sola instrucción para el caso if y para el caso else. A continuación se muestra la estructura básica de uso. ifelse(condición, operación SI cumple, operación NO cumple) Ejemplo Suponga que usted recibe un vector de números enteros, escriba un procedimiento que diga si cada elemento del vector es par o impar. x &lt;- c(5, 3, 2, 8, -4, 1) ifelse(x %% 2 == 0, &#39;Es par&#39;, &#39;Es impar&#39;) ## [1] &quot;Es impar&quot; &quot;Es impar&quot; &quot;Es par&quot; &quot;Es par&quot; &quot;Es par&quot; &quot;Es impar&quot; 2.6.4 Instrucción else if En caso de querer actuar de forma distinta dependiendo de la condición, puede especificarse más de una condición de la siguiente manera: sal &lt;- 5000 # Salario básico por semana hlab &lt;- 20 # Horas laboradas por semana if (hlab &gt; 40) { hext &lt;- hlab - 40 salext &lt;- hext * 150 sal &lt;- sal + salext print(paste0(&quot;Pago semanal: $&quot;, sal)) } else if (hlab &lt; 16) { h_faltantes &lt;- 40 - hlab sueldo_sobrante &lt;- h_faltantes * sal/40 sal &lt;- sal - 1.05 * sueldo_sobrante print(&quot;despedido&quot;) print(paste0(&quot;liquidación: $&quot;, sal)) } else { h_faltantes &lt;- 40 - hlab sueldo_sobrante &lt;- h_faltantes * sal/40 sal &lt;- sal - sueldo_sobrante print(paste0(&quot;Pago con descuento: $&quot;, sal)) }# Cálculo de salario por horas trabajadas ## [1] &quot;Pago con descuento: $2500&quot; Otro caso: sal &lt;- 5000 # Salario básico por semana hlab &lt;- 10 # Horas laboradas por semana if (hlab &gt; 40) { hext &lt;- hlab - 40 salext &lt;- hext * 150 sal &lt;- sal + salext print(paste0(&quot;Pago semanal: $&quot;, sal)) } else if (hlab &lt; 16) { h_faltantes &lt;- 40 - hlab sueldo_sobrante &lt;- h_faltantes * sal/40 sal &lt;- sal - 1.05 * sueldo_sobrante print(&quot;¡Empleado despedido! :O&quot;) print(paste0(&quot;liquidación: $&quot;, sal)) } else { h_faltantes &lt;- 40 - hlab sueldo_sobrante &lt;- h_faltantes * sal/40 sal &lt;- sal - sueldo_sobrante print(paste0(&quot;Pago con descuento: $&quot;, sal)) }# Cálculo de salario por horas trabajadas ## [1] &quot;¡Empleado despedido! :O&quot; ## [1] &quot;liquidación: $1062.5&quot; 2.6.5 Instrucción for La instrucción for es muy útil para repetir un procedimiento cierta cantidad de veces. A continuación se muestra la estructura básica de uso. for (i in secuencia) { operación 1 operación 2 ... operación final } Ejemplo Escriba un procedimiento para crear 10 muestras de tamaño 100 de una distribución uniforme entre uno y tres. Para cada una de las muestra, se debe contar el número de elementos de la muestra que fueron mayores o iguales a 2.5. nrep &lt;- 10 # Número de repeticiones n &lt;- 100 # Tamaño de la muestra conteo &lt;- numeric(nrep) # Vector para almacenar el conteo for (i in 1:nrep) { x &lt;- runif(n=n, min=1, max=3) conteo[i] &lt;- sum(x &gt;= 2.5) } conteo # Para obtener el conteo ## [1] 24 37 28 26 30 18 29 23 19 19 2.6.6 Instrucción while La instrucción while es muy útil para repetir un procedimiento siempre que se cumple una condición. A continuación se muestra la estructura básica de uso. while (condición) { operación 1 operación 2 ... operación final } Ejemplo Suponga que se lanza una moneda en la cual el resultado es cara o cruz. Escribir un procedimiento que simule lanzamientos hasta que el número de caras obtenidas sea 5. El procedimiento debe entregar el historial de lanzamientos. Para simular el lanzamiento de una moneda se puede usar la función sample y definiendo el vector resultados con size=1 para simular un lanzamiento, a continuación el código y tres pruebas ilustrativas. resultados &lt;- c(&#39;Cara&#39;, &#39;Cruz&#39;) sample(x=resultados, size=1) # Prueba 1 ## [1] &quot;Cruz&quot; Una vez seamos capaces de simular un lanzamiento podemos escribir el procedimiento para generar tantos lanzamientos hasta que se cumpla la condición. El código mostrado abajo permite hacer lo solicitado. num.lanza &lt;- 0 # Contador de lanzamientos num.caras &lt;- 0 # Contados de caras obtenidas historial &lt;- NULL # Vector vacío para almacenar while (num.caras &lt; 5) { res &lt;- sample(x=resultados, size=1) num.lanza &lt;- num.lanza + 1 historial[num.lanza] &lt;- res if (res == &#39;Cara&#39;) { num.caras &lt;- num.caras + 1 } } historial ## [1] &quot;Cruz&quot; &quot;Cruz&quot; &quot;Cruz&quot; &quot;Cruz&quot; &quot;Cara&quot; &quot;Cara&quot; &quot;Cruz&quot; &quot;Cruz&quot; &quot;Cara&quot; &quot;Cara&quot; ## [11] &quot;Cara&quot; num.lanza ## [1] 11 La instrucción for se usa cuando sabemos el número de veces que se debe repetir el procedimiento, mientras que la instrucción while se usa cuando debemos repetir un procedimiento cuando se cumpla una condición. 2.6.7 Instrucción repeat La instrucción while es muy útil para repetir un procedimiento siempre que se cumple una condición. A continuación se muestra la estructura básica de uso. repeat { operación 1 operación 2 ... operación final if (condición) break } Ejemplo Escribir un procedimiento para ir aumentando de uno en uno el valor de x hasta que x sea igual a siete El procedimiento debe imprimir por pantalla la secuencia de valores de x. x &lt;- 3 # Valor de inicio repeat { print(x) x &lt;- x + 1 if (x == 8) { break } } ## [1] 3 ## [1] 4 ## [1] 5 ## [1] 6 ## [1] 7 La instrucción break sirve para salir de un procedimiento iterativo. 2.7 Guía de estilo Así como en el español existen reglas ortográficas, la escritura de códigos en R también tiene unas reglas que se recomienda seguir para evitar confusiones. Tener una buena guía de estilo es importante para que el código creado por usted sea fácilmente entendido por sus lectores. No existe una única y mejor guía de estilo para escritura en R, sin embargo aquí vamos a mostrar unas sugerencias basadas en la guía llamada The tidyverse style guidee. 2.7.1 Nombres de los archivos Se sugiere que el nombre usado para nombrar un archivo tenga sentido y que termine con extensión “.R”. A continuación dos ejemplos de como nombrar bien y mal un archivo. Bien: \"2020-analisis_exploratorio.R Mal: ju89HR56_74.R 2.7.2 Nombres de los objetos Se recomienda usar los símbolos _ dentro de los nombres de objetos. Para las variables es preferible usar letras minúsculas (pesomaiz o peso_maiz) o utilizar la notación camello iniciando en minúscula (pesoMaiz). Para las funciones se recomienda usar la notación camello iniciando todas la palabras en mayúscula (PlotRes). Para los nombres de las constantes se recomienda que inicien con la letra k (kPrecioBus). 2.7.3 Longitud de una línea de código Se recomienda que cada línea tenga como máximo 80 caracteres. Si una línea es muy larga se debe cortar siempre por una coma. 2.7.4 Espacios Use espacios alrededor de todos los operadores binarios (=, +, -, &lt;-, etc.). Los espacios alrededor del símbolo = son opcionales cuando se usan para ingresar valores dentro de una función. Así como en español, nunca coloque espacio antes de una coma, pero siempre use espacio luego de una coma. A continuación ejemplos de buenas y malas prácticas. tab &lt;- table(df[df$days &lt; 0, 2]) # Bien tot &lt;- sum(x[, 1]) # Bien tot &lt;- sum(x[1, ]) # Bien tab &lt;- table(df[df$days&lt;0, 2]) # Faltan espacios alrededor &#39;&lt;&#39; tab &lt;- table(df[df$days &lt; 0,2]) # Falta espacio luego de coma tab &lt;- table(df[df$days &lt; 0 , 2]) # Sobra espacio antes de coma tab&lt;- table(df[df$days &lt; 0, 2]) # Falta espacio antes de &#39;&lt;-&#39; tab&lt;-table(df[df$days &lt; 0, 2]) # Falta espacio alrededor de &#39;&lt;-&#39; tot &lt;- sum(x[,1]) # Falta espacio luego de coma tot &lt;- sum(x[1,]) # Falta espacio luego de coma Otra buena práctica es colocar espacio antes de un paréntesis excepto cuando se llama una función. if (debug) # Correcto if(debug) # Funciona pero no se recomienda colMeans (x) # Funciona pero no se recomienda Espacios extras pueden ser usados si con esto se mejora la apariencia del código, ver el ejemplo siguiente. plot(x = x.coord, y = data.mat[, MakeColName(metric, ptiles[1], &quot;roiOpt&quot;)], ylim = ylim, xlab = &quot;dates&quot;, ylab = metric, main = (paste(metric, &quot; for 3 samples &quot;, sep = &quot;&quot;))) No coloque espacios alrededor del código que esté dentro de paréntesis ( ) o corchetes [ ], la única excepción es luego de una coma, ver el ejemplo siguiente. if (condicion) # Correcto x[1, ] # Correcto if ( condicion ) # Sobran espacios alrededor de condición x[1,] # Se necesita espacio luego de coma Los signos de agrupación llaves { } se utilizan para agrupar bloques de código y se recomienda que nunca una llave abierta { esté sola en una línea; una llave cerrada } si debe ir sola en su propia línea. Se pueden omitir las llaves cuando el bloque de instrucciones esté formado por una sola línea pero esa línea de código NO debe ir en la misma línea de la condición. A continuación dos ejemplos de lo que se recomienda. if (is.null(ylim)) { # Correcto ylim &lt;- c(0, 0.06) } if (is.null(ylim)) # Correcto ylim &lt;- c(0, 0.06) if (is.null(ylim)) ylim &lt;- c(0, 0.06) # Aceptable if (is.null(ylim)) # No se recomienda { ylim &lt;- c(0, 0.06) } if (is.null(ylim)) {ylim &lt;- c(0, 0.06)} # Frente a la llave { no debe ir nada # la llave de cierre } debe ir sola La sentencia else debe ir siempre entre llaves } {, ver el siguiente ejemplo. if (condition) { one or more lines } else { # Correcto one or more lines } if (condition) { one or more lines } else { # Incorrecto one or more lines } if (condition) one line else # Incorrecto one line 2.7.5 Asignación Para realizar asignaciones se recomienda usar el símbolo &lt;-, el símbolo de igualdad = no se recomienda usarlo para asignaciones. x &lt;- 5 # Correcto x = 5 # No recomendado Para una explicación más detallada sobre el símbolo de asignación se recomienda visitar este enlace. 2.7.6 Punto y coma No se recomienda colocar varias instrucciones separadas por ; en la misma línea, aunque funciona dificulta la revisión del código. n &lt;- 100; y &lt;- rnorm(n, mean=5); hist(y) # No se recomienda n &lt;- 100 # Correcto y &lt;- rnorm(n, mean=5) hist(y) A pesar de la anterior advertencia es posible que en este libro usemos el ; en algunas ocasiones, si lo hacemos es para ahorrar espacio en la presentación del código. "],["tidyverse.html", "Capítulo 3 Tidyverse 3.1 Lectura de archivos 3.2 Consultas de datos 3.3 Orden y estructura 3.4 Manipulación de texto 3.5 Manipulación de tiempo 3.6 Iteraciones", " Capítulo 3 Tidyverse La compañía Rstudio ha desarrollado un conjunto de librerías que revolucionó la programación en R. Este conjunto de librerías permite al usuario mayor orden, legibilidad e intuición a la hora de escribir y leer código. El conjunto de librerías lleva por nombre: TIDYVERSE. En este capítulo se estudiarán las distintas librerías que componen este conjunto. Cada una de las librerías puede usarse de modo independiente. En caso de que el usuario lo prefiera, puede disponer de todas las librerías al mandar ejecutar la función: library(tidyverse) 3.1 Lectura de archivos Usualmente, no creamos los datos desde la sesión de R, sino que a través de un archivo externo o una base de datos se realiza la lectura de datos. Los más comunes son: 3.1.1 Archivos csv A la hora de importar conjuntos de datos en R, uno de los formatos más habituales en los que hallamos información es en archivos separados por comas (comma separated values), cuya extensión suele ser .csv. En ellos encontramos múltiples líneas que recogen la tabla de interés, y en las cuales los valores aparecen, de manera consecutiva, separados por el carácter ,. Para importar este tipo de archivos en nuestra sesión de R, se utiliza la función read_csv(). Para acceder a su documentación utilizamos el comando ?read_csv. El único argumento que debemos de pasar a esta función de manera obligatoria, es file, el nombre o la ruta completa del archivo que pretendemos importar. library(readr) read_csv( file, col_names = TRUE, col_types = NULL, locale = default_locale(), na = c(&quot;&quot;, &quot;NA&quot;), quoted_na = TRUE, quote = &quot;\\&quot;&quot;, comment = &quot;&quot;) La paquetería readr fue desarrollada recientemente para lidiar con la lectura de archivos grandes rápidamente. El paquete proporciona reemplazos para funciones como read.table(), read.csv() entre otras. Esta paquetería proporciona funciones que suelen ser mucho más rápidas que las funciones base que proporciona R. Ventajas de readr: Por lo general, son mucho más rápidos (~ 10x) que sus funciones equivalentes. Producen tibbles: No convierten vectores de caracteres en factores. No usan nombres de filas ni modifican los nombres de columnas. Reproducibilidad No convierte, automáticamente, las columnas con cadenas de caracteres a factores, como sí hacen por defecto las otras funciones base de R. Reconoce ocho clases diferentes de datos (enteros, lógicos, etc.), dejando el resto como cadenas de caracteres. Veamos un ejemplo: La base de datos llamada AmesHousing contiene un conjunto de datos con información de la Oficina del Tasador de Ames utilizada para calcular los valores tasados para las propiedades residenciales individuales vendidas en Ames, Iowa, de 2006 a 2010. FUENTES: Ames, Oficina del Tasador de Iowa. Pueden descargar los datos para la clase aquí base &lt;- read.csv(&quot;data/ames.csv&quot;) head(base, 2) ## MS_SubClass MS_Zoning Lot_Frontage ## 1 One_Story_1946_and_Newer_All_Styles Residential_Low_Density 141 ## 2 One_Story_1946_and_Newer_All_Styles Residential_High_Density 80 ## Lot_Area Street Alley Lot_Shape Land_Contour Utilities ## 1 31770 Pave No_Alley_Access Slightly_Irregular Lvl AllPub ## 2 11622 Pave No_Alley_Access Regular Lvl AllPub ## Lot_Config Land_Slope Neighborhood Condition_1 Condition_2 Bldg_Type ## 1 Corner Gtl North_Ames Norm Norm OneFam ## 2 Inside Gtl North_Ames Feedr Norm OneFam ## House_Style Overall_Cond Year_Built Year_Remod_Add Roof_Style Roof_Matl ## 1 One_Story Average 1960 1960 Hip CompShg ## 2 One_Story Above_Average 1961 1961 Gable CompShg ## Exterior_1st Exterior_2nd Mas_Vnr_Type Mas_Vnr_Area Exter_Cond Foundation ## 1 BrkFace Plywood Stone 112 Typical CBlock ## 2 VinylSd VinylSd None 0 Typical CBlock ## Bsmt_Cond Bsmt_Exposure BsmtFin_Type_1 BsmtFin_SF_1 BsmtFin_Type_2 ## 1 Good Gd BLQ 2 Unf ## 2 Typical No Rec 6 LwQ ## BsmtFin_SF_2 Bsmt_Unf_SF Total_Bsmt_SF Heating Heating_QC Central_Air ## 1 0 441 1080 GasA Fair Y ## 2 144 270 882 GasA Typical Y ## Electrical First_Flr_SF Second_Flr_SF Gr_Liv_Area Bsmt_Full_Bath ## 1 SBrkr 1656 0 1656 1 ## 2 SBrkr 896 0 896 0 ## Bsmt_Half_Bath Full_Bath Half_Bath Bedroom_AbvGr Kitchen_AbvGr TotRms_AbvGrd ## 1 0 1 0 3 1 7 ## 2 0 1 0 2 1 5 ## Functional Fireplaces Garage_Type Garage_Finish Garage_Cars Garage_Area ## 1 Typ 2 Attchd Fin 2 528 ## 2 Typ 0 Attchd Unf 1 730 ## Garage_Cond Paved_Drive Wood_Deck_SF Open_Porch_SF Enclosed_Porch ## 1 Typical Partial_Pavement 210 62 0 ## 2 Typical Paved 140 0 0 ## Three_season_porch Screen_Porch Pool_Area Pool_QC Fence ## 1 0 0 0 No_Pool No_Fence ## 2 0 120 0 No_Pool Minimum_Privacy ## Misc_Feature Misc_Val Mo_Sold Year_Sold Sale_Type Sale_Condition Sale_Price ## 1 None 0 5 2010 WD Normal 215000 ## 2 None 0 6 2010 WD Normal 105000 ## Longitude Latitude ## 1 -93.61975 42.05403 ## 2 -93.61976 42.05301 tidy &lt;- read_csv(&quot;data/ames.csv&quot;) head(tidy, 2) ## # A tibble: 2 × 74 ## MS_SubC…¹ MS_Zo…² Lot_F…³ Lot_A…⁴ Street Alley Lot_S…⁵ Land_…⁶ Utili…⁷ Lot_C…⁸ ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 One_Stor… Reside… 141 31770 Pave No_A… Slight… Lvl AllPub Corner ## 2 One_Stor… Reside… 80 11622 Pave No_A… Regular Lvl AllPub Inside ## # … with 64 more variables: Land_Slope &lt;chr&gt;, Neighborhood &lt;chr&gt;, ## # Condition_1 &lt;chr&gt;, Condition_2 &lt;chr&gt;, Bldg_Type &lt;chr&gt;, House_Style &lt;chr&gt;, ## # Overall_Cond &lt;chr&gt;, Year_Built &lt;dbl&gt;, Year_Remod_Add &lt;dbl&gt;, ## # Roof_Style &lt;chr&gt;, Roof_Matl &lt;chr&gt;, Exterior_1st &lt;chr&gt;, Exterior_2nd &lt;chr&gt;, ## # Mas_Vnr_Type &lt;chr&gt;, Mas_Vnr_Area &lt;dbl&gt;, Exter_Cond &lt;chr&gt;, Foundation &lt;chr&gt;, ## # Bsmt_Cond &lt;chr&gt;, Bsmt_Exposure &lt;chr&gt;, BsmtFin_Type_1 &lt;chr&gt;, ## # BsmtFin_SF_1 &lt;dbl&gt;, BsmtFin_Type_2 &lt;chr&gt;, BsmtFin_SF_2 &lt;dbl&gt;, … ¿Y si el archivo que necesitamos leer esta en excel? 3.1.2 Archivos txt Uno de los archivos más comunes es el .txt. La librería readr también cuenta con funciones que permiten leer fácilmente los datos contenidos en formato tabular. ames_txt &lt;- read_delim(&quot;data/ames.txt&quot;, delim = &quot;;&quot;, col_names = TRUE) head(ames_txt, 2) ## # A tibble: 2 × 74 ## MS_SubC…¹ MS_Zo…² Lot_F…³ Lot_A…⁴ Street Alley Lot_S…⁵ Land_…⁶ Utili…⁷ Lot_C…⁸ ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 One_Stor… Reside… 141 31770 Pave No_A… Slight… Lvl AllPub Corner ## 2 One_Stor… Reside… 80 11622 Pave No_A… Regular Lvl AllPub Inside ## # … with 64 more variables: Land_Slope &lt;chr&gt;, Neighborhood &lt;chr&gt;, ## # Condition_1 &lt;chr&gt;, Condition_2 &lt;chr&gt;, Bldg_Type &lt;chr&gt;, House_Style &lt;chr&gt;, ## # Overall_Cond &lt;chr&gt;, Year_Built &lt;dbl&gt;, Year_Remod_Add &lt;dbl&gt;, ## # Roof_Style &lt;chr&gt;, Roof_Matl &lt;chr&gt;, Exterior_1st &lt;chr&gt;, Exterior_2nd &lt;chr&gt;, ## # Mas_Vnr_Type &lt;chr&gt;, Mas_Vnr_Area &lt;dbl&gt;, Exter_Cond &lt;chr&gt;, Foundation &lt;chr&gt;, ## # Bsmt_Cond &lt;chr&gt;, Bsmt_Exposure &lt;chr&gt;, BsmtFin_Type_1 &lt;chr&gt;, ## # BsmtFin_SF_1 &lt;dbl&gt;, BsmtFin_Type_2 &lt;chr&gt;, BsmtFin_SF_2 &lt;dbl&gt;, … La función read_delim() funciona para leer archivos con diferentes delimitadores posibles, es decir, es posible especificar si las columnas están separadas por espacios, comas, punto y coma, tabulador o algún otro delimitador (““,”,“,”;“,”, “@”). Adicionalmente, se puede especificar si el archivo contiene encabezado, si existen renglones a saltar, codificación, tipo de variable y muchas más opciones. Todos estos detalles pueden consultarse en la documentación de ayuda. 3.1.3 Archivos xls y xlsx La paquetería readxl facilita la obtención de datos tabulares de archivos de Excel. Admite tanto el formato .xls heredado como el formato .xlsx moderno basado en XML. Esta paquetería pone a disposición las siguientes funciones: read_xlsx() lee un archivo con extensión xlsx. read_xlsx( path, sheet = NULL, range = NULL, col_names = TRUE, col_types = NULL, na = &quot;&quot;, trim_ws = TRUE, skip = 0, n_max = Inf, guess_max = min(1000, n_max), progress = readxl_progress(), .name_repair = &quot;unique&quot; ) read_xls() lee un archivo con extensión xls. read_xls( path, sheet = NULL, range = NULL, col_names = TRUE, col_types = NULL, na = &quot;&quot;, trim_ws = TRUE, skip = 0, n_max = Inf, guess_max = min(1000, n_max), progress = readxl_progress(), .name_repair = &quot;unique&quot; ) read_excel() determina si el archivo es de tipo xls o xlsx para después llamar a una de las funciones mencionadas anteriormente. read_excel( path, sheet = NULL, range = NULL, col_names = TRUE, col_types = NULL, na = &quot;&quot;, trim_ws = TRUE, skip = 0, n_max = Inf, guess_max = min(1000, n_max), progress = readxl_progress(), .name_repair = &quot;unique&quot; ) EJERCICIO: Leer archivo excel de la carpeta del curso 3.1.4 Archivos json Se utiliza la función fromJSON de la paquetería jsonlite library(jsonlite) base_json &lt;- jsonlite::fromJSON(&quot;data/ames.json&quot;) head(base_json, 2) ## MS_SubClass MS_Zoning Lot_Frontage ## 1 One_Story_1946_and_Newer_All_Styles Residential_Low_Density 141 ## 2 One_Story_1946_and_Newer_All_Styles Residential_High_Density 80 ## Lot_Area Street Alley Lot_Shape Land_Contour Utilities ## 1 31770 Pave No_Alley_Access Slightly_Irregular Lvl AllPub ## 2 11622 Pave No_Alley_Access Regular Lvl AllPub ## Lot_Config Land_Slope Neighborhood Condition_1 Condition_2 Bldg_Type ## 1 Corner Gtl North_Ames Norm Norm OneFam ## 2 Inside Gtl North_Ames Feedr Norm OneFam ## House_Style Overall_Cond Year_Built Year_Remod_Add Roof_Style Roof_Matl ## 1 One_Story Average 1960 1960 Hip CompShg ## 2 One_Story Above_Average 1961 1961 Gable CompShg ## Exterior_1st Exterior_2nd Mas_Vnr_Type Mas_Vnr_Area Exter_Cond Foundation ## 1 BrkFace Plywood Stone 112 Typical CBlock ## 2 VinylSd VinylSd None 0 Typical CBlock ## Bsmt_Cond Bsmt_Exposure BsmtFin_Type_1 BsmtFin_SF_1 BsmtFin_Type_2 ## 1 Good Gd BLQ 2 Unf ## 2 Typical No Rec 6 LwQ ## BsmtFin_SF_2 Bsmt_Unf_SF Total_Bsmt_SF Heating Heating_QC Central_Air ## 1 0 441 1080 GasA Fair Y ## 2 144 270 882 GasA Typical Y ## Electrical First_Flr_SF Second_Flr_SF Gr_Liv_Area Bsmt_Full_Bath ## 1 SBrkr 1656 0 1656 1 ## 2 SBrkr 896 0 896 0 ## Bsmt_Half_Bath Full_Bath Half_Bath Bedroom_AbvGr Kitchen_AbvGr TotRms_AbvGrd ## 1 0 1 0 3 1 7 ## 2 0 1 0 2 1 5 ## Functional Fireplaces Garage_Type Garage_Finish Garage_Cars Garage_Area ## 1 Typ 2 Attchd Fin 2 528 ## 2 Typ 0 Attchd Unf 1 730 ## Garage_Cond Paved_Drive Wood_Deck_SF Open_Porch_SF Enclosed_Porch ## 1 Typical Partial_Pavement 210 62 0 ## 2 Typical Paved 140 0 0 ## Three_season_porch Screen_Porch Pool_Area Pool_QC Fence ## 1 0 0 0 No_Pool No_Fence ## 2 0 120 0 No_Pool Minimum_Privacy ## Misc_Feature Misc_Val Mo_Sold Year_Sold Sale_Type Sale_Condition Sale_Price ## 1 None 0 5 2010 WD Normal 215000 ## 2 None 0 6 2010 WD Normal 105000 ## Longitude Latitude ## 1 -93.6198 42.054 ## 2 -93.6198 42.053 3.1.5 Archivos rds Un tipo de archivo que resulta de particular interés, es el .RDS. Este archivo comprime cualquier objeto o resultado que sea usado o producido en R. Uno puede almacenar el objeto de interés de la siguiente manera: saveRDS(base_json, &quot;data/ames.rds&quot;) Puede observarse que en el explorador de archivos se encuentra ahora el nuevo archivo con extensión .rds, el cual puede ser posteriormente incorporado a una sesión de R para seguir trabajando con él. base_rds &lt;- readRDS(&quot;data/ames.rds&quot;) Algunas de las grandes ventajas que tiene almacenar los archivos en formato rds, son las siguientes: No es necesario volver a ejecutar procesos largos cuando ya se ha logrado realizar una vez. El tiempo de lectura de la información es considerablemente más rápido. 3.1.6 Bases de Datos En muchos de los casos la información estará dentro de un Sistema Manejador de Bases de Datos, existen bibliotecas que nos permiten establecer las conexiones con ellas, algunos ejemplos son: ODBC DBI JDBC Un ejemplo con un SMBD como Oracle: 3.1.7 Oracle Database Referencias Configuración de conexión: Se necesitan seis configuraciones para realizar una conexión: Controlador : consulte la sección Controladores para obtener más información Url : una ruta de red al servidor de la base de datos. Base de datos : el nombre de la base de datos. Usuario : el ID de red del usuario o la cuenta local del servidor Contraseña : la contraseña de la cuenta Puerto : debe establecerse en 1526 o 1521 Para establecer la conexión con la base de datos: library(DBI) library(RJDBC) jdbcDriver = JDBC(driverClass = &quot;oracle.jdbc.OracleDriver&quot;,&quot;c:/Drivers/Oracle/ojdbc8.jar&quot;) con &lt;- dbConnect( jdbcDriver, url = &quot;jdbc:oracle:thin:@//Hostname:Port/Service_Name&quot; user = rstudioapi::askForPassword(&quot;Database user&quot;), password = rstudioapi::askForPassword(&quot;Database password&quot;), dbname = &quot;Data Base Name&quot; ) sql_translation.JDBCConnection &lt;- dbplyr:::sql_translation.Oracle sql_select.JDBCConnection &lt;- dbplyr:::sql_query_select.Oracle sql_subquery.JDBCConnection &lt;- dbplyr:::sql_query_wrap.Oracle dbExistsTable(jdbcConnection, &quot;nombre_tabla&quot;) # Probar si hay conexión con la tabla Información sobre la base de datos: El paquete odbc le brinda herramientas para explorar objetos y columnas en la base de datos. # Top level objects odbcListObjects(con) # Tables in a schema odbcListObjects(con, catalog = &quot;mydb&quot;, schema = &quot;dbo&quot;) # Columns in a table odbcListColumns(con, catalog = &quot;mydb&quot;, schema = &quot;dbo&quot;, table = &quot;cars&quot;) # Database structure odbcListObjectTypes(con) Consultas con SQL: Para consultas interactivas, utilice dbGetQuery() para enviar una consulta y obtener los resultados. Para obtener los resultados por separado, utilice dbSendQuery() y dbFetch(). El argumento n en dbFetch() se puede utilizar para obtener resultados parciales. # Return the results for an arbitrary query dbGetQuery(con, &quot;SELECT speed, dist FROM cars&quot;) # Fetch the first 100 records query &lt;- dbSendQuery(con, &quot;SELECT speed, dist FROM cars&quot;) dbFetch(query, n = 10) dbClearResult(query) Puedes usar los ejemplos anteriores para probar con diferentes consultas y bases de datos. Tengamos un ejemplo de manera local: remotes::install_version(&quot;RSQLite&quot;) library(dplyr) library(dbplyr) library(RSQLite) con &lt;- src_memdb() copy_to(con, storms, overwrite = T) copy_to(con, mtcars, overwrite = T) tbl_storms &lt;- tbl(con, &quot;storms&quot;) tbl_storms ## # Source: table&lt;storms&gt; [?? x 13] ## # Database: sqlite 3.39.3 [:memory:] ## name year month day hour lat long status categ…¹ wind press…² ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 Amy 1975 6 27 0 27.5 -79 tropical dep… -1 25 1013 ## 2 Amy 1975 6 27 6 28.5 -79 tropical dep… -1 25 1013 ## 3 Amy 1975 6 27 12 29.5 -79 tropical dep… -1 25 1013 ## 4 Amy 1975 6 27 18 30.5 -79 tropical dep… -1 25 1013 ## 5 Amy 1975 6 28 0 31.5 -78.8 tropical dep… -1 25 1012 ## 6 Amy 1975 6 28 6 32.4 -78.7 tropical dep… -1 25 1012 ## 7 Amy 1975 6 28 12 33.3 -78 tropical dep… -1 25 1011 ## 8 Amy 1975 6 28 18 34 -77 tropical dep… -1 30 1006 ## 9 Amy 1975 6 29 0 34.4 -75.8 tropical sto… 0 35 1004 ## 10 Amy 1975 6 29 6 34 -74.8 tropical sto… 0 40 1002 ## # … with more rows, 2 more variables: tropicalstorm_force_diameter &lt;int&gt;, ## # hurricane_force_diameter &lt;int&gt;, and abbreviated variable names ¹​category, ## # ²​pressure tbl_mtcars &lt;- tbl(con, &quot;mtcars&quot;) tbl_mtcars ## # Source: table&lt;mtcars&gt; [?? x 11] ## # Database: sqlite 3.39.3 [:memory:] ## mpg cyl disp hp drat wt qsec vs am gear carb ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 21 6 160 110 3.9 2.62 16.5 0 1 4 4 ## 2 21 6 160 110 3.9 2.88 17.0 0 1 4 4 ## 3 22.8 4 108 93 3.85 2.32 18.6 1 1 4 1 ## 4 21.4 6 258 110 3.08 3.22 19.4 1 0 3 1 ## 5 18.7 8 360 175 3.15 3.44 17.0 0 0 3 2 ## 6 18.1 6 225 105 2.76 3.46 20.2 1 0 3 1 ## 7 14.3 8 360 245 3.21 3.57 15.8 0 0 3 4 ## 8 24.4 4 147. 62 3.69 3.19 20 1 0 4 2 ## 9 22.8 4 141. 95 3.92 3.15 22.9 1 0 4 2 ## 10 19.2 6 168. 123 3.92 3.44 18.3 1 0 4 4 ## # … with more rows Existe otra metodología de conexión, la cual puede encontrarse en la siguiente documentación 3.2 Consultas de datos Ahora que ya se ha estudiado la manera de cargar datos, aprenderemos como manipularlos con dplyr. El paquete dplyr proporciona un conjunto de funciones muy útiles para manipular data-frames y así reducir el número de repeticiones, la probabilidad de cometer errores y el número de caracteres que hay que escribir. Como valor extra, podemos encontrar que la gramática de dplyr es más fácil de entender. Revisaremos algunas de sus funciones más usadas (verbos), así como el uso de pipes (%&gt;%) para combinarlas. select() filter() arrange() mutate() summarise() join() group_by() Primero tenemos que instalar y cargar la paquetería (parte de tidyverse): # install.packages(&quot;dplyr&quot;) library(dplyr) library(readr) Usaremos el dataset AmesHousing que se proporcionó en el capítulo anterior (el alumno puede hacer el ejercicio con datos propios) ames_housing &lt;- read_csv(&quot;data/ames.csv&quot;) glimpse(ames_housing) ## Rows: 2,930 ## Columns: 74 ## $ MS_SubClass &lt;chr&gt; &quot;One_Story_1946_and_Newer_All_Styles&quot;, &quot;One_Story_1… ## $ MS_Zoning &lt;chr&gt; &quot;Residential_Low_Density&quot;, &quot;Residential_High_Densit… ## $ Lot_Frontage &lt;dbl&gt; 141, 80, 81, 93, 74, 78, 41, 43, 39, 60, 75, 0, 63,… ## $ Lot_Area &lt;dbl&gt; 31770, 11622, 14267, 11160, 13830, 9978, 4920, 5005… ## $ Street &lt;chr&gt; &quot;Pave&quot;, &quot;Pave&quot;, &quot;Pave&quot;, &quot;Pave&quot;, &quot;Pave&quot;, &quot;Pave&quot;, &quot;Pa… ## $ Alley &lt;chr&gt; &quot;No_Alley_Access&quot;, &quot;No_Alley_Access&quot;, &quot;No_Alley_Acc… ## $ Lot_Shape &lt;chr&gt; &quot;Slightly_Irregular&quot;, &quot;Regular&quot;, &quot;Slightly_Irregula… ## $ Land_Contour &lt;chr&gt; &quot;Lvl&quot;, &quot;Lvl&quot;, &quot;Lvl&quot;, &quot;Lvl&quot;, &quot;Lvl&quot;, &quot;Lvl&quot;, &quot;Lvl&quot;, &quot;H… ## $ Utilities &lt;chr&gt; &quot;AllPub&quot;, &quot;AllPub&quot;, &quot;AllPub&quot;, &quot;AllPub&quot;, &quot;AllPub&quot;, &quot;… ## $ Lot_Config &lt;chr&gt; &quot;Corner&quot;, &quot;Inside&quot;, &quot;Corner&quot;, &quot;Corner&quot;, &quot;Inside&quot;, &quot;… ## $ Land_Slope &lt;chr&gt; &quot;Gtl&quot;, &quot;Gtl&quot;, &quot;Gtl&quot;, &quot;Gtl&quot;, &quot;Gtl&quot;, &quot;Gtl&quot;, &quot;Gtl&quot;, &quot;G… ## $ Neighborhood &lt;chr&gt; &quot;North_Ames&quot;, &quot;North_Ames&quot;, &quot;North_Ames&quot;, &quot;North_Am… ## $ Condition_1 &lt;chr&gt; &quot;Norm&quot;, &quot;Feedr&quot;, &quot;Norm&quot;, &quot;Norm&quot;, &quot;Norm&quot;, &quot;Norm&quot;, &quot;N… ## $ Condition_2 &lt;chr&gt; &quot;Norm&quot;, &quot;Norm&quot;, &quot;Norm&quot;, &quot;Norm&quot;, &quot;Norm&quot;, &quot;Norm&quot;, &quot;No… ## $ Bldg_Type &lt;chr&gt; &quot;OneFam&quot;, &quot;OneFam&quot;, &quot;OneFam&quot;, &quot;OneFam&quot;, &quot;OneFam&quot;, &quot;… ## $ House_Style &lt;chr&gt; &quot;One_Story&quot;, &quot;One_Story&quot;, &quot;One_Story&quot;, &quot;One_Story&quot;,… ## $ Overall_Cond &lt;chr&gt; &quot;Average&quot;, &quot;Above_Average&quot;, &quot;Above_Average&quot;, &quot;Avera… ## $ Year_Built &lt;dbl&gt; 1960, 1961, 1958, 1968, 1997, 1998, 2001, 1992, 199… ## $ Year_Remod_Add &lt;dbl&gt; 1960, 1961, 1958, 1968, 1998, 1998, 2001, 1992, 199… ## $ Roof_Style &lt;chr&gt; &quot;Hip&quot;, &quot;Gable&quot;, &quot;Hip&quot;, &quot;Hip&quot;, &quot;Gable&quot;, &quot;Gable&quot;, &quot;Ga… ## $ Roof_Matl &lt;chr&gt; &quot;CompShg&quot;, &quot;CompShg&quot;, &quot;CompShg&quot;, &quot;CompShg&quot;, &quot;CompSh… ## $ Exterior_1st &lt;chr&gt; &quot;BrkFace&quot;, &quot;VinylSd&quot;, &quot;Wd Sdng&quot;, &quot;BrkFace&quot;, &quot;VinylS… ## $ Exterior_2nd &lt;chr&gt; &quot;Plywood&quot;, &quot;VinylSd&quot;, &quot;Wd Sdng&quot;, &quot;BrkFace&quot;, &quot;VinylS… ## $ Mas_Vnr_Type &lt;chr&gt; &quot;Stone&quot;, &quot;None&quot;, &quot;BrkFace&quot;, &quot;None&quot;, &quot;None&quot;, &quot;BrkFac… ## $ Mas_Vnr_Area &lt;dbl&gt; 112, 0, 108, 0, 0, 20, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6… ## $ Exter_Cond &lt;chr&gt; &quot;Typical&quot;, &quot;Typical&quot;, &quot;Typical&quot;, &quot;Typical&quot;, &quot;Typica… ## $ Foundation &lt;chr&gt; &quot;CBlock&quot;, &quot;CBlock&quot;, &quot;CBlock&quot;, &quot;CBlock&quot;, &quot;PConc&quot;, &quot;P… ## $ Bsmt_Cond &lt;chr&gt; &quot;Good&quot;, &quot;Typical&quot;, &quot;Typical&quot;, &quot;Typical&quot;, &quot;Typical&quot;,… ## $ Bsmt_Exposure &lt;chr&gt; &quot;Gd&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Mn&quot;, &quot;No&quot;, &quot;No… ## $ BsmtFin_Type_1 &lt;chr&gt; &quot;BLQ&quot;, &quot;Rec&quot;, &quot;ALQ&quot;, &quot;ALQ&quot;, &quot;GLQ&quot;, &quot;GLQ&quot;, &quot;GLQ&quot;, &quot;A… ## $ BsmtFin_SF_1 &lt;dbl&gt; 2, 6, 1, 1, 3, 3, 3, 1, 3, 7, 7, 1, 7, 3, 3, 1, 3, … ## $ BsmtFin_Type_2 &lt;chr&gt; &quot;Unf&quot;, &quot;LwQ&quot;, &quot;Unf&quot;, &quot;Unf&quot;, &quot;Unf&quot;, &quot;Unf&quot;, &quot;Unf&quot;, &quot;U… ## $ BsmtFin_SF_2 &lt;dbl&gt; 0, 144, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1120, 0… ## $ Bsmt_Unf_SF &lt;dbl&gt; 441, 270, 406, 1045, 137, 324, 722, 1017, 415, 994,… ## $ Total_Bsmt_SF &lt;dbl&gt; 1080, 882, 1329, 2110, 928, 926, 1338, 1280, 1595, … ## $ Heating &lt;chr&gt; &quot;GasA&quot;, &quot;GasA&quot;, &quot;GasA&quot;, &quot;GasA&quot;, &quot;GasA&quot;, &quot;GasA&quot;, &quot;Ga… ## $ Heating_QC &lt;chr&gt; &quot;Fair&quot;, &quot;Typical&quot;, &quot;Typical&quot;, &quot;Excellent&quot;, &quot;Good&quot;, … ## $ Central_Air &lt;chr&gt; &quot;Y&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;… ## $ Electrical &lt;chr&gt; &quot;SBrkr&quot;, &quot;SBrkr&quot;, &quot;SBrkr&quot;, &quot;SBrkr&quot;, &quot;SBrkr&quot;, &quot;SBrkr… ## $ First_Flr_SF &lt;dbl&gt; 1656, 896, 1329, 2110, 928, 926, 1338, 1280, 1616, … ## $ Second_Flr_SF &lt;dbl&gt; 0, 0, 0, 0, 701, 678, 0, 0, 0, 776, 892, 0, 676, 0,… ## $ Gr_Liv_Area &lt;dbl&gt; 1656, 896, 1329, 2110, 1629, 1604, 1338, 1280, 1616… ## $ Bsmt_Full_Bath &lt;dbl&gt; 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, … ## $ Bsmt_Half_Bath &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ Full_Bath &lt;dbl&gt; 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 3, 2, … ## $ Half_Bath &lt;dbl&gt; 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, … ## $ Bedroom_AbvGr &lt;dbl&gt; 3, 2, 3, 3, 3, 3, 2, 2, 2, 3, 3, 3, 3, 2, 1, 4, 4, … ## $ Kitchen_AbvGr &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … ## $ TotRms_AbvGrd &lt;dbl&gt; 7, 5, 6, 8, 6, 7, 6, 5, 5, 7, 7, 6, 7, 5, 4, 12, 8,… ## $ Functional &lt;chr&gt; &quot;Typ&quot;, &quot;Typ&quot;, &quot;Typ&quot;, &quot;Typ&quot;, &quot;Typ&quot;, &quot;Typ&quot;, &quot;Typ&quot;, &quot;T… ## $ Fireplaces &lt;dbl&gt; 2, 0, 0, 2, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, … ## $ Garage_Type &lt;chr&gt; &quot;Attchd&quot;, &quot;Attchd&quot;, &quot;Attchd&quot;, &quot;Attchd&quot;, &quot;Attchd&quot;, &quot;… ## $ Garage_Finish &lt;chr&gt; &quot;Fin&quot;, &quot;Unf&quot;, &quot;Unf&quot;, &quot;Fin&quot;, &quot;Fin&quot;, &quot;Fin&quot;, &quot;Fin&quot;, &quot;R… ## $ Garage_Cars &lt;dbl&gt; 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, … ## $ Garage_Area &lt;dbl&gt; 528, 730, 312, 522, 482, 470, 582, 506, 608, 442, 4… ## $ Garage_Cond &lt;chr&gt; &quot;Typical&quot;, &quot;Typical&quot;, &quot;Typical&quot;, &quot;Typical&quot;, &quot;Typica… ## $ Paved_Drive &lt;chr&gt; &quot;Partial_Pavement&quot;, &quot;Paved&quot;, &quot;Paved&quot;, &quot;Paved&quot;, &quot;Pav… ## $ Wood_Deck_SF &lt;dbl&gt; 210, 140, 393, 0, 212, 360, 0, 0, 237, 140, 157, 48… ## $ Open_Porch_SF &lt;dbl&gt; 62, 0, 36, 0, 34, 36, 0, 82, 152, 60, 84, 21, 75, 0… ## $ Enclosed_Porch &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 170, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ Three_season_porch &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ Screen_Porch &lt;dbl&gt; 0, 120, 0, 0, 0, 0, 0, 144, 0, 0, 0, 0, 0, 0, 140, … ## $ Pool_Area &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ Pool_QC &lt;chr&gt; &quot;No_Pool&quot;, &quot;No_Pool&quot;, &quot;No_Pool&quot;, &quot;No_Pool&quot;, &quot;No_Poo… ## $ Fence &lt;chr&gt; &quot;No_Fence&quot;, &quot;Minimum_Privacy&quot;, &quot;No_Fence&quot;, &quot;No_Fenc… ## $ Misc_Feature &lt;chr&gt; &quot;None&quot;, &quot;None&quot;, &quot;Gar2&quot;, &quot;None&quot;, &quot;None&quot;, &quot;None&quot;, &quot;No… ## $ Misc_Val &lt;dbl&gt; 0, 0, 12500, 0, 0, 0, 0, 0, 0, 0, 0, 500, 0, 0, 0, … ## $ Mo_Sold &lt;dbl&gt; 5, 6, 6, 4, 3, 6, 4, 1, 3, 6, 4, 3, 5, 2, 6, 6, 6, … ## $ Year_Sold &lt;dbl&gt; 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 201… ## $ Sale_Type &lt;chr&gt; &quot;WD&quot;, &quot;WD&quot;, &quot;WD&quot;, &quot;WD&quot;, &quot;WD&quot;, &quot;WD&quot;, &quot;WD&quot;, &quot;WD&quot;, &quot;WD… ## $ Sale_Condition &lt;chr&gt; &quot;Normal&quot;, &quot;Normal&quot;, &quot;Normal&quot;, &quot;Normal&quot;, &quot;Normal&quot;, &quot;… ## $ Sale_Price &lt;dbl&gt; 215000, 105000, 172000, 244000, 189900, 195500, 213… ## $ Longitude &lt;dbl&gt; -93.61975, -93.61976, -93.61939, -93.61732, -93.638… ## $ Latitude &lt;dbl&gt; 42.05403, 42.05301, 42.05266, 42.05125, 42.06090, 4… 3.2.1 Seleccionar columnas Observamos que nuestros datos tienen 2,930 observaciones y 74 variables, con select() podemos seleccionar las variables que se indiquen. ames_housing %&gt;% select(Lot_Area, Neighborhood, Year_Sold, Sale_Price) ## # A tibble: 2,930 × 4 ## Lot_Area Neighborhood Year_Sold Sale_Price ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 31770 North_Ames 2010 215000 ## 2 11622 North_Ames 2010 105000 ## 3 14267 North_Ames 2010 172000 ## 4 11160 North_Ames 2010 244000 ## 5 13830 Gilbert 2010 189900 ## 6 9978 Gilbert 2010 195500 ## 7 4920 Stone_Brook 2010 213500 ## 8 5005 Stone_Brook 2010 191500 ## 9 5389 Stone_Brook 2010 236500 ## 10 7500 Gilbert 2010 189000 ## # … with 2,920 more rows ¡¡ RECORDAR !! El operador pipe (%&gt;%) se usa para conectar un elemento con una función o acción a realizar. En este caso solo se indica que en los datos de ames se seleccionan 4 variables. Con select() y contains() podemos seleccionar variables con alguna cadena de texto. ames_housing %&gt;% select(contains(&quot;Area&quot;)) ## # A tibble: 2,930 × 5 ## Lot_Area Mas_Vnr_Area Gr_Liv_Area Garage_Area Pool_Area ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 31770 112 1656 528 0 ## 2 11622 0 896 730 0 ## 3 14267 108 1329 312 0 ## 4 11160 0 2110 522 0 ## 5 13830 0 1629 482 0 ## 6 9978 20 1604 470 0 ## 7 4920 0 1338 582 0 ## 8 5005 0 1280 506 0 ## 9 5389 0 1616 608 0 ## 10 7500 0 1804 442 0 ## # … with 2,920 more rows De igual manera, con select(), ends_with y start_with() podemos seleccionar que inicien o terminen con alguna cadena de texto. ames_housing %&gt;% select(starts_with(&quot;Garage&quot;)) ## # A tibble: 2,930 × 5 ## Garage_Type Garage_Finish Garage_Cars Garage_Area Garage_Cond ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Attchd Fin 2 528 Typical ## 2 Attchd Unf 1 730 Typical ## 3 Attchd Unf 1 312 Typical ## 4 Attchd Fin 2 522 Typical ## 5 Attchd Fin 2 482 Typical ## 6 Attchd Fin 2 470 Typical ## 7 Attchd Fin 2 582 Typical ## 8 Attchd RFn 2 506 Typical ## 9 Attchd RFn 2 608 Typical ## 10 Attchd Fin 2 442 Typical ## # … with 2,920 more rows Funciones útiles para select(): contains(): Selecciona variables cuyo nombre contiene la cadena de texto. ends_with(): Selecciona variables cuyo nombre termina con la cadena de caracteres. everything(): Selecciona todas las columnas. matches(): Selecciona las variables cuyos nombres coinciden con una expresión regular. num_range(): Selecciona las variables por posición. start_with(): Selecciona variables cuyos nombres empiezan con la cadena de caracteres. any_of: Selecciona cualquiera de estas variables, en caso de existir EJERCICIO: Crear con datos propios una consulta de columnas usando como variable auxiliar cada una de las listadas anteriormente. Será suficiente con realizar un ejemplo de cada una. 3.2.2 Filtrar observaciones La función filter() nos permite filtrar filas según una condición, primero notemos que la variable Sale_Condition tiene distintas categorías. table(ames_housing$Sale_Condition) ## ## Abnorml AdjLand Alloca Family Normal Partial ## 190 12 24 46 2413 245 ¡¡ SPOILER !! En un modelo predictivo de Machine Learning, no es correcto agregar columnas cuyo valor es conocido hasta el momento de la observación. Es decir, no deben agregarse variables que no se conozca su valor al momento de la predicción, como es el caso de condición de venta. Ahora usaremos la función filter para quedarnos solo con las observaciones con condición de venta “normal”. ames_housing %&gt;% filter(Sale_Condition == &quot;Normal&quot;) ## # A tibble: 2,413 × 74 ## MS_Sub…¹ MS_Zo…² Lot_F…³ Lot_A…⁴ Street Alley Lot_S…⁵ Land_…⁶ Utili…⁷ Lot_C…⁸ ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 One_Sto… Reside… 141 31770 Pave No_A… Slight… Lvl AllPub Corner ## 2 One_Sto… Reside… 80 11622 Pave No_A… Regular Lvl AllPub Inside ## 3 One_Sto… Reside… 81 14267 Pave No_A… Slight… Lvl AllPub Corner ## 4 One_Sto… Reside… 93 11160 Pave No_A… Regular Lvl AllPub Corner ## 5 Two_Sto… Reside… 74 13830 Pave No_A… Slight… Lvl AllPub Inside ## 6 Two_Sto… Reside… 78 9978 Pave No_A… Slight… Lvl AllPub Inside ## 7 One_Sto… Reside… 41 4920 Pave No_A… Regular Lvl AllPub Inside ## 8 One_Sto… Reside… 43 5005 Pave No_A… Slight… HLS AllPub Inside ## 9 One_Sto… Reside… 39 5389 Pave No_A… Slight… Lvl AllPub Inside ## 10 Two_Sto… Reside… 60 7500 Pave No_A… Regular Lvl AllPub Inside ## # … with 2,403 more rows, 64 more variables: Land_Slope &lt;chr&gt;, ## # Neighborhood &lt;chr&gt;, Condition_1 &lt;chr&gt;, Condition_2 &lt;chr&gt;, Bldg_Type &lt;chr&gt;, ## # House_Style &lt;chr&gt;, Overall_Cond &lt;chr&gt;, Year_Built &lt;dbl&gt;, ## # Year_Remod_Add &lt;dbl&gt;, Roof_Style &lt;chr&gt;, Roof_Matl &lt;chr&gt;, ## # Exterior_1st &lt;chr&gt;, Exterior_2nd &lt;chr&gt;, Mas_Vnr_Type &lt;chr&gt;, ## # Mas_Vnr_Area &lt;dbl&gt;, Exter_Cond &lt;chr&gt;, Foundation &lt;chr&gt;, Bsmt_Cond &lt;chr&gt;, ## # Bsmt_Exposure &lt;chr&gt;, BsmtFin_Type_1 &lt;chr&gt;, BsmtFin_SF_1 &lt;dbl&gt;, … También se puede usar para filtrar variables numéricas: ames_housing %&gt;% filter(Lot_Area &gt; 1000 &amp; Sale_Price &gt;= 150000) ## # A tibble: 1,677 × 74 ## MS_Sub…¹ MS_Zo…² Lot_F…³ Lot_A…⁴ Street Alley Lot_S…⁵ Land_…⁶ Utili…⁷ Lot_C…⁸ ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 One_Sto… Reside… 141 31770 Pave No_A… Slight… Lvl AllPub Corner ## 2 One_Sto… Reside… 81 14267 Pave No_A… Slight… Lvl AllPub Corner ## 3 One_Sto… Reside… 93 11160 Pave No_A… Regular Lvl AllPub Corner ## 4 Two_Sto… Reside… 74 13830 Pave No_A… Slight… Lvl AllPub Inside ## 5 Two_Sto… Reside… 78 9978 Pave No_A… Slight… Lvl AllPub Inside ## 6 One_Sto… Reside… 41 4920 Pave No_A… Regular Lvl AllPub Inside ## 7 One_Sto… Reside… 43 5005 Pave No_A… Slight… HLS AllPub Inside ## 8 One_Sto… Reside… 39 5389 Pave No_A… Slight… Lvl AllPub Inside ## 9 Two_Sto… Reside… 60 7500 Pave No_A… Regular Lvl AllPub Inside ## 10 Two_Sto… Reside… 75 10000 Pave No_A… Slight… Lvl AllPub Corner ## # … with 1,667 more rows, 64 more variables: Land_Slope &lt;chr&gt;, ## # Neighborhood &lt;chr&gt;, Condition_1 &lt;chr&gt;, Condition_2 &lt;chr&gt;, Bldg_Type &lt;chr&gt;, ## # House_Style &lt;chr&gt;, Overall_Cond &lt;chr&gt;, Year_Built &lt;dbl&gt;, ## # Year_Remod_Add &lt;dbl&gt;, Roof_Style &lt;chr&gt;, Roof_Matl &lt;chr&gt;, ## # Exterior_1st &lt;chr&gt;, Exterior_2nd &lt;chr&gt;, Mas_Vnr_Type &lt;chr&gt;, ## # Mas_Vnr_Area &lt;dbl&gt;, Exter_Cond &lt;chr&gt;, Foundation &lt;chr&gt;, Bsmt_Cond &lt;chr&gt;, ## # Bsmt_Exposure &lt;chr&gt;, BsmtFin_Type_1 &lt;chr&gt;, BsmtFin_SF_1 &lt;dbl&gt;, … Notemos que en el ejemplo anterior se usa &amp;, que ayuda a filtrar por dos condiciones. También puede usarse | para filtrar por alguna de las dos condiciones. ames_housing %&gt;% filter(Lot_Area &lt; 1000 | Sale_Price &lt;= 150000) ## # A tibble: 1,271 × 74 ## MS_Sub…¹ MS_Zo…² Lot_F…³ Lot_A…⁴ Street Alley Lot_S…⁵ Land_…⁶ Utili…⁷ Lot_C…⁸ ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 One_Sto… Reside… 80 11622 Pave No_A… Regular Lvl AllPub Inside ## 2 One_Sto… Reside… 140 19138 Pave No_A… Regular Lvl AllPub Corner ## 3 One_Sto… Reside… 0 11241 Pave No_A… Slight… Lvl AllPub CulDSac ## 4 One_Sto… Reside… 0 12537 Pave No_A… Slight… Lvl AllPub CulDSac ## 5 One_Sto… Reside… 65 8450 Pave No_A… Regular Lvl AllPub Inside ## 6 One_Sto… Reside… 70 8400 Pave No_A… Regular Lvl AllPub Corner ## 7 One_Sto… Reside… 70 10500 Pave No_A… Regular Lvl AllPub FR2 ## 8 Two_Sto… Reside… 21 1680 Pave No_A… Regular Lvl AllPub Inside ## 9 Two_Sto… Reside… 21 1680 Pave No_A… Regular Lvl AllPub Inside ## 10 Two_Sto… Reside… 21 1680 Pave No_A… Regular Lvl AllPub Inside ## # … with 1,261 more rows, 64 more variables: Land_Slope &lt;chr&gt;, ## # Neighborhood &lt;chr&gt;, Condition_1 &lt;chr&gt;, Condition_2 &lt;chr&gt;, Bldg_Type &lt;chr&gt;, ## # House_Style &lt;chr&gt;, Overall_Cond &lt;chr&gt;, Year_Built &lt;dbl&gt;, ## # Year_Remod_Add &lt;dbl&gt;, Roof_Style &lt;chr&gt;, Roof_Matl &lt;chr&gt;, ## # Exterior_1st &lt;chr&gt;, Exterior_2nd &lt;chr&gt;, Mas_Vnr_Type &lt;chr&gt;, ## # Mas_Vnr_Area &lt;dbl&gt;, Exter_Cond &lt;chr&gt;, Foundation &lt;chr&gt;, Bsmt_Cond &lt;chr&gt;, ## # Bsmt_Exposure &lt;chr&gt;, BsmtFin_Type_1 &lt;chr&gt;, BsmtFin_SF_1 &lt;dbl&gt;, … Las condiciones pueden ser expresiones lógicas construidas mediante los operadores relacionales y lógicos: &lt; : Menor que &gt; : Mayor que == : Igual que &lt;= : Menor o igual que &gt;= : Mayor o igual que != : Diferente que %in% : Pertenece al conjunto is.na : Es NA !is.na : No es NA EJERCICIO: Practicar la función de filtro de observaciones usando los operadores auxiliares. Concatenar el resultado de seleccionar columnas y posteriormente filtrar columnas. 3.2.3 Ordenar registros La función arrange() se utiliza para ordenar las filas de un data frame de acuerdo a una o varias variables. Este ordenamiento puede ser ascendente o descendente. Por defecto arrange() ordena las filas por orden ascendente: ames_housing %&gt;% arrange(Sale_Price) ## # A tibble: 2,930 × 74 ## MS_Sub…¹ MS_Zo…² Lot_F…³ Lot_A…⁴ Street Alley Lot_S…⁵ Land_…⁶ Utili…⁷ Lot_C…⁸ ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 One_Sto… Reside… 68 9656 Pave No_A… Regular Lvl AllPub Inside ## 2 One_Sto… A_agr 80 14584 Pave No_A… Regular Low AllPub Inside ## 3 One_Sto… C_all 60 7879 Pave No_A… Regular Lvl AllPub Inside ## 4 One_Sto… Reside… 60 8088 Pave Grav… Regular Lvl AllPub Inside ## 5 One_Sto… C_all 50 9000 Pave No_A… Regular Lvl AllPub Inside ## 6 One_and… Reside… 50 5925 Pave No_A… Regular Lvl AllPub Inside ## 7 One_Sto… Reside… 50 5000 Pave No_A… Regular Low AllPub Inside ## 8 Two_Sto… C_all 50 8500 Pave Paved Regular Lvl AllPub Inside ## 9 One_Sto… C_all 72 9392 Pave No_A… Regular Lvl AllPub Corner ## 10 One_Sto… Reside… 50 5925 Pave No_A… Regular Lvl AllPub Corner ## # … with 2,920 more rows, 64 more variables: Land_Slope &lt;chr&gt;, ## # Neighborhood &lt;chr&gt;, Condition_1 &lt;chr&gt;, Condition_2 &lt;chr&gt;, Bldg_Type &lt;chr&gt;, ## # House_Style &lt;chr&gt;, Overall_Cond &lt;chr&gt;, Year_Built &lt;dbl&gt;, ## # Year_Remod_Add &lt;dbl&gt;, Roof_Style &lt;chr&gt;, Roof_Matl &lt;chr&gt;, ## # Exterior_1st &lt;chr&gt;, Exterior_2nd &lt;chr&gt;, Mas_Vnr_Type &lt;chr&gt;, ## # Mas_Vnr_Area &lt;dbl&gt;, Exter_Cond &lt;chr&gt;, Foundation &lt;chr&gt;, Bsmt_Cond &lt;chr&gt;, ## # Bsmt_Exposure &lt;chr&gt;, BsmtFin_Type_1 &lt;chr&gt;, BsmtFin_SF_1 &lt;dbl&gt;, … Si las queremos ordenar de forma ascendente, lo haremos del siguiente modo: ames_housing %&gt;% arrange(desc(Sale_Price)) ## # A tibble: 2,930 × 74 ## MS_Sub…¹ MS_Zo…² Lot_F…³ Lot_A…⁴ Street Alley Lot_S…⁵ Land_…⁶ Utili…⁷ Lot_C…⁸ ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Two_Sto… Reside… 104 21535 Pave No_A… Slight… Lvl AllPub Corner ## 2 Two_Sto… Reside… 160 15623 Pave No_A… Slight… Lvl AllPub Corner ## 3 Two_Sto… Reside… 118 35760 Pave No_A… Slight… Lvl AllPub CulDSac ## 4 One_Sto… Reside… 106 12720 Pave No_A… Regular HLS AllPub Inside ## 5 One_Sto… Reside… 100 12919 Pave No_A… Slight… Lvl AllPub Inside ## 6 One_Sto… Reside… 105 13693 Pave No_A… Regular Lvl AllPub Inside ## 7 One_Sto… Reside… 52 51974 Pave No_A… Slight… Lvl AllPub CulDSac ## 8 Two_Sto… Reside… 114 17242 Pave No_A… Slight… Lvl AllPub Inside ## 9 Two_Sto… Reside… 107 13891 Pave No_A… Regular Lvl AllPub Inside ## 10 Two_Sto… Reside… 85 16056 Pave No_A… Slight… Lvl AllPub Inside ## # … with 2,920 more rows, 64 more variables: Land_Slope &lt;chr&gt;, ## # Neighborhood &lt;chr&gt;, Condition_1 &lt;chr&gt;, Condition_2 &lt;chr&gt;, Bldg_Type &lt;chr&gt;, ## # House_Style &lt;chr&gt;, Overall_Cond &lt;chr&gt;, Year_Built &lt;dbl&gt;, ## # Year_Remod_Add &lt;dbl&gt;, Roof_Style &lt;chr&gt;, Roof_Matl &lt;chr&gt;, ## # Exterior_1st &lt;chr&gt;, Exterior_2nd &lt;chr&gt;, Mas_Vnr_Type &lt;chr&gt;, ## # Mas_Vnr_Area &lt;dbl&gt;, Exter_Cond &lt;chr&gt;, Foundation &lt;chr&gt;, Bsmt_Cond &lt;chr&gt;, ## # Bsmt_Exposure &lt;chr&gt;, BsmtFin_Type_1 &lt;chr&gt;, BsmtFin_SF_1 &lt;dbl&gt;, … Si se desea usar dos o más columnas para realizar el ordenamiento, deben separarse por comas cada una de las características ames_housing %&gt;% arrange(Sale_Condition, desc(Sale_Price), Lot_Area) %&gt;% select(Sale_Condition, Sale_Price, Lot_Area) ## # A tibble: 2,930 × 3 ## Sale_Condition Sale_Price Lot_Area ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Abnorml 745000 15623 ## 2 Abnorml 552000 14836 ## 3 Abnorml 475000 11778 ## 4 Abnorml 390000 13418 ## 5 Abnorml 328900 5119 ## 6 Abnorml 310000 14541 ## 7 Abnorml 290000 9950 ## 8 Abnorml 287000 15498 ## 9 Abnorml 258000 12090 ## 10 Abnorml 257000 10994 ## # … with 2,920 more rows Notemos que en el ejemplo anterior usamos dos pipes (%&gt;%), como habíamos mencionado se pueden usar los necesarios para combinar funciones. 3.2.4 Agregar / Modificar Con la función mutate() podemos computar transformaciones de variables en un data frame. A menudo, tendremos la necesidad de crear nuevas variables que se calculan a partir de variables existentes. La función mutate() proporciona una interfaz clara para realizar este tipo de operaciones. Por ejemplo, haremos el cálculo de la antigüedad del inmueble a partir de las variables Year_Sold y Year_Remod_Add: ejemplo_mutate &lt;- ames_housing %&gt;% select(Year_Sold, Year_Remod_Add) %&gt;% mutate(Antique = Year_Sold - Year_Remod_Add) ejemplo_mutate ## # A tibble: 2,930 × 3 ## Year_Sold Year_Remod_Add Antique ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2010 1960 50 ## 2 2010 1961 49 ## 3 2010 1958 52 ## 4 2010 1968 42 ## 5 2010 1998 12 ## 6 2010 1998 12 ## 7 2010 2001 9 ## 8 2010 1992 18 ## 9 2010 1996 14 ## 10 2010 1999 11 ## # … with 2,920 more rows El ejemplo anterior crea una nueva variable. Ahora se presenta otro ejemplo en donde se modifica una variable ya creada. ejemplo_mutate %&gt;% mutate(Antique = Antique * 12) ## # A tibble: 2,930 × 3 ## Year_Sold Year_Remod_Add Antique ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2010 1960 600 ## 2 2010 1961 588 ## 3 2010 1958 624 ## 4 2010 1968 504 ## 5 2010 1998 144 ## 6 2010 1998 144 ## 7 2010 2001 108 ## 8 2010 1992 216 ## 9 2010 1996 168 ## 10 2010 1999 132 ## # … with 2,920 more rows En este segundo ejemplo, se modifica el número de años de antigüedad y se multiplica por un factor de 12 para modificar el tiempo en una escala de meses. 3.2.5 Resumen estadístico La función summarise() se comporta de forma análoga a la función mutate(), excepto que en lugar de añadir nuevas columnas crea un nuevo data frame. Podemos usar el ejemplo anterior y calcular la media de la variable creada Antique: ames_housing %&gt;% select(Year_Sold, Year_Remod_Add) %&gt;% mutate(Antique = Year_Sold - Year_Remod_Add) %&gt;% summarise(Mean_Antique = mean(Antique)) ## # A tibble: 1 × 1 ## Mean_Antique ## &lt;dbl&gt; ## 1 23.5 Solo fue necesario agregar un pipe, especificar el nombre de la variable creada y la operación a realizar. A continuación se muestran funciones que trabajando conjuntamente con la función summarise() facilitarán nuestro trabajo diario. Las primeras pertenecen al paquete base y las otras son del paquete dplyr. Todas ellas toman como argumento un vector y devuelven un único resultado: min(), max() : Valores max y min. mean() : Media. median() : Mediana. sum() : Suma de los valores. var(), sd() : Varianza y desviación estándar. first() : Primer valor en un vector. last() : El último valor en un vector n() : El número de valores en un vector. n_distinct() : El número de valores distintos en un vector. nth() : Extrae el valor que ocupa la posición n en un vector. Mas adelante veremos como combinar esta función con la función group_by() para calcular estadísticos agrupados por alguna característica de interés. EJERCICIO: Realizar una consulta usando summarise() y cada una de las funciones estadísticas listadas anteriormente. 3.2.6 Agrupamiento La función group_by() agrupa un conjunto de filas de acuerdo con los valores de una o más columnas o expresiones. Usaremos el ejemplo anterior. Primero creamos nuestra nueva variable Antique, después agrupamos por vecindario y al final calculamos la media de la variable Antique. Gracias al agrupamiento, nos regresara una media por cada grupo creado, es decir, nos regresara el promedio de la antigüedad por vecindario. ames_housing %&gt;% mutate(Antique = Year_Sold - Year_Remod_Add) %&gt;% group_by(Neighborhood) %&gt;% summarise(Mean_Antique = round(mean(Antique), 0)) ## # A tibble: 28 × 2 ## Neighborhood Mean_Antique ## &lt;chr&gt; &lt;dbl&gt; ## 1 Bloomington_Heights 2 ## 2 Blueste 25 ## 3 Briardale 35 ## 4 Brookside 39 ## 5 Clear_Creek 28 ## 6 College_Creek 8 ## 7 Crawford 29 ## 8 Edwards 33 ## 9 Gilbert 9 ## 10 Green_Hills 14 ## # … with 18 more rows 3.2.7 Cruces de tablas Una operación fundamental por agregar al flujo de trabajo es el cruce de tablas, las cuales pueden proceder de la misma o de distinta fuente. Comúnmente este proceso se realiza para enriquecer y unificar la información proveniente de distintas tablas. Para lograr esta tarea es indispensable que exista una variable llave en ambos conjuntos de datos que sirva como puente o identificador de cada caso o renglón. Si se cuenta con la columna llave entonces será posible cruzar las tablas y lograr su enriquecimiento. En el siguiente ejemplo se muestra el uso de la variable llave a través de la columna “ID”. Se puede apreciar que en la tabla final se cuenta con información de la variable “Weight” para los elementos que existen en las tablas “A” y “B”. La función que hace posible el complemento de la información es llamada left_join(). El primer argumento de la función corresponde al conjunto de datos que se desea complementar, mientras que en el segundo argumento se ingresa el conjunto de datos con la información que enriquecerá al primer conjunto. Es necesario especificar en el argumento “by” el nombre de la columna llave. conjuntoX &lt;- tibble(&quot;Llave&quot; = LETTERS[1:8], &quot;C1&quot; = 1:8) conjuntoY &lt;- tibble( &quot;Llave&quot; = sample(LETTERS[11:3], size = 9, replace = T), &quot;Ex1&quot; = letters[2:10], &quot;Ex2&quot; = 1002:1010,&quot;Ex3&quot; = paste0(letters[12:20], 2:10) ) conjuntoX ## # A tibble: 8 × 2 ## Llave C1 ## &lt;chr&gt; &lt;int&gt; ## 1 A 1 ## 2 B 2 ## 3 C 3 ## 4 D 4 ## 5 E 5 ## 6 F 6 ## 7 G 7 ## 8 H 8 conjuntoY ## # A tibble: 9 × 4 ## Llave Ex1 Ex2 Ex3 ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 F b 1002 l2 ## 2 I c 1003 m3 ## 3 K d 1004 n4 ## 4 H e 1005 o5 ## 5 F f 1006 p6 ## 6 J g 1007 q7 ## 7 C h 1008 r8 ## 8 J i 1009 s9 ## 9 I j 1010 t10 left_join(x = conjuntoX, y = conjuntoY, by = &quot;Llave&quot;) ## # A tibble: 9 × 5 ## Llave C1 Ex1 Ex2 Ex3 ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 A 1 &lt;NA&gt; NA &lt;NA&gt; ## 2 B 2 &lt;NA&gt; NA &lt;NA&gt; ## 3 C 3 h 1008 r8 ## 4 D 4 &lt;NA&gt; NA &lt;NA&gt; ## 5 E 5 &lt;NA&gt; NA &lt;NA&gt; ## 6 F 6 b 1002 l2 ## 7 F 6 f 1006 p6 ## 8 G 7 &lt;NA&gt; NA &lt;NA&gt; ## 9 H 8 e 1005 o5 Es posible que no todas las observaciones de un conjunto de datos estén en el otro conjunto. Cuando esto sucede, un aviso aparece indicando que los factores o categorías de la variable llave son diferentes. En caso de no encontrarse uno o más de los valores, el resultado para esos casos será NA (no disponible, por su traducción del inglés “Not Available”), y aparecerá siempre que no se cuente con información en un registro, como se muestra en el ejemplo anterior. Existen diferentes maneras de conjuntar datos. La primera, como en el ejemplo mostrado anteriormente, se hace por lo izquierda y quiere decir que, al primer conjunto de datos es al que se le agregará la información del segundo conjunto. Esto se realizará exclusivamente para aquellos registros de la segunda tabla que existan también en la primera, los cuales se identifican mediante la llave definida. Otra manera de realizar la conjunción de los datos es por la derecha. Funciona de manera análoga a la primera, con la diferencia de que son los datos del primer conjunto los que se agregan al segundo. De igual manera, esto sólo ocurre para los elemento del primer conjunto que se encuentran en el segundo y que son identificables a través de una llave. La función en R que permite realizar la conjunción por la derecha lleva por nombre right_join(). right_join(x = conjuntoX, y = conjuntoY, by = &quot;Llave&quot;) ## # A tibble: 9 × 5 ## Llave C1 Ex1 Ex2 Ex3 ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 C 3 h 1008 r8 ## 2 F 6 b 1002 l2 ## 3 F 6 f 1006 p6 ## 4 H 8 e 1005 o5 ## 5 I NA c 1003 m3 ## 6 K NA d 1004 n4 ## 7 J NA g 1007 q7 ## 8 J NA i 1009 s9 ## 9 I NA j 1010 t10 Una tercer forma de unir los datos es a través de la función full_join(), la cual es una combinación de las dos anteriores. Agrega todos los elementos llave tanto del primer conjunto como del segundo y posteriormente realiza el cruce de información de ambos conjuntos. full_join(x = conjuntoX, y = conjuntoY, by = &quot;Llave&quot;) ## # A tibble: 14 × 5 ## Llave C1 Ex1 Ex2 Ex3 ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 A 1 &lt;NA&gt; NA &lt;NA&gt; ## 2 B 2 &lt;NA&gt; NA &lt;NA&gt; ## 3 C 3 h 1008 r8 ## 4 D 4 &lt;NA&gt; NA &lt;NA&gt; ## 5 E 5 &lt;NA&gt; NA &lt;NA&gt; ## 6 F 6 b 1002 l2 ## 7 F 6 f 1006 p6 ## 8 G 7 &lt;NA&gt; NA &lt;NA&gt; ## 9 H 8 e 1005 o5 ## 10 I NA c 1003 m3 ## 11 K NA d 1004 n4 ## 12 J NA g 1007 q7 ## 13 J NA i 1009 s9 ## 14 I NA j 1010 t10 Estos 3 primeros métodos pueden resumirse en la siguiente imagen: Adicionalmente, existen otras funciones que ayudan con gestionar las operaciones entre conjuntos de datos. Tal es el caso de la función inner_join(), la cuál no es otra cosa que el filtro de aquellos elementos que se tengan en común en ambas tablas y la combinación de un join. Internamente, la función primero filtra el ID de aquellos elementos que tienen presencia en ambas tablas y finalmente hace el cruce de los datos. inner_join(x = conjuntoX, y = conjuntoY, by = &quot;Llave&quot;) ## # A tibble: 4 × 5 ## Llave C1 Ex1 Ex2 Ex3 ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 C 3 h 1008 r8 ## 2 F 6 b 1002 l2 ## 3 F 6 f 1006 p6 ## 4 H 8 e 1005 o5 Existen otras dos operaciones interesantes que agilizan la extracción de subconjuntos de tablas sin cruzar información. Se trata de las funciones semi_join() y anti_join(), las cuales funcionan de la siguiente manera: La función semi_join() detecta y filtra los elementos del primer conjunto que se encuentran en un segundo conjunto, mientras que la función anti_join() es su complemento, pues regresa los elementos del primer conjunto que no se encuentran en el segundo. En ambos casos, la información contenida en el segundo conjunto no es trasmitida al resultado. A continuación se presenta un ejemplo: semi_join(x = conjuntoX, y = conjuntoY, by = &quot;Llave&quot;) ## # A tibble: 3 × 2 ## Llave C1 ## &lt;chr&gt; &lt;int&gt; ## 1 C 3 ## 2 F 6 ## 3 H 8 Ahora revisemos el caso de la función anti_join() anti_join(x = conjuntoX, y = conjuntoY, by = &quot;Llave&quot;) ## # A tibble: 5 × 2 ## Llave C1 ## &lt;chr&gt; &lt;int&gt; ## 1 A 1 ## 2 B 2 ## 3 D 4 ## 4 E 5 ## 5 G 7 WARNING: llaves duplicadas La mayoría de los ejemplos anteriores suponen que las llaves son únicas en cada conjunto de datos, sin embargo, esto no es cierto en una gran cantidad de ocasiones. Existen dos casos importantes por analizar: Llaves duplicadas en 1 conjunto de datos Llaves duplicadas en ambos conjuntos El caso más sencillo es cuando solo uno de los conjuntos contiene llaves duplicadas. En este caso se creará un renglón por cada duplicado. En el segundo caso, por cada elemento duplicado en el primer conjunto habrá como resultado un elemento por cada duplicado en el segundo conjunto. A continuación se ejemplifica este escenario. ¡¡ RECORDAR !! En este link se encuentra un buen resumen de las funciones básicas de dplyr 3.3 Orden y estructura Un conjunto de datos puede ser representado de muchas maneras distintas y contener en todos los casos la misma información. Sin embargo, no todos los modos en que se presenta la información resulta óptimo para su procesamiento y análisis. Los conjuntos de datos ordenados serán más fáciles de trabajar y analizar. Algunas de las características principales que presentan los conjuntos de datos ordenados son las siguientes: Cada variable debe tener su propia columna. Cada observación debe tener su propio renglón. Cada valor debe tener su propia celda. La figura anterior muestra la estructura de orden que debe tener un conjunto de datos. A pesar de que pueda parecer intuitivo y sencillo, en la práctica es considerable el número de conjuntos de datos desordenados. La limpieza y ordenamiento debe ser trabajado de forma impecable a fin de que puedan realizarse buenas prácticas. El tiempo de limpieza y ordenamiento varía mucho dependiendo de la dimensión del conjunto de datos. Algunos de los principales problemas que pueden tener los conjuntos de datos no ordenados son: Una variable puede estar dispersa en múltiples columnas Una observación puede estar esparcida en múltiples renglones La paquetería tidyr cuenta con funciones para resolver dichos problemas. Entre las principales funciones que tiene la paquetería, se encuentran pivot_longer(), pivot_wider(), separate() y unite(), mismas que se analizarán a continuación. 3.3.1 Pivote horizontal La función pivot_wider() resulta muy útil a la hora de organizar los datos. Su función consiste en dispersar una variable clave en múltiples columnas. Lo primero que se debe hacer para poder hacer uso de dicha función es instalar y cargar la librería. El siguiente conjunto de datos contiene el número de localidades rurales y urbanas por municipio de la Ciudad de México. Como es posible observar, algunos municipios aparecen más de una vez en el marco de datos, esto se debe a que cada municipio puede tener ambos ámbitos, rural y urbano. Para hacer que el conjunto de datos sea ordenado, es necesario que cada observación aparezca una sola vez por renglón y cada una de las categorías (rural y urbano) de la variable “Ámbito” deberá ocupar el lugar de una columna. El siguiente código muestra cómo convertir los datos no ordenados en un conjunto ordenado. library(tidyr) Resumen &lt;- readRDS(&quot;data/loc_mun_cdmx.rds&quot;) Resumen %&gt;% pivot_wider( names_from = Ambito, values_from = Total_localidades ) ## # A tibble: 16 × 3 ## NOM_MUN Rural Urbano ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 Álvaro Obregón 3 1 ## 2 La Magdalena Contreras 8 1 ## 3 Cuajimalpa de Morelos 14 2 ## 4 Tláhuac 31 5 ## 5 Xochimilco 78 1 ## 6 Tlalpan 95 4 ## 7 Milpa Alta 187 10 ## 8 Azcapotzalco NA 1 ## 9 Benito Juárez NA 1 ## 10 Coyoacán NA 1 ## 11 Cuauhtémoc NA 1 ## 12 Gustavo A. Madero NA 1 ## 13 Iztacalco NA 1 ## 14 Iztapalapa NA 1 ## 15 Miguel Hidalgo NA 1 ## 16 Venustiano Carranza NA 1 En la tabla actual existe ahora un y solo un registro por cada observación (nombre de municipio en este caso). El valor que le corresponde a cada una de las columnas creadas es la frecuencia absoluta de localidades que tienen la característica “Rural” y “Urbano” respectivamente. Pero… ¿qué pasa cuando no existen todos los valores en ambas columnas? Si no se especifica la manera de llenar los datos faltantes, estos contendrán NAs. Siempre se puede elegir el caracter o número con el cual se imputan los datos faltantes. fish_encounters %&gt;% pivot_wider(names_from = station, values_from = seen) ## # A tibble: 19 × 12 ## fish Release I80_1 Lisbon Rstr Base_TD BCE BCW BCE2 BCW2 MAE MAW ## &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 4842 1 1 1 1 1 1 1 1 1 1 1 ## 2 4843 1 1 1 1 1 1 1 1 1 1 1 ## 3 4844 1 1 1 1 1 1 1 1 1 1 1 ## 4 4845 1 1 1 1 1 NA NA NA NA NA NA ## 5 4847 1 1 1 NA NA NA NA NA NA NA NA ## 6 4848 1 1 1 1 NA NA NA NA NA NA NA ## 7 4849 1 1 NA NA NA NA NA NA NA NA NA ## 8 4850 1 1 NA 1 1 1 1 NA NA NA NA ## 9 4851 1 1 NA NA NA NA NA NA NA NA NA ## 10 4854 1 1 NA NA NA NA NA NA NA NA NA ## 11 4855 1 1 1 1 1 NA NA NA NA NA NA ## 12 4857 1 1 1 1 1 1 1 1 1 NA NA ## 13 4858 1 1 1 1 1 1 1 1 1 1 1 ## 14 4859 1 1 1 1 1 NA NA NA NA NA NA ## 15 4861 1 1 1 1 1 1 1 1 1 1 1 ## 16 4862 1 1 1 1 1 1 1 1 1 NA NA ## 17 4863 1 1 NA NA NA NA NA NA NA NA NA ## 18 4864 1 1 NA NA NA NA NA NA NA NA NA ## 19 4865 1 1 1 NA NA NA NA NA NA NA NA fish_encounters %&gt;% pivot_wider(names_from = station, values_from = seen, values_fill = 0) ## # A tibble: 19 × 12 ## fish Release I80_1 Lisbon Rstr Base_TD BCE BCW BCE2 BCW2 MAE MAW ## &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 4842 1 1 1 1 1 1 1 1 1 1 1 ## 2 4843 1 1 1 1 1 1 1 1 1 1 1 ## 3 4844 1 1 1 1 1 1 1 1 1 1 1 ## 4 4845 1 1 1 1 1 0 0 0 0 0 0 ## 5 4847 1 1 1 0 0 0 0 0 0 0 0 ## 6 4848 1 1 1 1 0 0 0 0 0 0 0 ## 7 4849 1 1 0 0 0 0 0 0 0 0 0 ## 8 4850 1 1 0 1 1 1 1 0 0 0 0 ## 9 4851 1 1 0 0 0 0 0 0 0 0 0 ## 10 4854 1 1 0 0 0 0 0 0 0 0 0 ## 11 4855 1 1 1 1 1 0 0 0 0 0 0 ## 12 4857 1 1 1 1 1 1 1 1 1 0 0 ## 13 4858 1 1 1 1 1 1 1 1 1 1 1 ## 14 4859 1 1 1 1 1 0 0 0 0 0 0 ## 15 4861 1 1 1 1 1 1 1 1 1 1 1 ## 16 4862 1 1 1 1 1 1 1 1 1 0 0 ## 17 4863 1 1 0 0 0 0 0 0 0 0 0 ## 18 4864 1 1 0 0 0 0 0 0 0 0 0 ## 19 4865 1 1 1 0 0 0 0 0 0 0 0 Ejercicio: Realiza un pivote horizontal a través del ámbito y el total de localidades. Rellena los datos faltantes con ceros. En caso de que existan múltiples columnas que se desean dispersar mediante el pivote de una columna con múltiples categorías, es posible especificar tal re estructuración a través del siguiente código. us_rent_income %&gt;% arrange(NAME) ## # A tibble: 104 × 5 ## GEOID NAME variable estimate moe ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 01 Alabama income 24476 136 ## 2 01 Alabama rent 747 3 ## 3 02 Alaska income 32940 508 ## 4 02 Alaska rent 1200 13 ## 5 04 Arizona income 27517 148 ## 6 04 Arizona rent 972 4 ## 7 05 Arkansas income 23789 165 ## 8 05 Arkansas rent 709 5 ## 9 06 California income 29454 109 ## 10 06 California rent 1358 3 ## # … with 94 more rows us_rent_income %&gt;% pivot_wider(names_from = variable, values_from = c(estimate, moe)) ## # A tibble: 52 × 6 ## GEOID NAME estimate_income estimate_rent moe_income moe_rent ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 01 Alabama 24476 747 136 3 ## 2 02 Alaska 32940 1200 508 13 ## 3 04 Arizona 27517 972 148 4 ## 4 05 Arkansas 23789 709 165 5 ## 5 06 California 29454 1358 109 3 ## 6 08 Colorado 32401 1125 109 5 ## 7 09 Connecticut 35326 1123 195 5 ## 8 10 Delaware 31560 1076 247 10 ## 9 11 District of Columbia 43198 1424 681 17 ## 10 12 Florida 25952 1077 70 3 ## # … with 42 more rows Ejercicio: Agrupa los datos de localidades por ámbito Agrega una columna con el porcentaje de localidades por alcaldía Realiza un pivote horizontal sobre el ámbito y las variables numéricas de total de localidades y su respectivo porcentaje creado en el paso anterior Ordena los registros de forma descendente de acuerdo con el total de localidades rural y urbano. Adicionalmente, se puede especificar una función de agregación que operara antes de acomodar los datos en las respectivas columnas indicadas. Un ejemplo de funciones agregadas en la re estructuración de tabla se muestra a continuación, donde se muestra la media de los valores en las categorías tension y breaks. warpbreaks &lt;- warpbreaks[c(&quot;wool&quot;, &quot;tension&quot;, &quot;breaks&quot;)] %&gt;% as_tibble() warpbreaks ## # A tibble: 54 × 3 ## wool tension breaks ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 A L 26 ## 2 A L 30 ## 3 A L 54 ## 4 A L 25 ## 5 A L 70 ## 6 A L 52 ## 7 A L 51 ## 8 A L 26 ## 9 A L 67 ## 10 A M 18 ## # … with 44 more rows warpbreaks %&gt;% pivot_wider( names_from = wool, values_from = breaks, values_fn = ~mean(.x, na.rm = T) ) ## # A tibble: 3 × 3 ## tension A B ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 L 44.6 28.2 ## 2 M 24 28.8 ## 3 H 24.6 18.8 Ejercicio: Sobre el conjunto de localidades crea una variable con 5 categorías numéricas creadas aleatoriamente. Elimina la columna con el nombre del municipio. Crea un pivote horizontal con el ámbito, sumando el total de localidades y rellenando con ceros los datos faltantes. Ordena las categorías numéricas de forma ascendente. 3.3.2 Pivote vertical pivot_longer() es podría ser la función inversa de la anterior, se necesita comúnmente para ordenar los conjuntos de datos capturados en crudo, ya que a menudo no son capturados acorde a las mejores estructuras para facilitar el análisis. El conjunto de datos relig_income almacena recuentos basados en una encuesta que (entre otras cosas) preguntó a las personas sobre su religión e ingresos anuales: relig_income ## # A tibble: 18 × 11 ## religion `&lt;$10k` $10-2…¹ $20-3…² $30-4…³ $40-5…⁴ $50-7…⁵ $75-1…⁶ $100-…⁷ ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Agnostic 27 34 60 81 76 137 122 109 ## 2 Atheist 12 27 37 52 35 70 73 59 ## 3 Buddhist 27 21 30 34 33 58 62 39 ## 4 Catholic 418 617 732 670 638 1116 949 792 ## 5 Don’t know/r… 15 14 15 11 10 35 21 17 ## 6 Evangelical … 575 869 1064 982 881 1486 949 723 ## 7 Hindu 1 9 7 9 11 34 47 48 ## 8 Historically… 228 244 236 238 197 223 131 81 ## 9 Jehovah&#39;s Wi… 20 27 24 24 21 30 15 11 ## 10 Jewish 19 19 25 25 30 95 69 87 ## 11 Mainline Prot 289 495 619 655 651 1107 939 753 ## 12 Mormon 29 40 48 51 56 112 85 49 ## 13 Muslim 6 7 9 10 9 23 16 8 ## 14 Orthodox 13 17 23 32 32 47 38 42 ## 15 Other Christ… 9 7 11 13 13 14 18 14 ## 16 Other Faiths 20 33 40 46 49 63 46 40 ## 17 Other World … 5 2 3 4 2 7 3 4 ## 18 Unaffiliated 217 299 374 365 341 528 407 321 ## # … with 2 more variables: `&gt;150k` &lt;dbl&gt;, `Don&#39;t know/refused` &lt;dbl&gt;, and ## # abbreviated variable names ¹​`$10-20k`, ²​`$20-30k`, ³​`$30-40k`, ⁴​`$40-50k`, ## # ⁵​`$50-75k`, ⁶​`$75-100k`, ⁷​`$100-150k` ¿Crees que ésta es la mejor estructura para la tabla? ¿Cómo imaginas que podría modificarse? Este conjunto de datos contiene tres variables: religión, almacenada en las filas income repartidos entre los nombres de columna count almacenado en los valores de las celdas. Para ordenarlo usamos pivot_longer(): relig_income %&gt;% pivot_longer(cols = -religion, names_to = &quot;income&quot;, values_to = &quot;count&quot;) ## # A tibble: 180 × 3 ## religion income count ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Agnostic &lt;$10k 27 ## 2 Agnostic $10-20k 34 ## 3 Agnostic $20-30k 60 ## 4 Agnostic $30-40k 81 ## 5 Agnostic $40-50k 76 ## 6 Agnostic $50-75k 137 ## 7 Agnostic $75-100k 122 ## 8 Agnostic $100-150k 109 ## 9 Agnostic &gt;150k 84 ## 10 Agnostic Don&#39;t know/refused 96 ## # … with 170 more rows El primer argumento es el conjunto de datos para remodelar, relig_income. El segundo argumento describe qué columnas necesitan ser reformadas. En este caso, es cada columna aparte de religion. El names_to da el nombre de la variable que se creará a partir de los datos almacenados en los nombres de columna, es decir, ingresos. Los values_to dan el nombre de la variable que se creará a partir de los datos almacenados en el valor de la celda, es decir, count. Ni la columna names_to ni la values_to existen en relig_income, por lo que las proporcionamos como cadenas de caracteres entre comillas. 3.3.3 Unión de columnas Es común que en los conjuntos de datos exista información esparcida en distintas columnas que sería deseable (en muy pocas ocasiones) tenerlas en una sola columna. Algunos ejemplos de esta situación deseable son las fechas y claves geoestadísticas. La función unite() sirve para concatenar el contenido de columnas mediante un separador elegible. Se usará la variable de la clave geoestadística de localidades del país como ejemplo. El formato para las claves geoestadísticas para estado, municipio y localidad son claves alfanuméricas que contienen 2, 3 y 4 caracteres respectivamente. Es indispensable que al trabajar con claves geoestadísticas, las claves estén en su formato original. A continuación se hará la homologación de las claves para usar la función unite(). library(magrittr) library(readxl) library(stringr) Datos &lt;- read_excel(&quot;data/Margin CONAPO.xlsx&quot;, sheet = &quot;Margin CONAPO&quot;) Datos ## # A tibble: 107,458 × 21 ## ENT NOM_ENT MUN NOM_MUN LOC NOM_LOC POB_TOT VPH ANAL10 SPRIM10 ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 Aguascalient… 1 Aguasc… 1 Aguasc… 722250 184123 2.26 10.9 ## 2 1 Aguascalient… 1 Aguasc… 96 Agua A… 37 11 17.9 48.1 ## 3 1 Aguascalient… 1 Aguasc… 104 Ardill… 14 3 0 20 ## 4 1 Aguascalient… 1 Aguasc… 106 Arella… 1382 255 5.60 24.7 ## 5 1 Aguascalient… 1 Aguasc… 112 Bajío … 55 11 14.3 38.1 ## 6 1 Aguascalient… 1 Aguasc… 114 Reside… 757 202 0 1.63 ## 7 1 Aguascalient… 1 Aguasc… 120 Buenav… 935 217 10.7 29.5 ## 8 1 Aguascalient… 1 Aguasc… 121 Cabeci… 184 44 4.55 32.6 ## 9 1 Aguascalient… 1 Aguasc… 125 Cañada… 395 82 8.86 23.9 ## 10 1 Aguascalient… 1 Aguasc… 126 Cañada… 509 123 4.75 19.6 ## # … with 107,448 more rows, and 11 more variables: SEXC10 &lt;dbl&gt;, SEE10 &lt;dbl&gt;, ## # SAGUAE10 &lt;dbl&gt;, PROM_OCC10 &lt;dbl&gt;, PISOTIE10 &lt;dbl&gt;, SREFRI10 &lt;dbl&gt;, ## # IM_2010 &lt;dbl&gt;, GM_2010 &lt;chr&gt;, IMC0A100 &lt;dbl&gt;, LUG_NAL &lt;dbl&gt;, LUG_EDO &lt;dbl&gt; Como puede apreciarse en la tabla anterior, las claves de los campos Ent, Mun y Loc aparecen como numéricos. La estructura deseada para estos campos es de tipo alfanumérico y de longitud 2, 3 y 4 respectivamente. Para lograr esta estructura de datos, es necesario concatenar tantos ceros como sean necesarios antes del valor actual hasta lograr la longitud deseada. Datos2 &lt;- Datos %&gt;% select(ENT, MUN, LOC) Datos2$ENT %&lt;&gt;% str_pad(width = 2, side = &quot;left&quot;, pad = &quot;0&quot;) Datos2$MUN %&lt;&gt;% str_pad(width = 3, side = &quot;left&quot;, pad = &quot;0&quot;) Datos2$LOC %&lt;&gt;% str_pad(width = 4, side = &quot;left&quot;, pad = &quot;0&quot;) Datos2 %&gt;% head(5) ## # A tibble: 5 × 3 ## ENT MUN LOC ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 01 001 0001 ## 2 01 001 0096 ## 3 01 001 0104 ## 4 01 001 0106 ## 5 01 001 0112 Datos2 %&gt;% unite(&quot;CVE_GEO&quot;, c(&quot;ENT&quot;,&quot;MUN&quot;,&quot;LOC&quot;), sep=&quot;&quot;, remove = F) %&gt;% head(5) ## # A tibble: 5 × 4 ## CVE_GEO ENT MUN LOC ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 010010001 01 001 0001 ## 2 010010096 01 001 0096 ## 3 010010104 01 001 0104 ## 4 010010106 01 001 0106 ## 5 010010112 01 001 0112 Datos2 %&gt;% unite(&quot;CVE_GEO&quot;, c(&quot;ENT&quot;,&quot;MUN&quot;,&quot;LOC&quot;), sep=&quot;/&quot;,remove = T) %&gt;% head(5) ## # A tibble: 5 × 1 ## CVE_GEO ## &lt;chr&gt; ## 1 01/001/0001 ## 2 01/001/0096 ## 3 01/001/0104 ## 4 01/001/0106 ## 5 01/001/0112 En el código anterior se carga la librería magrittr para poder hacer uso del operador pipe doble “%&lt;&gt;%”, que permite al igual que el operador pipe simple “%&gt;%”, usar como argumento al primer elemento y mandarlo hacia la función definida, además de guardar el resultado final de la cadena de pipes en el argumento original que fue usado como insumo para la función. Es importante tener en cuenta que el dato será reescrito y no se podrá tener acceso a su información almacenada antes de ser usado el operador. Es opción del programador poder eliminar las variables originales que crearon la nueva variable o mantenerlas en el conjunto de datos. Esta opción está disponible en el parámetro remove de la función unite(). 3.3.4 Separador de columnas Los procesos que se han visto hasta ahora han tenido cada uno una función inversa, este es también el caso de la función unite que tiene por objetivo unir dos o más columnas en una. La función separate() separará una columna en dos o más dependiendo de la longitud que tenga y de las especificaciones de separación. Datos_unite1 &lt;- Datos2 %&gt;% unite(&quot;CVE_GEO&quot;, c(&quot;ENT&quot;,&quot;MUN&quot;,&quot;LOC&quot;), sep = &quot;&quot;, remove = T) Datos_unite1 %&gt;% head(5) ## # A tibble: 5 × 1 ## CVE_GEO ## &lt;chr&gt; ## 1 010010001 ## 2 010010096 ## 3 010010104 ## 4 010010106 ## 5 010010112 Datos_unite1 %&gt;% separate(&quot;CVE_GEO&quot;, c(&quot;EDO&quot;,&quot;MUNI&quot;,&quot;LOC&quot;), sep = c(2, 5), remove=F) %&gt;% head(5) ## # A tibble: 5 × 4 ## CVE_GEO EDO MUNI LOC ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 010010001 01 001 0001 ## 2 010010096 01 001 0096 ## 3 010010104 01 001 0104 ## 4 010010106 01 001 0106 ## 5 010010112 01 001 0112 Ya sea que se le especifique el número de caracteres que debe de contar para hacer un corte o que se le indique qué caracter debe identificar para hacer la separación, la función separate() puede dividir la columna indicada y crear nuevas a partir de la original. 3.4 Manipulación de texto Anteriormente se mencionaron algunas paqueterías que están incluidas dentro del conjunto Tidyverse. La paquetería stringr no forma parte del núcleo de Tidyverse, no obstante, sí pertenece a dicho conjunto y es ampliamente usada junto a las paqueterías nucleares debido a la facilidad que tiene para trabajar con cadenas, textos y expresiones regulares en general. Las expresiones regulares, también conocidas como regex o regexp, son patrones de texto repetidos en los datos y que son usados para operar con otras cadenas de texto. El resultado de dichas operaciones es obtener o resumir información, además de manipular y ordenar los conjuntos de datos. Entre las principales operaciones se encuentran los conteos, concatenaciones, separaciones, búsquedas, extracciones, imputaciones y sustituciones. 3.4.1 Caracteres especiales Debido a que la paquetería stringr trabaja principalmente con cadenas de texto, es importante tener en cuenta que los objetos de caracteres se definen a través de comillas, ya sea simples o dobles. La única diferencia está en el caso en que se pretendan usar comillas como parte del texto. En estos casos, la comilla simple es preferible para ser la que defina el texto. library(stringr) Cadena1 &lt;- &quot;Se definen cadenas a través de comillas dobles&quot; print(Cadena1) ## [1] &quot;Se definen cadenas a través de comillas dobles&quot; Cadena2 &lt;- &#39;También es posible con comillas simples y el resultado es el mismo&#39; print(Cadena2) ## [1] &quot;También es posible con comillas simples y el resultado es el mismo&quot; Cadena3 &lt;- &#39;Se pueden implementar &quot;comillas&quot; dentro de la cadena&#39; print(Cadena3) ## [1] &quot;Se pueden implementar \\&quot;comillas\\&quot; dentro de la cadena&quot; Cadena4 &lt;- &#39;Esta es otra forma de incluir \\&quot;comillas\\&quot; dentro de un texto&#39; print(Cadena4) ## [1] &quot;Esta es otra forma de incluir \\&quot;comillas\\&quot; dentro de un texto&quot; Como es posible apreciar en los ejemplos anteriores, la cadena 3 y 4 no se imprimen exactamente como se definieron. Podría parecer que existe un error, ya que aparecen diagonales antes de los caracteres deseados. Para resolver este problema, únicamente es necesario mandar llamar a la función cat() . Esta función sirve como intérprete de los caracteres que se definen en una cadena. La función cat() sirve como sustituto de la función print() (cuya función es imprimir de forma literal lo que existe dentro de las comillas), con la diferencia de que cat() interpreta las salidas de caracteres especiales en el texto y concatena las salidas finales con el separador específico que se indique. Por default, el separador es un espacio vacío, sin embargo, es posible modificarlo. A continuación se ejemplifica su uso. cat(Cadena3) ## Se pueden implementar &quot;comillas&quot; dentro de la cadena cat(Cadena4) ## Esta es otra forma de incluir &quot;comillas&quot; dentro de un texto A través del operador diagonal invertida “\\” también es posible definir comillas de manera literal. En caso de querer escribir textualmente una diagonal invertida, se necesitará escribir entre comillas dos diagonales invertidas “\\\\”. Existen otros caracteres espaciales que ayudan a mejorar el formato de las cadenas. Algunos de los caracteres más comunes son “\\n” (nueva línea) y “\\t” (tabulador). Todas las funciones que se presentarán a continuación, son posibles encontrarlas con otro nombre dentro de la paquetería básica. La ventaja que tienen las funciones de la paquetería stringr es que, los nombres son más intuitivos y comienzan con el mismo prefijo “str_”, haciendo que al escribir las primeras tres letras, la función de autocompletar de RStudio muestre una lista con sugerencias de los nombres de las posibles funciones a usarse. 3.4.2 Tamaño de cadena Es común que al procesar los conjuntos de datos, se requiera contar el número de caracteres que tiene una cadena. La paquetería básica cuenta con la función nchar() para realizar esta tarea. Con la ayuda de la paquetería stringr, es posible realizar esta misma tarea a través de la función str_length() str_length(&quot;Esta es una cadena de 35 caracteres&quot;) ## [1] 35 str_length(c(&quot;Un&quot;,&quot;vector&quot;,&quot;con&quot;,&quot;diferente&quot;,&quot;cantidad&quot;,&quot;de&quot;,&quot;carecteres&quot;,&quot;por&quot;,&quot;cadena&quot;)) ## [1] 2 6 3 9 8 2 10 3 6 Puede usarse la función tanto para objetos de cadenas individuales como para vectores. Existen muchos conjuntos de datos que durante su manipulación podemos encontrar claves alfanuméricas, que pueden ser necesarias unir información relevante o para crear claves de identificación única, por ejemplo poder unir el nombre completo de las personas dentro de una base de datos. Uniendo Nombre + Segundo Nombre + Primer Apellido + Segundo Apellido. Por otro lado de esto puede extraerse información sobre la CURP o en algunos otros ejemplos la unión de un ID + una matricula de carrera + el año pueden ser el registro para un estudiante o podrías querer replicar mensajes para alertas que dependan de una variable. 3.4.3 Concatenar Concatenar cadenas es una de las prácticas constantes en el manejo de conjuntos de datos. La función de la paquetería básica que se encarga de dicha tarea es la función paste(). A través de la paquetería stringr se logrará el concatenado mediante la función str_c(). Existe la opción de definir el caracter que hará la combinación de las cadenas mediante el argumento “sep”, que por default no deja ni un espacio entre las cadenas a combinar. Varios ejemplos se mostrarán a continuación. str_c(&quot;Concatenado&quot;, &quot;de&quot;,&quot;varias&quot;, &quot;cadenas&quot;, &quot;sin&quot;,&quot;espacios&quot;) ## [1] &quot;Concatenadodevariascadenassinespacios&quot; str_c(&quot;Concatenado&quot;,&quot;con&quot;,&quot;espacios&quot;, sep=&quot; &quot;) ## [1] &quot;Concatenado con espacios&quot; str_c(&quot;separando&quot;,&quot;mediante&quot;,&quot;otro&quot;,&quot;caracter&quot;,&quot;definido&quot;, sep=&quot;-&quot;) ## [1] &quot;separando-mediante-otro-caracter-definido&quot; str_c(&quot;Valores&quot;,str_c(&quot;09&quot;,&quot;006&quot;),&quot;anidados también se pueden concatenar&quot;,sep=&quot; &quot;) ## [1] &quot;Valores 09006 anidados también se pueden concatenar&quot; Es posible, al igual que con la función paste(), colapsar todas las cadenas de un vector en una sola cadena mediante el mismo parámetro: “collapse”. El caracter que divide a los elementos del vector debe ser especificado entre comillas, de lo contrario el valor por default será ” “. str_c(c(&quot;Colapsamiento&quot;, &quot;de&quot;,&quot;un&quot;,&quot;vector&quot;,&quot;de&quot;, &quot;cadenas&quot;, &quot;en&quot;,&quot;una&quot;, &quot;sola&quot;,&quot;cadena&quot;), collapse = &quot; &quot;) ## [1] &quot;Colapsamiento de un vector de cadenas en una sola cadena&quot; str_c(c(&quot;Colapsamiento&quot;, &quot;de&quot;,&quot;un&quot;,&quot;vector&quot;,&quot;de&quot;, &quot;cadenas&quot;, &quot;separado&quot;,&quot;por&quot;,&quot;signos&quot;), collapse = &quot;+&quot;) ## [1] &quot;Colapsamiento+de+un+vector+de+cadenas+separado+por+signos&quot; 3.4.4 Extraer y reemplazar Cuando únicamente interesa un subconjunto de alguna cadena para continuar con el manejo de la información, suele recurrirse a la expresión regular substr() de la paquetería básica para extraer este subconjunto de interés. Con stringr, la función para usar esta expresión regular es str_sub(). Esta función recibe como parámetros el texto desde el cuál se desea extraer el subconjunto, el índice que marque el inicio de la subcadena y el índice del final de la subcadena. str_sub(&quot;subcadenas&quot;, start = 4, end = 9) ## [1] &quot;cadena&quot; x &lt;- &quot;00000090060002&quot; str_sub(x, start = str_length(x) - 8, str_length(x)) ## [1] &quot;090060002&quot; str_sub(x, start = -9) ## [1] &quot;090060002&quot; Con la misma función str_sub() es posible sustituir parcial o totalmente la cadena “X” que sea introducida como argumento. Ésto se logra asignando a la subcadena seleccionada el valor que se usará para sustituir. En los siguientes ejemplos se muestra cómo modificar la cadena “substring” y la cadena “090060002”. En el primer caso, a partir de la cadena de caracteres “substring” se procede a generar una nueva cadena al sustituir las letras 4 a la 9, reemplazando así la subcadena “string” por “cadena”, dando lugar a “subcadena”. En el segundo ejemplo, es la clave correspondiente a los dígitos 3 a 5 los que cambian para dar lugar a otra clave numérica. y &lt;- &quot;substring&quot; str_sub(y, start = 4, end = 9) &lt;- &quot;cadena&quot;; y ## [1] &quot;subcadena&quot; x &lt;- &quot;090060002&quot; str_sub(x, start = str_length(x)-6,str_length(x)-4) &lt;- &quot;555&quot;; x ## [1] &quot;095550002&quot; Lo anterior es ampliamente usado en el proceso de limpieza de los datos. A veces es posible encontrar errores ortográficos o los llamados errores “de dedo” (hacen referencia a errores accidentales al escribir) que se dan a la hora de capturar la información. Cuando una gran cantidad de datos presentan el mismo error, es buena idea recurrir a esta función. 3.4.5 Expresiones regulares En el estudio de las expresiones regulares se puede encontrar de manera sobresaliente la aplicación de los patrones coincidentes, los cuáles sirven para describir y descubrir coincidencias de interés en conjuntos específicos a partir de cadenas de caracteres. Con el fin de comprender y visualizar de manera práctica el uso de los patrones coincidentes con las expresiones regulares, se hará uso de la función str_view(), que permite distinguir los elementos coincidentes de un vector con un patrón de caracteres descrito. La paquetería stringr cuenta con tres conjuntos predefinidos de oraciones y palabras que sirven para ejemplificar el uso de las expresiones regulares. Estos conjuntos llevan el nombre de sentences, words y fruit, su contenido es de sentencias, palabras y nombres de frutas. En los tres casos, estos conjuntos han sido escritos en inglés. Los conjuntos sirven bien para ejemplificar el uso de las siguientes funciones y de las expresiones regulares. head(sentences, 10) ## [1] &quot;The birch canoe slid on the smooth planks.&quot; ## [2] &quot;Glue the sheet to the dark blue background.&quot; ## [3] &quot;It&#39;s easy to tell the depth of a well.&quot; ## [4] &quot;These days a chicken leg is a rare dish.&quot; ## [5] &quot;Rice is often served in round bowls.&quot; ## [6] &quot;The juice of lemons makes fine punch.&quot; ## [7] &quot;The box was thrown beside the parked truck.&quot; ## [8] &quot;The hogs were fed chopped corn and garbage.&quot; ## [9] &quot;Four hours of steady work faced us.&quot; ## [10] &quot;Large size in stockings is hard to sell.&quot; head(words, 20) ## [1] &quot;a&quot; &quot;able&quot; &quot;about&quot; &quot;absolute&quot; &quot;accept&quot; &quot;account&quot; ## [7] &quot;achieve&quot; &quot;across&quot; &quot;act&quot; &quot;active&quot; &quot;actual&quot; &quot;add&quot; ## [13] &quot;address&quot; &quot;admit&quot; &quot;advertise&quot; &quot;affect&quot; &quot;afford&quot; &quot;after&quot; ## [19] &quot;afternoon&quot; &quot;again&quot; head(fruit, 20) ## [1] &quot;apple&quot; &quot;apricot&quot; &quot;avocado&quot; &quot;banana&quot; &quot;bell pepper&quot; ## [6] &quot;bilberry&quot; &quot;blackberry&quot; &quot;blackcurrant&quot; &quot;blood orange&quot; &quot;blueberry&quot; ## [11] &quot;boysenberry&quot; &quot;breadfruit&quot; &quot;canary melon&quot; &quot;cantaloupe&quot; &quot;cherimoya&quot; ## [16] &quot;cherry&quot; &quot;chili pepper&quot; &quot;clementine&quot; &quot;cloudberry&quot; &quot;coconut&quot; Para realizar una coincidencia de patrones, es necesario ingresar como argumento el vector de cadenas de caracteres en donde se desea hacer la búsqueda. Como segundo argumento, se ingresa el patrón con el cuál se desea buscar las coincidencias. Para que únicamente se muestren las coincidencias exitosas se debe agregar el argumento match = TRUE. str_view(sentences,&quot;great&quot;, match = TRUE) str_view(sentences,&quot;this&quot;, match = TRUE) Como es posible apreciarse en los dos ejemplos anteriores, la función str_view() filtra los casos coincidentes con los patrones “great” y “this”. Esta coincidencia no necesariamente es perfecta. Basta con que un subconjunto de la sentencia coincida con el patrón definido para que la función str_view() reconozca como coincidencia válida a toda la cadena. En caso de no contar con todos los caracteres del patrón deseado o de querer ver todas las combinaciones que tengan un patrón adyacente común, es posible ingresar un “comodín” a través del caracter punto “.”, el cuál coincidirá con cualquier caracter. Este caracter especial puede usarse al principio, al final o de manera intermedia dentro del patrón. str_view(sentences,&quot;up.&quot;, match = TRUE) Como se puede observar, esta es una manera de detectar subconjuntos de particular interés. Si \\ es usado para escapar un caracter especial, ¿cómo hacemos para hacer match con el caracter \\ literalmente? Necesita ser escapado, creando la expresión regular \\\\. Para crear la expresión regular, se necesita usar un string, el cual también requiere ser escapado. Esto significa que para hacer match con  se necesita escribir “\\\\\\\\” cuatro diagonales invertidas para hacer match con una sola diagonal invertida \\. Los primeros 2 diagonales son para crear la expresión regular, la tercera es para escapar el caracter especial siguiente, el cual corresponde a la cuarta diagonal. Anclajes Como se mencionó anteriormente, por default, la coincidencia de patrones se efectuará sobre cualquier subconjunto de la cadena de caracteres. Es posible definir el caracter inicial y/o el caracter final con el cuál se buscará la coincidencia de patrones. Esto se logra al hacer uso de los siguientes caracteres especiales. ^ Para hacer coincidir el inicio de la cadena $ Para hacer coincidir el final de la cadena str_view(words,&quot;^y&quot;, match = TRUE) str_view(words,&quot;x$&quot;, match = TRUE) En caso de desear hacer coincidir todo el contenido de la cadena, deben usarse ambos caracteres especiales para definir el inicio y el final del patrón coincidente como se muestra a continuación. vector &lt;- c(&quot;nulo aprendizaje&quot;, &quot;poco aprendizaje&quot;,&quot;aprendizaje moderado&quot;,&quot;aprendizaje&quot;, &quot;aprendizaje total&quot;) str_view(vector,&quot;^aprendizaje$&quot;) Wickham menciona que, además del caracter especial punto “.”, existen otros cuatro que resultan muy útiles para mostrar patrones particulares. \\d Hace coincidir cualquier dígito. \\s Hace coincidir cualquier espacio en blanco (espacio, tabulador, salto de línea). [abc] Hace coincidir a, b ó c. [^abc] Coincide con cualquier cosa excepto a, b ó c. Al momento de definir cualquiera de estos 4 patrones coincidentes será necesario usar doble diagonal invertida. Por ejemplo, “\\\\d” será el patrón para encontrar la coincidencia con cualquier dígito. cadena &lt;- &quot;El año 2022 será un año de mucho crecimiento.&quot; str_view(cadena,&quot;\\\\d&quot;, match = T) Wickham menciona que, “es posible usar alternancias para elegir entre uno o más patrones alternativos. Por ejemplo, abc|d..f coincidirá con”abc” o con “deaf”. Haciendo notar que la prioridad para el operador “|” es baja, por lo que abc|xyz coincide con abc o xyz, no con abcyz o abxyz. Al igual que en las matemáticas, si existe ambigüedad en la prioridad de las operaciones, usar paréntesis lo hará todo más claro.” str_view(c(&quot;tamaño&quot;,&quot;tasa&quot;,&quot;tata&quot;,&quot;taza&quot;,&quot;tapa&quot;),&quot;ta(s|z)a&quot;) Repeticiones El siguiente paso en complejidad para las expresiones regulares es, controlar el número de veces que aparece un patrón coincidente. ?: Se repite 0 o 1. +: Se repite 1 o más veces. *: Se repite 0 a más veces. x &lt;- &quot;1888 es el año más largo en números romanos: MDCCCLXXXVIII&quot; str_view(x,&quot;XX?&quot;) str_view(x,&quot;XX+&quot;) str_view(x,&quot;C[LX]+&quot;) Es posible especificar el número de repeticiones que se desea hacer coincidir un patrón. Ya sea de manera exacta o dentro de un intervalo. Esta repetición en el patrón se define de la siguiente manera. {n}: exactamente n veces {n,}: n o más veces {,m}: a lo más m veces {n,m}: entre n y m veces str_view(x,&quot;X{3}&quot;) str_view(x,&quot;X{1,2}&quot;) Match múltiple / nulo Es interesante el siguiente ejemplo. ¿Qué es lo que sucede? str_view(x,&quot;M*&quot;) Recordemos que los operadores * o ? busca un patrón que puede o no encontrarse dentro de la cadena de interés. En el caso anterior, la letra “M” puede o no encontrarse en la cadena “x”. En el ejemplo anterior, en cada posible caracter existe el hallazgo o no hallazgo de la letra “M”, de forma que el primer match que hace se encuentra al principio de la cadena. Una limitante de la función str_view() es que únicamente resalta la primer coincidencia encontrada con el patrón regular, sin embargo, la función str_view_all() se encarga de resaltar todas las coincidencias en la cadena, como se muestra a continuación. str_view_all(x,&quot;M*&quot;) Herramientas Una vez que se han visto los aspectos básicos de las expresiones regulares, es posible utilizar los patrones coincidentes y combinarlos para aplicarlos en problemas reales. Algunas de las aplicaciones más comunes son: Determinar cuáles cadenas coinciden con un patrón. Encontrar la posición de las coincidencias. Extraer el contenido de las coincidencias. Reemplazar coincidencias con nuevos valores. Dividir una cadena basándose en una coincidencia. A continuación se analizarán las funciones que permitirán realizar las acciones anteriores. 3.4.6 Detectar coincidencias Para determinar las cadenas de caracteres dentro de un vector que coinciden con un patrón, es posible utilizar la función str_detect(). La función regresará un vector booleano una vez que se introduzcan como argumentos el vector con cadenas y el patrón con el que se desea hacer la coincidencia. Aplicando esta función a un marco de muestreo es posible apreciar si los elementos coinciden o no con algún patrón indicado. En el siguiente ejemplo se puede apreciar cuáles frutas tienen entre sus letras una “a” o una “u”. str_detect(fruit, &quot;[au]&quot;) ## [1] TRUE TRUE TRUE TRUE FALSE FALSE TRUE TRUE TRUE TRUE FALSE TRUE ## [13] TRUE TRUE TRUE FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE TRUE ## [25] TRUE TRUE TRUE TRUE FALSE TRUE FALSE FALSE FALSE TRUE TRUE TRUE ## [37] FALSE TRUE TRUE TRUE TRUE TRUE TRUE FALSE FALSE TRUE FALSE TRUE ## [49] TRUE TRUE TRUE TRUE FALSE TRUE TRUE TRUE TRUE TRUE TRUE FALSE ## [61] TRUE TRUE TRUE TRUE FALSE TRUE TRUE TRUE TRUE TRUE TRUE FALSE ## [73] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE A continuación se puede apreciar una aplicación de la función str_detect(). Se usará el vector booleano para seleccionar el subconjunto del objeto fruit que tiene entre su nombre una letra “l” o una letra “o”. fruit[str_detect(fruit, &quot;[lo]&quot;)] ## [1] &quot;apple&quot; &quot;apricot&quot; &quot;avocado&quot; ## [4] &quot;bell pepper&quot; &quot;bilberry&quot; &quot;blackberry&quot; ## [7] &quot;blackcurrant&quot; &quot;blood orange&quot; &quot;blueberry&quot; ## [10] &quot;boysenberry&quot; &quot;canary melon&quot; &quot;cantaloupe&quot; ## [13] &quot;cherimoya&quot; &quot;chili pepper&quot; &quot;clementine&quot; ## [16] &quot;cloudberry&quot; &quot;coconut&quot; &quot;damson&quot; ## [19] &quot;dragonfruit&quot; &quot;eggplant&quot; &quot;elderberry&quot; ## [22] &quot;feijoa&quot; &quot;goji berry&quot; &quot;gooseberry&quot; ## [25] &quot;honeydew&quot; &quot;huckleberry&quot; &quot;jambul&quot; ## [28] &quot;lemon&quot; &quot;lime&quot; &quot;loquat&quot; ## [31] &quot;lychee&quot; &quot;mango&quot; &quot;mulberry&quot; ## [34] &quot;olive&quot; &quot;orange&quot; &quot;pamelo&quot; ## [37] &quot;passionfruit&quot; &quot;persimmon&quot; &quot;physalis&quot; ## [40] &quot;pineapple&quot; &quot;plum&quot; &quot;pomegranate&quot; ## [43] &quot;pomelo&quot; &quot;purple mangosteen&quot; &quot;rock melon&quot; ## [46] &quot;salal berry&quot; &quot;tamarillo&quot; &quot;ugli fruit&quot; ## [49] &quot;watermelon&quot; De esta manera se va filtrando un marco muestral para quedarse únicamente con los elementos que coincidan con un patrón coincidente. 3.4.7 Contabilizar coincidencias Una variación de la función anterior, es la función str_count(). Esta función en lugar de devolver un vector lógico, devuelve un vector de conteos que corresponde al número de veces que detectó una coincidencia para cada cadena dentro de un vector. La manera de usarse es análoga a la función str_detect(). str_count(fruit, &quot;[aeiou]&quot;) ## [1] 2 3 4 3 3 2 2 3 5 3 3 4 4 5 4 1 4 4 3 3 2 3 2 2 2 4 3 2 3 4 1 3 4 2 4 3 3 3 ## [39] 3 2 3 4 3 2 2 3 2 4 2 2 4 1 3 3 3 3 5 2 2 3 2 4 1 5 3 6 3 3 3 2 3 3 3 3 3 2 ## [77] 4 4 4 4 En el ejemplo anterior se puede apreciar el número de vocales que existen dentro de cada una de los nombres de las frutas. Una de las aplicaciones más comunes para esta función, se encuentra en los estudios de análisis de textos. A continuación, se puede apreciar el promedio de vocales que son usadas dentro de los nombres de frutas en el objeto fruit. mean(str_count(fruit, &quot;[aeiou]&quot;)) ## [1] 3.0125 Otro ejemplo más complejo de esta función y sus aplicaciones es el siguiente: tabla &lt;- tibble( &quot;Vocal&quot; = c(&quot;a&quot;,&quot;e&quot;,&quot;i&quot;,&quot;o&quot;,&quot;u&quot;), &quot;Conteos&quot; = c(sum(str_count(fruit, &quot;a&quot;)),sum(str_count(fruit, &quot;e&quot;)), sum(str_count(fruit, &quot;i&quot;)),sum(str_count(fruit, &quot;o&quot;)), sum(str_count(fruit, &quot;u&quot;)))) %&gt;% mutate(Porcentaje = Conteos/sum(Conteos)) tabla ## # A tibble: 5 × 3 ## Vocal Conteos Porcentaje ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 a 65 0.270 ## 2 e 72 0.299 ## 3 i 34 0.141 ## 4 o 36 0.149 ## 5 u 34 0.141 De esta manera es posible visualizar los conteos totales que tuvo cada vocal. Se calculó el porcentaje de aparición que tiene cada vocal con respecto al total de vocales en el conjunto. Un dato curioso que es posible concluir con este ejemplo, es que, las vocales “a” y “e” aparecen casi dos veces más que el resto de las vocales en el conjunto fruit. El anterior es un ejemplo sencillo que tiene por objetivo ilustrar el aprovechamiento de las funciones, sin embargo, una aplicación más robusta podría permitir que a través de los tweets emitidos a candidatos políticos, se realice un análisis de sentimientos, en el cual cada tweet sea asociado a uno o más sentimientos tales como: alegría, enojo, miedo, tristeza, aversión, etc. Posteriormente, de manera análoga a los conteos de vocales con porcentajes, se podría analizar la distribución de los sentimientos asociados a las opiniones de cada uno de los candidatos políticos. Adicionalmente, se podría realizar un análisis en donde se muestren las palabras que más se repiten al expresarse de un candidato. 3.4.8 Extraer coincidencias Cuando se desea identificar y extraer un subgrupo particular de elementos que cumplan con cierta condición definida a través de un patrón coincidente, la función str_extract() es la mejor opción para realizar esta tarea. Si se desea extraer el subconjunto de sentencias conjuntivas o disyuntivas es necesario definir el patrón coincidente con el cuál se compararán las sentencias para ser extraídas. patron &lt;- &quot;( and )|( or )&quot; Por su traducción en inglés, “and” y “or” son los conectores “y” y “o” respectivamente. Primero se filtrarán las oraciones que cumplen con la condición de que las conjunciones “and” u “or” se encuentren dentro y posteriormente se extraerán los elementos coincidentes. coincidencias &lt;- str_subset(sentences, patron) head(coincidencias,10) ## [1] &quot;The hogs were fed chopped corn and garbage.&quot; ## [2] &quot;Kick the ball straight and follow through.&quot; ## [3] &quot;Smoky fires lack flame and heat.&quot; ## [4] &quot;The fish twisted and turned on the bent hook.&quot; ## [5] &quot;Press the pants and sew a button on the vest.&quot; ## [6] &quot;The colt reared and threw the tall rider.&quot; ## [7] &quot;It snowed, rained, and hailed the same morning.&quot; ## [8] &quot;The wrist was badly strained and hung limp.&quot; ## [9] &quot;Hop over the fence and plunge in.&quot; ## [10] &quot;Cars and busses stalled in snow drifts.&quot; Apenas 123 sentencias cumplen con la condición. Éstas representan el 17.08% del total. Para extraer los elementos coincidentes, la función str_extract() se usa de la siguiente manera: str_extract(coincidencias, patron) ## [1] &quot; and &quot; &quot; and &quot; &quot; and &quot; &quot; and &quot; &quot; and &quot; &quot; and &quot; &quot; and &quot; &quot; and &quot; &quot; and &quot; ## [10] &quot; and &quot; &quot; and &quot; &quot; and &quot; &quot; and &quot; &quot; and &quot; &quot; and &quot; &quot; and &quot; &quot; and &quot; &quot; and &quot; ## [19] &quot; and &quot; &quot; and &quot; &quot; and &quot; &quot; and &quot; &quot; and &quot; &quot; and &quot; &quot; and &quot; &quot; and &quot; &quot; and &quot; ## [28] &quot; and &quot; &quot; or &quot; &quot; or &quot; &quot; and &quot; &quot; and &quot; &quot; and &quot; &quot; and &quot; &quot; and &quot; &quot; and &quot; ## [37] &quot; and &quot; &quot; and &quot; &quot; and &quot; &quot; and &quot; &quot; and &quot; &quot; or &quot; &quot; and &quot; &quot; and &quot; &quot; and &quot; ## [46] &quot; or &quot; &quot; and &quot; &quot; or &quot; &quot; and &quot; &quot; and &quot; &quot; and &quot; &quot; and &quot; &quot; and &quot; &quot; and &quot; ## [55] &quot; and &quot; &quot; and &quot; &quot; and &quot; &quot; and &quot; &quot; and &quot; &quot; and &quot; &quot; and &quot; &quot; and &quot; &quot; and &quot; ## [64] &quot; and &quot; &quot; and &quot; &quot; and &quot; &quot; and &quot; &quot; and &quot; &quot; and &quot; &quot; and &quot; &quot; and &quot; &quot; and &quot; ## [73] &quot; and &quot; &quot; and &quot; &quot; and &quot; &quot; and &quot; &quot; and &quot; &quot; and &quot; &quot; and &quot; &quot; and &quot; &quot; and &quot; ## [82] &quot; and &quot; &quot; and &quot; &quot; and &quot; &quot; and &quot; &quot; and &quot; &quot; or &quot; &quot; and &quot; &quot; and &quot; &quot; or &quot; ## [91] &quot; and &quot; &quot; and &quot; &quot; and &quot; &quot; and &quot; &quot; and &quot; &quot; and &quot; &quot; and &quot; &quot; and &quot; &quot; and &quot; ## [100] &quot; and &quot; &quot; and &quot; &quot; and &quot; &quot; and &quot; &quot; and &quot; &quot; and &quot; &quot; or &quot; &quot; and &quot; &quot; and &quot; ## [109] &quot; and &quot; &quot; and &quot; &quot; and &quot; &quot; and &quot; &quot; and &quot; &quot; and &quot; &quot; and &quot; &quot; and &quot; &quot; and &quot; ## [118] &quot; and &quot; &quot; and &quot; &quot; and &quot; &quot; and &quot; &quot; and &quot; &quot; and &quot; Con el código anterior se logra extraer el patrón coincidente de cada una de las sentencias, sin embargo, sólo se logra extraer la primer coincidencia de cada cadena en donde hubo al menos una coincidencia. Para extender este resultado a todos los patrones coincidentes dentro de la sentencia, se debe agregar el sufijo “_all” a la función. La función str_extract_all() extraerá todas las coincidencias y las agrupará en un objeto cuya estructura será la de una “lista”. Con la función head() se logrará visualizar los 5 primeros elementos de la lista que guarda el resultado generado por la función str_extract_all(). head(str_extract_all(coincidencias,patron), 5) ## [[1]] ## [1] &quot; and &quot; ## ## [[2]] ## [1] &quot; and &quot; ## ## [[3]] ## [1] &quot; and &quot; ## ## [[4]] ## [1] &quot; and &quot; ## ## [[5]] ## [1] &quot; and &quot; Un formato más compacto del resultado anterior se logra al agregar el parámetro “simplfy = TRUE” dentro de la función de extracción. Para visualizar aleatoriamente diez de los resultados generados, se puede hacer uso de la función sample_n() de la librería dplyr. dplyr::sample_n( tbl = as_tibble(str_extract_all(coincidencias, patron, simplify = TRUE)), size = 10 ) ## # A tibble: 10 × 2 ## V1 V2 ## &lt;chr&gt; &lt;chr&gt; ## 1 &quot; and &quot; &quot;&quot; ## 2 &quot; and &quot; &quot;&quot; ## 3 &quot; and &quot; &quot;&quot; ## 4 &quot; or &quot; &quot;&quot; ## 5 &quot; and &quot; &quot;&quot; ## 6 &quot; and &quot; &quot;&quot; ## 7 &quot; or &quot; &quot;&quot; ## 8 &quot; and &quot; &quot;&quot; ## 9 &quot; and &quot; &quot;&quot; ## 10 &quot; and &quot; &quot;&quot; Si en alguna cadena existieran más de dos patrones coincidentes, aparecería en la segunda columna el patrón encontrado (e.g., elemento en el séptimo renglón), de lo contrario, el elemento de la segunda columna permanecerá vacío a través de dos comillas. El resultado será un objeto de la clase data.frame que tendrá tantas columnas como coincidencias máximas hayan existido en una sentencia. La siguientes línea de código permite hacer conteos del número de veces que el patrón coincidente fue detectado en el vector de oraciones. La función str_count() indicará el número de veces que el patrón fue detectado en cada oración. Finalmente, el vector numérico se suma. sum(str_count(coincidencias, pattern = patron)) ## [1] 126 Con este dato, se puede decir que a lo largo de 123 oraciones, se puede encontrar 126 veces el patrón indicado. Este tipo de análisis nos permite hacer reportes como el siguiente: En promedio, cada oración tiene incluido 1.02 veces el patrón coincidente. A continuación, se revisará el modo de detectar y reemplazar patrones regulares. 3.4.9 Reemplazar coincidencias A menudo es necesario reemplazar algunos patrones. Ya sea derivado de un error en las cadenas de texto o por interés de presentar los resultados de una manera distinta, identificar y sustituir un subconjunto de caracteres es algo que la función str_replace() de la paquetería stringr puede hacer. Como todas las funciones vistas hasta el momento provenientes de la paquetería stringr, la función str_replace() recibe el objeto con las cadenas de caracteres originales y a través de un patrón de texto se hace la búsqueda de las coincidencias. Es posible incluir más de 1 patrón y asignar el nuevo texto que sustituirá al anterior para cada uno de los patrones definidos. La función str_replace() hará la sustitución de un solo patrón coincidente y la función str_replace_all() lo hará para todos los patrones definidos. # Ejemplos de cambio de códigos a palabras o invertido x &lt;- c(&quot;1&quot;,&quot;2&quot;,&quot;1&quot;,&quot;2&quot;,&quot;1&quot;,&quot;2&quot;,&quot;1&quot;,&quot;2&quot;,&quot;1&quot;,&quot;2&quot;) x1 &lt;- str_replace(x, &quot;1&quot;,&quot;Hombre&quot;) print(x1) ## [1] &quot;Hombre&quot; &quot;2&quot; &quot;Hombre&quot; &quot;2&quot; &quot;Hombre&quot; &quot;2&quot; &quot;Hombre&quot; &quot;2&quot; ## [9] &quot;Hombre&quot; &quot;2&quot; x2 &lt;- str_replace(x1, &quot;2&quot;,&quot;Mujer&quot;) print(x2) ## [1] &quot;Hombre&quot; &quot;Mujer&quot; &quot;Hombre&quot; &quot;Mujer&quot; &quot;Hombre&quot; &quot;Mujer&quot; &quot;Hombre&quot; &quot;Mujer&quot; ## [9] &quot;Hombre&quot; &quot;Mujer&quot; # Es posible replicar el resultado en un solo paso mediante str_replace_all str_replace_all(x, c(&quot;1&quot; = &quot;Hombre&quot;, &quot;2&quot; = &quot;Mujer&quot;)) ## [1] &quot;Hombre&quot; &quot;Mujer&quot; &quot;Hombre&quot; &quot;Mujer&quot; &quot;Hombre&quot; &quot;Mujer&quot; &quot;Hombre&quot; &quot;Mujer&quot; ## [9] &quot;Hombre&quot; &quot;Mujer&quot; 3.4.10 Divisiones mediante patrones La información se presenta en diferentes formatos todo el tiempo. A veces cada variable tiene su propia columna, pero a veces la información está mezclada y es necesario dividirla a fin de trabajar mejor con ella. Un caso recurrente en donde se presenta esta operación es con las fechas. El formato de una fecha a menudo se presenta como dd/mm/aaaa. Bajo este formato se puede encontrar tres datos en uno solo (día, mes y año). Para dividirlo, se podría utilizar el caracter “/” como patrón de coincidencia que permita dividir los datos en tres columnas separadas. Se debe agregar el parámetro “simplify = T” para poder simplificar los resultados y visualizarlos en un objeto data.frame(). La manera de hacerlo es la siguiente: Primero se genera un vector con fechas fechas &lt;- c(&quot;15/11/1991&quot;,&quot;20/11/1981&quot;,&quot;04/02/1966&quot;,&quot;01/10/1958&quot;,&quot;23/04/1992&quot;);fechas ## [1] &quot;15/11/1991&quot; &quot;20/11/1981&quot; &quot;04/02/1966&quot; &quot;01/10/1958&quot; &quot;23/04/1992&quot; Ahora, se generan tres columnas, una para el campo “Día”, otra para el campo “Mes” y otra para “Año” str_split(string = fechas,pattern = &quot;/&quot;,n = 3, simplify = T) %&gt;% as_tibble() %&gt;% rename(day = V1, month = V2, year = V3) ## # A tibble: 5 × 3 ## day month year ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 15 11 1991 ## 2 20 11 1981 ## 3 04 02 1966 ## 4 01 10 1958 ## 5 23 04 1992 3.4.11 Localización de coincidencias Para poder hacer operaciones con algunas cadenas de caracteres, en ocasiones es necesario proporcionar los índices que denotan el inicio y el término de algún patrón de caracteres. La función str_locate() devuelve dichos índices una vez que se le haya proporcionado como argumento el patrón coincidente y el vector de cadenas de texto. x &lt;- c(&quot;090020001-123&quot;,&quot;090001-512&quot;,&quot;09002-1236&quot;,&quot;90020001-1237&quot;,&quot;09001-123811&quot;) str_locate(x,&quot;-\\\\d{2,}&quot;) ## start end ## [1,] 10 13 ## [2,] 7 10 ## [3,] 6 10 ## [4,] 9 13 ## [5,] 6 12 El ejemplo anterior muestra la manera en la que se localizan los índices iniciales y finales de los caracteres posteriores al símbolo “-”, que cumplan con tener al menos 2 dígitos. Hay que tomar en cuenta que bajo esta forma de extraer los índices, la posición del caracter “-” está incluida y es ahí donde inicia el conteo. Habiendo platicado de esto, podemos hablar de uno de los temas importantes en muchas ocasiones complejos de la manipulación de datos… las fechas. 3.5 Manipulación de tiempo 3.5.1 Lectura y creación de datos temporales Una de la tareas más comunes en el análisis de datos es la manipulación de fechas y horas. Existe una infinidad de situaciones en donde saber operar con esta clase de datos es vital para el análisis de datos general. El primer paso es lograr identificar una cadena de caracteres como un dato temporal. Veamos un primer ejemplo: library(lubridate) fechas &lt;- c(&quot;2018/09/11&quot;, &quot;1992/04/23&quot;, &quot;1966/02/04&quot;, &quot;1958/10/01&quot;, &quot;1991/11/15&quot;) class(fechas) ## [1] &quot;character&quot; class(as_date(fechas)) ## [1] &quot;Date&quot; Como puede apreciarse, los elementos del vector ahora son de la clase “Date”. La función as_date() transforma caracteres a fechas. Otra forma de realizar la implementación de datos temporales es directamente la creación de datos temporales a través de las funciones make_date() y make_datetime(). Esta función recibe los parámetros de cada unidad temporal y crea el dato adecuado para representarlo, ya sea date o datetime. make_date(year = 1999, month = 06, day = 16) ## [1] &quot;1999-06-16&quot; make_datetime(year = 1999:2001, month = 06, day = 16, hour = 10:12, min = 37, sec = 15) ## [1] &quot;1999-06-16 10:37:15 UTC&quot; &quot;2000-06-16 11:37:15 UTC&quot; ## [3] &quot;2001-06-16 12:37:15 UTC&quot; El segundo ejemplo muestra la capacidad de estas funciones para crear una secuencia de fechas a partir de vectores de cada posible unidad temporal (día, mes año, hora, etc) Algo que puede apreciarse en los ejemplos anteriores y que es necesario mencionar, es que el formato universal de fecha se escribe: yyyy/mm/dd. En países como USA el formato es mm/dd/yyyy y en México y otras partes del mundo puede escribirse comúnmente dd/mm/yyyy. Este tipo de diferencia a veces puede llegar a generar confusión sobre la fecha exacta en cuestión, más aún si se abrevia el año a dos caracteres yy. La librería lubridate ofrece funciones para lidiar con el formato de lectura. Estas funciones son: dmy / dmy_h / dmy_hm / dmy_hms mdy / mdy_h / mdy_hm / mdy_hms ymd / ymd_h / ymd_hm / ymd_hms hm / hms Con todas las funciones mencionadas anteriormente se puede leer cualquier cadena de caracteres que contenga el formato especificado. A continuación, un ejemplo: mdy(&quot;11/25/1982&quot;) ## [1] &quot;1982-11-25&quot; Automáticamente cualquiera de las funciones antes mencionadas transforma el formato específico al formato universal. 3.5.2 Extracción de datos temporales Una vez que los datos temporales ya se encuentran creados, es importante saber la manera de extraer información particular de nuestro interés. En esta sección se revisará la manera de extraer cualquier unidad temporal a partir de una cadena completa de caracteres. a través de lubridate, las funciones para extracción de sub-unidades temporales son intuitivas. tiempo &lt;- make_datetime(year = 2004, month = 9, day = 25, hour = 11, min = 30, sec = 1) year(tiempo) ## [1] 2004 month(tiempo) ## [1] 9 month(tiempo, label = T, abbr = F) ## [1] septiembre ## 12 Levels: enero &lt; febrero &lt; marzo &lt; abril &lt; mayo &lt; junio &lt; ... &lt; diciembre day(tiempo) ## [1] 25 hour(tiempo) ## [1] 11 minute(tiempo) ## [1] 30 second(tiempo) ## [1] 1 Después de las unidades temporales básicas, es posible también extraer día de la semana o día del mes, si es de interés. wday(tiempo) ## [1] 7 wday(tiempo, label = T, abbr = F) ## [1] sábado ## 7 Levels: domingo &lt; lunes &lt; martes &lt; miércoles &lt; jueves &lt; ... &lt; sábado mday(tiempo) ## [1] 25 yday(tiempo) ## [1] 269 3.5.3 Operaciones temporales Las operaciones aritméticas entre datos temporales es cotidiano en todo momento cuando se analizan datos. Operaciones de sumas y restas de fechas y horas que permiten conocer la longitud de tiempo en dos momentos es una tarea que puede resolverse a través de los operadores aritméticos y lógicos. Resta entre fechas as_date(&quot;1991/11/15&quot;) - as_date(&quot;1992/04/23&quot;) ## Time difference of -160 days as_date(&quot;1992/04/23&quot;) - as_date(&quot;1991/11/15&quot;) ## Time difference of 160 days Suma y resta de días as_date(&quot;1991/11/15&quot;) + 365 ## [1] &quot;1992-11-14&quot; Operadores lógicos as_date(&quot;1992/04/23&quot;) &gt; as_date(&quot;1991/11/15&quot;) ## [1] TRUE as_date(&quot;1992/04/23&quot;) &lt; as_date(&quot;1991/11/15&quot;) ## [1] FALSE Cada una de las operaciones vistas en este capítulo son compatibles con dplyr y el resto de las librerías tidyverse. 3.6 Iteraciones Los procesos en R muchas veces son iterativos en distintas partes del desarrollo de una solución. Existe una librería que facilita el entendimiento, orden, legibilidad y limpieza a la orden de iterar. La librería lleva por nombre purrr. Purrr presenta funciones de mapeo, así como algunas funciones nuevas para manipular listas. Mientras que el caballo de batalla de dplyr es el marco de datos (data.frame / tibble), el caballo de batalla de purrr es la lista. Recordemos que un vector es una forma de almacenar muchos elementos individuales (un solo número o un solo carácter o cadena) del mismo tipo en un solo objeto. Un marco de datos (data.frame / tibble) es una forma de almacenar muchos vectores de la misma longitud pero posiblemente de diferentes tipos juntos en un solo objeto. Una lista es una forma de almacenar muchos objetos de cualquier tipo (por ejemplo, marcos de datos, diagramas, vectores) juntos en un solo objeto. Aquí hay un ejemplo de una lista que tiene tres elementos: un solo número, un vector y un marco de datos mi_2da_lista &lt;- list( my_number = 5, my_vector = c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;), my_dataframe = data.frame( a = 1:3, b = c(&quot;q&quot;, &quot;b&quot;, &quot;z&quot;), c = c(&quot;bananas&quot;, &quot;are&quot;, &quot;so very great&quot;)) ) mi_2da_lista ## $my_number ## [1] 5 ## ## $my_vector ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; ## ## $my_dataframe ## a b c ## 1 1 q bananas ## 2 2 b are ## 3 3 z so very great Un marco de datos es en realidad un caso especial de una lista donde cada elemento de la lista es un vector de la misma longitud. Una función de mapeo es aquella que aplica la misma acción/función a cada elemento de un objeto (por ejemplo, cada entrada de una lista o un vector, o cada una de las columnas de un marco de datos). ¡¡ RECORDAR !! La funciones que serán revisadas son súper útiles para realizar una acción de forma iterativa en las entradas de un vector o lista sin tener que escribir un bucle for. La convención de nomenclatura de las funciones de mapeo es tal que el tipo de salida se especifica mediante el término que sigue al guión bajo en el nombre de la función. map(.x, .f) Es la función principal y regresa una lista map_df(.x, .f) regresa un data frame map_dbl(.x, .f) regresa un vector numérico (double) map_chr(.x, .f) regresa un vector de caracter map_lgl(.x, .f) regresa un vector lógico De acuerdo con la forma del tidyverse, el primer argumento de cada función de mapeo es siempre el objeto de datos sobre el que desea mapear, y el segundo argumento es siempre la función que desea aplicar iterativamente a cada elemento del objeto de entrada. 3.6.1 Bucles Fundamentalmente, los mapeos son para iteraciones. En el ejemplo de abajo, se iterará sobre el vector 1, 4, 7 añadiendo el valor 10 a cada entrada del vector a través de una función llamada addTen(). Esta función será aplicada a cada número único, el cual llamaremos “.x”. addTen &lt;- function(.x) { return(.x + 10) } La función map() de abajo, itera sobre todos los enteros aplicando la función addTen() a los elementos del vector y regresa la salida como una lista. library(purrr) map( .x = c(1, 4, 7), .f = addTen ) ## [[1]] ## [1] 11 ## ## [[2]] ## [1] 14 ## ## [[3]] ## [1] 17 Hay que notar en los siguientes ejemplos que el mecanismo es funcional independientemente de si se trata de una lista o un marco de datos: map(list(1, 4, 7), addTen) ## [[1]] ## [1] 11 ## ## [[2]] ## [1] 14 ## ## [[3]] ## [1] 17 map(data.frame(a = 1, b = 4, c = 7), addTen) ## $a ## [1] 11 ## ## $b ## [1] 14 ## ## $c ## [1] 17 la estructura resultante por default es una lista, no obstante, es posible especificar el tipo de salida deseado. Por ejemplo, para mapear el resultado de una iteración en un vector ‘double’, puede implementarse la función map_dbl(), la cual mapea a un vector double: map_dbl(c(1, 4, 7), addTen) ## [1] 11 14 17 Para mapear a un vector de clase ‘caracter’, puede implementarse la función map_chr(): map_chr(c(1, 4, 7), addTen) ## [1] &quot;11.000000&quot; &quot;14.000000&quot; &quot;17.000000&quot; Incluso puede realizarse este mismo proceso iterativo en búsqueda de obtener como resultado un data frame: map_df(c(1, 4, 7), function(.x) { return(data.frame(old_number = .x, new_number = addTen(.x))) }) ## old_number new_number ## 1 1 11 ## 2 4 14 ## 3 7 17 3.6.2 Bucles dobles En caso de haber dominado el uso del bucle mediante la función map(), es posible proceder a intentar iterar sobre dos objetos. El código de abajo una las funciones de mapeo para crear una lista de gráficos para comparar la esperanza de vida y el ingreso per cápita en cada combinación de continente y país. La función de mapeo que itera sobre dos objetos en vez de uno, es llamada map2(). Los primeros dos argumentos son los dos objetos sobre los cuales se realiza la iteración y el tercero es la función con 2 argumentos (uno por cada objeto). x &lt;- list(1, 5, 15) y &lt;- list(10, 20, 30) map2(x, y, ~ .x + .y) ## [[1]] ## [1] 11 ## ## [[2]] ## [1] 25 ## ## [[3]] ## [1] 45 map2_dbl(x, y, ~ .x + .y) ## [1] 11 25 45 map2_chr(x, y, ~ .x + .y) ## [1] &quot;11.000000&quot; &quot;25.000000&quot; &quot;45.000000&quot; Otro ejemplo: df &lt;- data.frame( x = c(1, 2, 5), y = c(5, 4, 8) ) df ## x y ## 1 1 5 ## 2 2 4 ## 3 5 8 map2_dbl(df$x, df$y, min) ## [1] 1 2 5 "],["visualización.html", "Capítulo 4 Visualización 4.1 EDA: Análisis Exploratorio de Datos 4.2 GEDA: Análisis Exploratorio de Datos Gráficos 4.3 Creación de gráficos 4.4 Análisis univariado 4.5 Análisis multivariado 4.6 Visualización interactiva 4.7 Reporte interactivos", " Capítulo 4 Visualización “El análisis exploratorio de datos se refiere al proceso de realizar investigaciones iniciales sobre los datos para descubrir patrones, detectar anomalías, probar hipótesis y verificar suposiciones con la ayuda de estadísticas resumidas y representaciones gráficas.” Towards 4.1 EDA: Análisis Exploratorio de Datos Un análisis exploratorio de datos tiene principalmente 5 objetivos: Maximizar el conocimiento de un conjunto de datos Descubrir la estructura subyacente de los datos Extraer variables importantes Detectar valores atípicos y anomalías Probar los supuestos subyacentes EDA no es idéntico a los gráficos estadísticos aunque los dos términos se utilizan casi indistintamente. Los gráficos estadísticos son una colección de técnicas, todas basadas en gráficos y todas centradas en un aspecto de caracterización de datos. EDA abarca un lugar más grande. EDA es una filosofía sobre cómo diseccionar un conjunto de datos; lo que buscamos; cómo nos vemos; y cómo interpretamos. Los científicos de datos pueden utilizar el análisis exploratorio para garantizar que los resultados que producen sean válidos y aplicables a los resultados y objetivos comerciales deseados. EDA se utiliza principalmente para ver qué datos pueden revelar más allá del modelado formal o la tarea de prueba de hipótesis y proporciona una mejor comprensión de las variables del conjunto de datos y las relaciones entre ellas. También puede ayudar a determinar si las técnicas estadísticas que está considerando para el análisis de datos son apropiadas. Dependiendo del tipo de variable queremos obtener la siguiente información: Variables numéricas: Tipo de dato: float, integer Número de observaciones Mean Desviación estándar Cuartiles: 25%, 50%, 75% Valor máximo Valor mínimo Número de observaciones únicos Top 5 observaciones repetidas Número de observaciones con valores faltantes ¿Hay redondeos? Variables categóricas Número de categorías Valor de las categorías Moda Valores faltantes Número de observaciones con valores faltantes Proporción de observaciones por categoría Top 1, top 2, top 3 (moda 1, moda 2, moda 3) Faltas de ortografía ? Fechas Fecha inicio Fecha fin Huecos en las fechas: sólo tenemos datos entre semana, etc. Formatos de fecha (YYYY-MM-DD) Tipo de dato: date, time, timestamp Número de faltantes (NA) Número de observaciones Texto Longitud promedio de cada observación Identificar el lenguaje, si es posible Longitud mínima de cada observación Longitud máxima de cada observación Cuartiles de longitud: 25%, 50%, 75% Coordenadas geoespaciales Primero se pone la latitud y luego la longitud Primer decimal: 111 kms Segundo decimal: 11.1 kms Tercer decimal: 1.1 kms Cuarto decimal: 11 mts Quinto decimal: 1.1 mt Sexto decimal: 0.11 mts Valores que están cercanos al 100 representan la longitud El símbolo en cada coordenada representa si estamos al norte (positivo) o sur (negativo) -en la latitud-, al este (positivo) o al - oeste (negativo) -en la longitud-. 4.2 GEDA: Análisis Exploratorio de Datos Gráficos Como complemento al EDA podemos realizar un GEDA, que es un análisis exploratorio de los datos apoyándonos de visualizaciones, la visualización de datos no trata de hacer gráficas “bonitas” o “divertidas”, ni de simplificar lo complejo. Más bien, trata de aprovechar nuestra gran capacidad de procesamiento visual para exhibir de manera clara aspectos importantes de los datos. 4.2.1 Lo que no se debe hacer… Fuentes: WTF Visualizations Flowingdata 4.2.2 Principios de visualización El objetivo de una visualización es sintetizar información relevante al análisis presentada de manera sencilla y sin ambigüedad. Lo usamos de apoyo para explicar a una audiencia más amplia que puede no ser tan técnica. Una gráfica debe reportar el resultado de un análisis detallado, nunca lo reemplaza. No hacer gráficas porque se vean “cool” Antes de hacer una gráfica, debe pensarse en lo que se quiere expresar o representar Existen “reglas” o mejores gráficas para representar cierto tipo de información de acuerdo a los tipos de datos que se tienen o al objetivo se quiere lograr con la visualización. From Data to Viz No utilizar pie charts 4.2.3 Principios generales del diseño analítico: Muestra comparaciones, contrastes, diferencias. Muestra causalidad, mecanismo, explicación. Muestra datos multivariados, es decir, más de una o dos variables. Integra palabras, números, imágenes y diagramas. Las presentaciones analíticas, a fin de cuentas, se sostienen o caen dependiendo de la calidad, relevancia e integridad de su contenido. Esta categoría incluye técnicas específicas que dependen de la forma de nuestros datos y el tipo de pregunta que queremos investigar: 4.2.4 Técnicas de visualización: Tipos de gráficas: cuantiles, histogramas, caja y brazos, gráficas de dispersión, puntos/barras/ líneas, series de tiempo. Técnicas para mejorar gráficas: Transformación de datos, transparencia, vibración, suavizamiento y bandas de confianza. 4.2.5 Indicadores de calidad gráfica: Aplicables a cualquier gráfica en particular, son guías concretas y relativamente objetivas para evaluar la calidad de una gráfica. Integridad Gráfica: El factor de engaño, es decir, la distorsión gráfica de las cantidades representadas, debe ser mínimo. Chartjunk: Minimizar el uso de decoración gráfica que interfiera con la interpretación de los datos: 3D, rejillas, rellenos con patrones. Tinta de datos: Maximizar la proporción de tinta de datos vs. tinta total de la gráfica. La regla es: si hay tinta que no representa variación en los datos, o la eliminación de esa tinta no representa pérdidas de significado, esa tinta debe ser eliminada. El ejemplo más claro es el de las rejillas en gráficas y tablas: Densidad de datos: Las mejores gráficas tienen mayor densidad de datos, que es la razón entre el tamaño del conjunto de datos y el área de la gráfica. 4.2.6 Gráficos univariados: Histograma: El histograma es la forma más popular de mostrar la forma de un conjunto de datos. Se divide la escala de la variable en intervalos, y se realiza un conteo de los casos que caen en cada uno de los intervalos. Los histogramas pueden mostrar distintos aspectos de los datos dependiendo del tamaño y posición de los intervalos. Diagramas de caja y brazos: Es un método estandarizado para representar gráficamente una serie de datos numéricos a través de sus cuartiles. El diagrama de caja muestra a simple vista la mediana y los cuartiles de los datos, pudiendo también representar los valores atípicos de estos. Gráficas de barras: Una gráfica de este tipo nos muestra la frecuencia con la que se han observado los datos de una variable discreta, con una barra para cada categoría de esta variable. Gráficos Circulares (Pie Charts): Un gráfico circular o gráfica circular, también llamado “gráfico de pastel”, es un recurso estadístico que se utiliza para representar porcentajes y proporciones. 4.2.7 Gráficos multivariados Gráfico de dispersión: Los gráficos de dispersión se usan para trazar puntos de datos en un eje vertical y uno horizontal, mediante lo que se trata de mostrar cuánto afecta una variable a otra. Si no existe una variable dependiente, cualquier variable se puede representar en cada eje y el diagrama de dispersión mostrará el grado de correlación (no causalidad) entre las dos variables. Gráficas de líneas: Uno de los tipos de gráfica más utilizados es la de líneas, especialmente cuando se quieren comparar visualmente varias variables a lo largo del tiempo o algún otro parámetro. 4.3 Creación de gráficos Comparando con los gráficos base de R, ggplot2: Tiene una gramática más compleja para gráficos simples Tiene una gramática menos compleja para gráficos complejos o muy personalizados. Los datos siempre deben ser un data.frame. Usa un sistema diferente para añadir elementos al gráfico. Histograma con los gráficos base: Histograma con ggplot2: Ahora vamos a ver un gráfico con colores y varias series de datos. Con los gráficos base: Con ggplot2: 4.3.1 Estéticas En ggplot2, aestetics significa “algo que puedes ver”. Algunos ejemplos son: Posición (por ejemplo, los ejes x e y) Color (color “externo”) Fill (color de relleno) Shape (forma de puntos) Linetype (tipo de linea) Size (tamaño) Alpha (para la transparencia: los valores más altos tendrían formas opacas y los más bajos, casi transparentes). Hay que advertir que no todas las estéticas tienen la misma potencia en un gráfico. El ojo humano percibe fácilmente longitudes distintas. Pero tiene problemas para comparar áreas (que es lo que regula la estética size) o intensidades de color. Se recomienda usar las estéticas más potentes para representar las variables más importantes. Cada tipo de objeto geométrico (geom) solo acepta un subconjunto de todos los aestéticos. Puedes consultar la pagina de ayuda de geom() para ver que aestéticos acepta. El mapeo aestético se hace con la función aes(). 4.3.2 Objetos geométricos o capas Los objetos geométricos son las formas que puede tomar un gráfico. Algunos ejemplos son: Barras (geom_bar(), para las variables univariados discretos o nominales) Histogramas (geom_hist() para aquellas variables univariadas continuas) Puntos (geom_point() para scatter plots, gráficos de puntos, etc…) Lineas (geom_line() para series temporales, lineas de tendencia, etc…) Cajas (geom_boxplot() para gráficos de cajas) Un gráfico debe tener al menos un geom, pero no hay limite. Puedes añadir más geom usando el signo +. Una vez añadida una capa al gráfico a este pueden agregarse nuevas capas 4.3.3 Facetas Muchos de los gráficos que pueden generarse con los elementos anteriores pueden reproducirse usando los gráficos tradicionales de R, pero no los que usan facetas, que pueden permitirnos explorar las variables de diferente forma, por ejemplo: crea tres gráficos dispuestos horizontalmente que comparan la relación entre la anchura y la longitud del pétalo de las tres especies de iris. Una característica de estos gráficos, que es crítica para poder hacer comparaciones adecuadas, es que comparten ejes. 4.3.4 Más sobre estéticas Para los ejercicios en clase utilizaremos el set de datos: Diamonds: library(dplyr) library(ggplot2) library(reshape2) data(&quot;diamonds&quot;) Descripción Un conjunto de datos que contiene los precios y otros atributos de casi 54.000 diamantes. Las variables son las siguientes: price: precio en dólares estadounidenses ( $ 326 -  $ 18,823) carat: peso del diamante (0.2–5.01) cut: calidad del corte (Regular, Bueno, Muy Bueno, Premium, Ideal) color: color del diamante, de D (mejor) a J (peor) clarity: una medida de la claridad del diamante (I1 (peor), SI2, SI1, VS2, VS1, VVS2, VVS1, IF (mejor)) x: longitud en mm (0-10,74) y: ancho en mm (0–58,9) width in mm (0–58.9) z: profundidad en mm (0–31,8) depth porcentaje de profundidad total = z / media (x, y) = 2 * z / (x + y) (43–79) table: ancho de la parte superior del diamante en relación con el punto más ancho (43–95) Ejemplo práctico: diamonds %>% ggplot() + aes(x = cut_number(carat, 5), y = price) + geom_boxplot() + aes(color = cut) + labs(title = 'Distribución de precio por categoría de corte') + labs(caption = 'Data source:Diamont set') + labs(x = 'Peso del diamante') + labs(y = 'Precio') + guides(color = guide_legend(title = 'Calidad del corte')) + ylim(0, 20000) + scale_y_continuous( labels = scales::dollar_format(), breaks = seq(0, 20000,2500 ), limits = c(0, 20000) ) 4.3.5 Quick View library(DataExplorer) plot_intro(diamonds) plot_missing(diamonds) 4.4 Análisis univariado El análisis univariado tiene como objetivo conocer la calidad y distribución de los datos. Se busca conocer medidas de tendencia central, variación promedio, cantidad de valores perdidos, etc. Es vital conocer los datos y su calidad antes de usarlos. 4.4.1 Variables numéricas Los histogramas son gráficas de barras que se obtienen a partir de tablas de frecuencias, donde cada barra se escala según la frecuencia relativa entre el ancho del intervalo de clase correspondiente. Un histograma muestra la acumulación ó tendencia, la variabilidad o dispersión y la forma de la distribución. El Diagrama de Caja y bigotes un tipo de gráfico que muestra un resumen de una gran cantidad de datos en cinco medidas descriptivas, además de intuir su morfología y simetría. Este tipo de gráficos nos permite identificar valores atípicos y comparar distribuciones. Además de conocer de una forma cómoda y rápida como el 50% de los valores centrales se distribuyen. Se puede detectar rápidamente los siguientes valores: Primer cuartil: el 25% de los valores son menores o igual a este valor (punto 2 en el gráfico anterior). Mediana o Segundo Cuartil: Divide en dos partes iguales la distribución. De forma que el 50% de los valores son menores o igual a este valor (punto 3 en el gráfico siguiente). Tercer cuartil: el 75% de los valores son menores o igual a este valor (punto 4 en el gráfico siguiente). Rango Intercuartílico (RIC): Diferencia entre el valor del tercer cuartil y el primer cuartil. Tip: El segmento que divide la caja en dos partes es la mediana (punto 3 del gráfico), que facilitará la comprensión de si la distribución es simétrica o asimétrica, si la mediana se sitúa en el centro de la caja entonces la distribución es simétrica y tanto la media, mediana y moda coinciden. Precio diamonds %&gt;% ggplot( aes( x = price)) + geom_histogram(color= &quot;purple&quot;, fill= &quot;pink&quot;, bins = 30) + scale_x_continuous(labels = scales::dollar_format()) + scale_y_continuous(labels = scales::comma_format()) + ggtitle(&quot;Distribución de precio&quot;) diamonds %&gt;% ggplot( aes( x = price)) + geom_histogram( aes(y = ..density..), color= &quot;Blue&quot;, fill= &quot;White&quot;, bins = 30 ) + stat_density(geom = &quot;line&quot;, colour = &quot;black&quot;, size = 1)+ scale_x_continuous(labels = scales::dollar_format()) + scale_y_continuous(labels = scales::comma_format()) + stat_density(geom = &quot;line&quot;, colour = &quot;black&quot;, size = 1) + ggtitle(&quot;Distribución de precio&quot;) diamonds %&gt;% ggplot( aes( x = price)) + geom_boxplot(binwidth = 1000, color= &quot;Blue&quot;, fill= &quot;lightblue&quot;) + scale_x_continuous(labels = scales::dollar_format()) + scale_y_continuous(labels = scales::comma_format()) + ggtitle(&quot;Distribución de precio&quot;) diamonds %&gt;% ggplot( aes( x = carat)) + geom_boxplot(color= &quot;purple&quot;, fill= &quot;pink&quot;, alpha= 0.3) + scale_x_continuous(labels = scales::comma_format()) + ggtitle(&quot;Distribución de peso de los diamantes&quot;) + theme_bw() + coord_flip() Peso del diamante diamonds %&gt;% ggplot( aes( x = carat)) + geom_histogram(binwidth = .03, color= &quot;purple&quot;, fill= &quot;pink&quot;, alpha= 0.3) + scale_y_continuous(labels = scales::comma_format()) + ggtitle(&quot;Distribución de peso de los diamantes&quot;) + theme_bw() diamonds %&gt;% ggplot( aes( x = carat)) + geom_boxplot(color= &quot;purple&quot;, fill= &quot;pink&quot;, alpha= 0.3) + scale_x_continuous(labels = scales::comma_format()) + ggtitle(&quot;Distribución de peso de los diamantes&quot;) + theme_bw() 4.4.2 Variables nominales/categóricas Calidad de corte diamonds %&gt;% ggplot( aes( x = cut)) + geom_bar( color= &quot;darkblue&quot;, fill= &quot;cyan&quot;, alpha= 0.7) + scale_y_continuous(labels = scales::comma_format()) + ggtitle(&quot;Distribución de calidad de corte&quot;) + theme_dark() df_pie &lt;- diamonds %&gt;% group_by(cut) %&gt;% summarise(freq = n(), .groups=&#39;drop&#39;) df_pie %&gt;% ggplot( aes( x = &quot;&quot;, y=freq, fill = factor(cut))) + geom_bar(width = 1, stat = &quot;identity&quot;) + coord_polar(theta = &quot;y&quot;, start=0) ggplot(data = diamonds)+ geom_bar( mapping = aes(x = cut, fill = cut), show.legend = F, width = 1)+ theme(aspect.ratio = 1)+ labs(x= NULL, y = NULL)+ coord_polar() Claridad diamonds %&gt;% ggplot( aes( y = clarity)) + geom_bar( color= &quot;darkblue&quot;, fill= &quot;black&quot;, alpha= 0.7) + geom_text(aes(label = scales::comma(..count..)), stat = &quot;count&quot;, vjust = 1, hjust = 1.1,colour = &quot;white&quot;) + scale_x_continuous(labels = scales::comma_format()) + ggtitle(&quot;Distribución claridad&quot;) + theme_get() diamonds %&gt;% ggplot( aes( y = clarity)) + geom_bar( color= &quot;darkblue&quot;, fill= &quot;black&quot;, alpha= 0.7) + geom_text(aes(label = scales::percent(..count../sum(..count..) ) ), stat = &quot;count&quot;, vjust = -0.25, colour = &quot;darkblue&quot;) + scale_x_continuous(labels = scales::comma_format()) + ggtitle(&quot;Distribución claridad&quot;) + coord_flip() 4.5 Análisis multivariado Un buen análisis de datos, requiere del análisis conjunto de variables. Una sola variable es importante de analizar en cuanto a su distribución y calidad, no obstante, no dice mucho al analizarse por sí sola. Es por ello, que es indispensable analizar la covariabilidad y dependencia entre los distintos atributos de la información. En el análisis multivariado, se busca comparar la información haciendo contrastes de colores, formas, tamaños, paneles, etc. Precio vs Calidad del corte diamonds %&gt;% ggplot(aes(y= price,x=cut,color=cut)) + geom_jitter(size=1.2, alpha= 0.5) diamonds %&gt;% ggplot(aes(y= price,x=cut,color=cut)) + geom_boxplot(size=1.2, alpha= 0.5) diamonds %&gt;% ggplot(aes(x= price ,fill=cut)) + geom_histogram(position = &#39;identity&#39;, alpha = 0.5) diamonds %&gt;% ggplot(aes(x= price ,fill=cut)) + geom_histogram(position = &#39;identity&#39;, alpha = 0.5) + facet_wrap(~cut, ncol = 1) diamonds %&gt;% ggplot( aes(x = carat ,y=price)) + geom_point(aes(col = clarity) ) + geom_smooth() diamonds %&gt;% ggplot( aes(x = carat ,y=price)) + geom_point(aes(col = clarity) ) + facet_wrap(~clarity)+ geom_smooth() 4.6 Visualización interactiva A través de la librería plotly es posible crear interactividad entre las gráficas creadas con gglot2. Es posible tanto usar las funciones existentes como crear funciones que enriquezcan las estéticas comunes. A continuación se hace uso de una combinación de nuevas estéticas con la interactividad añadida. library(stringr) fun_mean &lt;- function(x){ mean &lt;- data.frame( y = mean(x), label = mean(x, na.rm = T) ) return(mean) } means &lt;- diamonds %&gt;% group_by(clarity) %&gt;% summarise(price = round(mean(price), 1)) plot &lt;- diamonds %&gt;% ggplot(aes(x = clarity, y = price)) + geom_boxplot(aes(fill = clarity)) + stat_summary( fun = mean, geom = &quot;point&quot;, colour = &quot;darkred&quot;, shape = 18, size = 2 ) + geom_text( data = means, aes(label = str_c(&quot;$&quot;,price), y = price + 600) ) + ggtitle(&quot;Precio vs Claridad de diamantes&quot;) + xlab(&quot;Claridad&quot;) + ylab(&quot;Precio&quot;) plotly::ggplotly(plot) Como puede apreciarse, este nuevo gráfico permite hacer uso de zoom, filtros, etiquetas, snapshots, etc. El contenido es creado como HTML, por lo que puede integrarse a documentos web como bookdown, shiny, xaringan, markdown, etc. ¡ Warning ! Nunca se debe olvidar que debemos de analizar los datos de manera objetiva, nuestro criterio sobre un problema o negocio no debe de tener sesgos sobre lo que “nos gustaría encontrar en los datos” o lo “que creemos que debe pasar”…. 4.7 Reporte interactivos Es posible automatizar reportes de análisis de datos. Los reportes pueden realizarse tanto en formato estático (.docx y .pdf) como en formato interactivo (.html). Existen diversos manuales sumamente amplios que permiten conocer las múltiples funcionalidades de las librerías que hacen posible la creación de documentos. Para el caso de documentos, existe la librería Rmarkdown, la cual crea un documento estático o interactivo, mientras que para reportes en presentaciones existe la librería Xaringan, la cual sustituye a las presentaciones de power point. Es importante tomar en cuenta el balance entre complejidad y funcionalidad. Si bien es cierto que a través de esta herramienta pueden automatizarse reportes que consideren los resultados salientes de R, es importante considerar que las personas que puedan editar tal presentación serán limitadas. A continuación se enlistan los links de tutoriales para crear los documentos mencionados: Rmarkdown Xaringan Otros lenguajes Tablas estáticas Tablas interactivas "],["introducción-a-machine-learning.html", "Capítulo 5 Introducción a Machine Learning 5.1 Análisis Supervisado vs No supervisado 5.2 Sesgo vs varianza 5.3 Orden y estructura de proyecto 5.4 Partición de datos 5.5 Pre-procesamiento de datos 5.6 Ingeniería de datos 5.7 Recetas 5.8 Datos y tipos de modelos", " Capítulo 5 Introducción a Machine Learning Como se había mencionado, el Machine Learning es una disciplina del campo de la Inteligencia Artificial que, a través de algoritmos, dota a los ordenadores de la capacidad de identificar patrones en datos para hacer predicciones. Este aprendizaje permite a los computadores realizar tareas específicas de forma autónoma. El término se utilizó por primera vez en 1959. Sin embargo, ha ganado relevancia en los últimos años debido al aumento de la capacidad de computación y al BOOM de los datos. Un algoritmo para computadoras puede ser pensado como una receta. Describe exactamente qué pasos se realizan uno tras otro. Los ordenadores no entienden las recetas de cocina, sino los lenguajes de programación: En ellos, el algoritmo se descompone en pasos formales (comandos) que el ordenador puede entender. La cuestión no es solo saber para qué sirve el Machine Learning, sino que saber cómo funciona y cómo poder implementarlo en la industria para aprovecharse de sus beneficios. Hay ciertos pasos que usualmente se siguen para crear un modelo de Machine Learning. Estos son típicamente realizados por científicos de los datos que trabajan en estrecha colaboración con los profesionales de los negocios para los que se está desarrollando el modelo. Seleccionar y preparar un conjunto de datos de entrenamiento Los datos de entrenamiento son un conjunto de datos representativos de los datos que el modelo de Machine Learning ingerirá para resolver el problema que está diseñado para resolver. Los datos de entrenamiento deben prepararse adecuadamente: aleatorizados y comprobados en busca de desequilibrios o sesgos que puedan afectar al entrenamiento. También deben dividirse en dos subconjuntos: el subconjunto de entrenamiento, que se utilizará para entrenar el algoritmo, y el subconjunto de validación, que se utilizará para probarlo y perfeccionarlo. Elegir un algoritmo para ejecutarlo en el conjunto de datos de entrenamiento Este es uno de los pasos más importantes, ya que se debe elegir qué algoritmo utilizar, siendo este un conjunto de pasos de procesamiento estadístico. El tipo de algoritmo depende del tipo (supervisado o no supervisado), la cantidad de datos del conjunto de datos de entrenamiento y del tipo de problema que se debe resolver. Entrenamiento del algoritmo para crear el modelo El entrenamiento del algoritmo es un proceso iterativo: implica ejecutar las variables a través del algoritmo, comparar el resultado con los resultados que debería haber producido, ajustar los pesos y los sesgos dentro del algoritmo que podrían dar un resultado más exacto, y ejecutar las variables de nuevo hasta que el algoritmo devuelva el resultado correcto la mayoría de las veces. El algoritmo resultante, entrenado y preciso, es el modelo de Machine Learning. Usar y mejorar el modelo El paso final es utilizar el modelo con nuevos datos y, en el mejor de los casos, para que mejore en precisión y eficacia con el tiempo. De dónde procedan los nuevos datos dependerá del problema que se resuelva. Por ejemplo, un modelo de Machine Learning diseñado para identificar el spam ingerirá mensajes de correo electrónico, mientras que un modelo de Machine Learning que maneja una aspiradora robot ingerirá datos que resulten de la interacción en el mundo real con muebles movidos o nuevos objetos en la habitación. 5.1 Análisis Supervisado vs No supervisado Los algoritmos de Machine Learning se dividen en tres categorías, siendo las dos primeras las más comunes: Aprendizaje supervisado: estos algoritmos cuentan con un aprendizaje previo basado en un sistema de etiquetas asociadas a unos datos que les permiten tomar decisiones o hacer predicciones. Algunos ejemplos son: - Un detector de spam que etiqueta un e-mail como spam o no. - Predecir precios de casas - Clasificación de imagenes - Predecir el clima - ¿Quiénes son los clientes descontentos? Aprendizaje no supervisado: en el aprendizaje supervisado, la idea principal es aprender bajo supervisión, donde la señal de supervisión se nombra como valor objetivo o etiqueta. En el aprendizaje no supervisado, carecemos de este tipo de etiqueta. Por lo tanto, necesitamos encontrar nuestro camino sin ninguna supervisión ni guía. Esto simplemente significa que necesitamos descubrir qué es qué por nosotros mismos. Algunos ejemplos son: - Encontrar segmentos de clientes. - Reducir la complejidad de un problema - Selección de variables - Encontrar grupos - Reducción de dimensionalidad Aprendizaje por refuerzo: su objetivo es que un algoritmo aprenda a partir de la propia experiencia. Esto es, que sea capaz de tomar la mejor decisión ante diferentes situaciones de acuerdo a un proceso de prueba y error en el que se recompensan las decisiones correctas. Algunos ejemplos son: - Reconocimiento facial - Diagnósticos médicos - Clasificar secuencias de ADN 5.1.1 Regresión vs clasificación Existen dos tipos principales de aprendizaje supervisado, esto depende del tipo de la variable respuesta: Clasificación En el aprendizaje supervisado, los algoritmos de clasificación se usan cuando el resultado es una etiqueta discreta. Esto quiere decir que se utilizan cuando la respuesta se fundamenta en conjunto finito de resultados. Regresión El análisis de regresión es un subcampo del aprendizaje automático supervisado cuyo objetivo es establecer un método para la relación entre un cierto número de características y una variable objetivo continua. 5.2 Sesgo vs varianza En el mundo de Machine Learning cuando desarrollamos un modelo nos esforzamos para hacer que sea lo más preciso, ajustando los parámetros, pero la realidad es que no se puede construir un modelo 100% preciso ya que nunca pueden estar libres de errores. Comprender cómo las diferentes fuentes de error generan sesgo y varianza nos ayudará a mejorar el proceso de ajuste de datos, lo que resulta en modelos más precisos, adicionalmente también evitará el error de sobre-ajuste y falta de ajuste. 5.2.1 Errores reducibles Error por sesgo: Es la diferencia entre la predicción esperada de nuestro modelo y los valores verdaderos. Aunque al final nuestro objetivo es siempre construir modelos que puedan predecir datos muy cercanos a los valores verdaderos, no siempre es tan fácil porque algunos algoritmos son simplemente demasiado rígidos para aprender señales complejas del conjunto de datos. Imagina ajustar una regresión lineal a un conjunto de datos que tiene un patrón no lineal, no importa cuántas observaciones más recopiles, una regresión lineal no podrá modelar las curvas en esos datos. Esto se conoce como underfitting. Error por varianza: Se refiere a la cantidad que la estimación de la función objetivo cambiará si se utiliza diferentes datos de entrenamiento. La función objetivo se estima a partir de los datos de entrenamiento mediante un algoritmo de Machine Learning, por lo que deberíamos esperar que el algoritmo tenga alguna variación. Idealmente no debería cambiar demasiado de un conjunto de datos de entrenamiento a otro. Los algoritmos de Machine Learning que tienen una gran varianza están fuertemente influenciados por los detalles de los datos de entrenamiento, esto significa que los detalles de la capacitación influyen en el número y los tipos de parámetros utilizados para caracterizar la función de mapeo. 5.2.2 Error irreducible El error irreducible no se puede reducir, independientemente de qué algoritmo se usa. También se le conoce como ruido y, por lo general, proviene por factores como variables desconocidas que influyen en el mapeo de las variables de entrada a la variable de salida, un conjunto de características incompleto o un problema mal enmarcado. Acá es importante comprender que no importa cuán bueno hagamos nuestro modelo, nuestros datos tendrán cierta cantidad de ruido o un error irreductible que no se puede eliminar. 5.2.3 Balance entre sesgo y varianza o Trade-off El objetivo de cualquier algoritmo supervisado de Machine Learning es lograr un sesgo bajo, una baja varianza y a su vez el algoritmo debe lograr un buen rendimiento de predicción. El sesgo frente a la varianza se refiere a la precisión frente a la consistencia de los modelos entrenados por su algoritmo. Podemos diagnosticarlos de la siguiente manera: Los algoritmos de baja varianza (alto sesgo) tienden a ser menos complejos, con una estructura subyacente simple o rígida. Los algoritmos de bajo sesgo (alta varianza) tienden a ser más complejos, con una estructura subyacente flexible. No hay escapatoria a la relación entre el sesgo y la varianza en Machine Learning, aumentar el sesgo disminuirá la varianza, aumentar la varianza disminuirá el sesgo. 5.2.4 Error total Comprender el sesgo y la varianza es fundamental para comprender el comportamiento de los modelos de predicción, pero en general lo que realmente importa es el error general, no la descomposición específica. El punto ideal para cualquier modelo es el nivel de complejidad en el que el aumento en el sesgo es equivalente a la reducción en la varianza. Para construir un buen modelo, necesitamos encontrar un buen equilibrio entre el sesgo y la varianza de manera que minimice el error total. 5.2.5 Overfitting El modelo es muy particular. Error debido a la varianza Durante el entrenamiento tiene un desempeño muy bueno, pero al pasar nuevos datos su desempeño es malo. 5.2.6 Underfitting El modelo es demasiado general. Error debido al sesgo. Durante el entrenamiento no tiene un buen desempeño. 5.3 Orden y estructura de proyecto Resulta elemental contar con una adecuada estructura de carpetas que permitan al analista mantener orden y control a lo largo de todo el proyecto. Gran parte del caos en los problemas de analítica de datos nace desde el momento en que no se sabe en donde ubicar cada uno de los archivos necesarios para el proyecto. 5.3.1 Plantilla de estructura proyecto En esta sección, se presenta una introducción a la librería ProjectTemplate, la cual facilita una estructura predeterminada que ayudará como punto de partida para mantener orden y control en cada momento del proyecto. library(ProjectTemplate) create.project(project.name = &#39;intro2dsml&#39;, rstudio.project = T) create.project() creará toda la estructura de carpetas para un nuevo proyecto. Configurará todos los directorios relevantes y sus contenidos iniciales. Para aquellos que solo desean la funcionalidad mínima, el argumento de template se puede establecer en minimal para crear un subconjunto de directorios predeterminados de ProjectTemplate. cache: En esta carpeta se almacenarán los datos que desear cargarse automáticamente cuando se cargue la sesión del proyecto. config: Se realiza la configuración de R y su sesión, la cual será establecida cada que se abra el proyecto. data: Se almacenan las fuentes de información crudas necesarias en el proyecto. En caso de encontrarse codificadas en algún formato de archivo soportado por la librería, automáticamente serán cargadas a la sesión con la función load.project() diagnostics: En este folder puedes almacenar cualquier script usado para realizar diagnósticos sobre los datos. Es particularmente útil para al análisis de elementos corruptos o problemáticos dentro del conjunto de datos. doc: En este folder puede almacenarse cualquier documentación que haya escrito sobre el análisis. También se puede usar como directorio raíz para las páginas de GitHub para crear un sitio web de proyecto. graphs: Sirve para almacenar las gráficas producidas por el análisis lib: Aquí se almacenarán todos los archivos que proporcionen una funcionalidad útil para su trabajo, pero que no constituyan un análisis estadístico per se. Específicamente, debe usar el script lib/helpers.R para organizar cualquier función que use en su proyecto que no sea lo suficientemente general como para pertenecer a un paquete. Si tiene una configuración específica del proyecto que le gustaría almacenar en el objeto de configuración, puede especificarla en lib/globals.R. logs: Aquí puede almacenarse un archivo de registro de cualquier trabajo que haya realizado en este proyecto. Si va a registrar su trabajo, se recomienda utilizar el paquete log4r, que ProjectTemplate cargará automáticamente si activa la opción de configuración de registro. El nivel de registro se puede establecer a través de la configuración logging_level en la configuración, el valor predeterminado es “INFO”. munge: En este folder puede almacenarse cualquier código de pre-procesamiento o manipulación de datos para el proyecto. Por ejemplo, si necesita agregar columnas en tiempo de ejecución, fusionar conjuntos de datos normalizados o censurar globalmente cualquier punto de datos, ese código debe almacenarse en el directorio munge. Los scripts de pre-procesamiento almacenados en munge se ejecutarán en orden alfabético cuando se llame a la función load.project(), por lo que debe anteponerse números a los nombres de archivo para indicar su orden secuencial. profiling: Aquí puede almacenar cualquier script que use para comparar y cronometrar su código. reports: Aquí puede almacenar cualquier informe de salida, como versiones de tablas HTML o LaTeX, que produzca. Los documentos de sweave o brew también deben ir en el directorio de informes. src: Aquí se almacenarán los scripts de análisis estadístico finales. Debe agregar el siguiente fragmento de código al comienzo de cada secuencia de comandos de análisis: library('ProjectTemplate); load.project(). También debe hacer todo lo posible para asegurarse de que cualquier código compartido entre los análisis en src se mueva al directorio munge; si lo hace, puede ejecutar todos los análisis en el directorio src en paralelo. Una versión futura de ProjectTemplate proporcionará herramientas para ejecutar automáticamente cada análisis individual de src en paralelo. tests: Aquí puede almacenarse cualquier caso de prueba para las funciones que ha escrito. Los archivos de prueba deben usar pruebas de estilo testthat para que pueda llamar a la función test.project() para ejecutar automáticamente todo su código de prueba. README: En este archivo, debe escribir algunas notas para ayudar a orientar a los recién llegados a su proyecto. TODO: En este archivo, debe escribir una lista de futuras mejoras y correcciones de errores que planea realizar en sus análisis. Si algunas o todas estas carpetas resultan innecesarias, puede comenzarse con una versión simplificada a través del comando: create.project(project.name = &#39;intro2dsml&#39;, template=&#39;minimal&#39;) 5.3.2 Reproducibilidad Trabajar de esta manera permitirá que un proyecto sea reproducible por cualquier persona con el acceso y los permisos adecuados para colaborar. Sin importar que existan nuevas versiones de R o sus librerías, este ambiente virtual creado será resiliente a tales cambios, permitiendo que el proyecto perdure a lo largo del tiempo. library(renv) renv::init() renv::init() inicializa un nuevo entorno local de proyecto con una biblioteca R privada. La función renv::init() intenta garantizar que la biblioteca del proyecto recién creada incluya todos los paquetes de R utilizados actualmente por el proyecto. Lo hace rastreando archivos R dentro del proyecto en busca de dependencias. Los paquetes descubiertos luego se instalan en la biblioteca del proyecto, que también intentará ahorrar tiempo copiando paquetes de la biblioteca del usuario (en lugar de reinstalarlos desde CRAN) según corresponda. Al usar renv, es posible “salvar” y “cargar” el estado de las librerías del proyecto a través de las siguientes funciones: renv::snapshot() guarda el estado del proyecto en el archivo renv.lock renv::restore() Restablece el estado del proyecto desde la última actualización de renv.lock. 5.4 Partición de datos Cuando hay una gran cantidad de datos disponibles, una estrategia inteligente es asignar subconjuntos específicos de datos para diferentes tareas, en lugar de asignar la mayor cantidad posible solo a la estimación de los parámetros del modelo. Si el conjunto inicial de datos no es lo suficientemente grande, habrá cierta superposición de cómo y cuándo se asignan nuestros datos, y es importante contar con una metodología sólida para la partición de datos. 5.4.1 Métodos comunes para particionar datos El enfoque principal para la validación del modelo es dividir el conjunto de datos existente en dos conjuntos distintos: Entrenamiento: Este conjunto suele contener la mayoría de los datos, los cuales sirven para la construcción de modelos donde se pueden ajustar diferentes modelos, se investigan estrategias de ingeniería de características, etc. La mayor parte del proceso de modelado se utiliza este conjunto. Prueba: La otra parte de las observaciones se coloca en este conjunto. Estos datos se mantienen en reserva hasta que se elijan uno o dos modelos como los de mejor rendimiento. El conjunto de prueba se utiliza como árbitro final para determinar la eficiencia del modelo, por lo que es fundamental mirar el conjunto de prueba una sola vez. Supongamos que asignamos el \\(80\\%\\) de los datos al conjunto de entrenamiento y el \\(20\\%\\) restante a las pruebas. El método más común es utilizar un muestreo aleatorio simple. El paquete rsample tiene herramientas para realizar divisiones de datos como esta; la función initial_split() fue creada para este propósito. library(tidymodels) tidymodels_prefer() # Fijar un número aleatorio con para que los resultados puedan ser reproducibles set.seed(123) # Partición 80/20 de los datos ames_split &lt;- initial_split(ames, prop = 0.80) ames_split ## &lt;Training/Testing/Total&gt; ## &lt;2344/586/2930&gt; La información impresa denota la cantidad de datos en el conjunto de entrenamiento \\((n = 2,344)\\), la cantidad en el conjunto de prueba \\((n = 586)\\) y el tamaño del grupo original de muestras \\((n = 2,930)\\). El objeto ames_split es un objeto rsplit y solo contiene la información de partición; para obtener los conjuntos de datos resultantes, aplicamos dos funciones más: ames_train &lt;- training(ames_split) ames_test &lt;- testing(ames_split) dim(ames_train) ## [1] 2344 74 El muestreo aleatorio simple es apropiado en muchos casos, pero hay excepciones. Cuando hay un desbalance de clases en los problemas de clasificación, el uso de una muestra aleatoria simple puede asignar al azar estas muestras poco frecuentes de manera desproporcionada al conjunto de entrenamiento o prueba. Para evitar esto, se puede utilizar un muestreo estratificado. La división de entrenamiento/prueba se lleva a cabo por separado dentro de cada clase y luego estas submuestras se combinan en el conjunto general de entrenamiento y prueba. Para los problemas de regresión, los datos de los resultados se pueden agrupar artificialmente en cuartiles y luego realizar un muestreo estratificado cuatro veces por separado. Este es un método eficaz para mantener similares las distribuciones del resultado entre el conjunto de entrenamiento y prueba. Observamos que la distribución del precio de venta está sesgada a la derecha. Las casas más caras no estarían bien representadas en el conjunto de entrenamiento con una simple partición; esto aumentaría el riesgo de que nuestro modelo sea ineficaz para predecir el precio de dichas propiedades. Las líneas verticales punteadas indican los cuatro cuartiles para estos datos. Una muestra aleatoria estratificada llevaría a cabo la división 80/20 dentro de cada uno de estos subconjuntos de datos y luego combinaría los resultados. En rsample, esto se logra usando el argumento de estratos: set.seed(123) ames_split &lt;- initial_split(ames, prop = 0.80, strata = Sale_Price) ames_train &lt;- training(ames_split) ames_test &lt;- testing(ames_split) Hay muy pocas desventajas en el uso de muestreo estratificado. Un caso es cuando los datos tienen un componente de tiempo, como los datos de series de tiempo. Aquí, es más común utilizar los datos más recientes como conjunto de prueba. El paquete rsample contiene una función llamada initial_time_split() que es muy similar a initial_split(). En lugar de usar un muestreo aleatorio, el argumento prop denota qué proporción de la primera parte de los datos debe usarse como conjunto de entrenamiento; la función asume que los datos se han clasificado previamente en un orden apropiado. ¿Qué proporción debería ser usada? No hay un porcentaje de división óptimo para el conjunto de entrenamiento y prueba. Muy pocos datos en el conjunto de entrenamiento obstaculizan la capacidad del modelo para encontrar estimaciones de parámetros adecuadas y muy pocos datos en el conjunto de prueba reducen la calidad de las estimaciones de rendimiento. Se debe elegir un porcentaje que cumpla con los objetivos de nuestro proyecto con consideraciones que incluyen: Costo computacional en el entrenamiento del modelo. Costo computacional en la evaluación del modelo. Representatividad del conjunto de formación. Representatividad del conjunto de pruebas. Los porcentajes de división más comunes comunes son: Entrenamiento: \\(80\\%\\), Prueba: \\(20\\%\\) Entrenamiento: \\(67\\%\\), Prueba: \\(33\\%\\) Entrenamiento: \\(50\\%\\), Prueba: \\(50\\%\\) 5.4.2 Conjunto de validación El conjunto de validación se definió originalmente cuando los investigadores se dieron cuenta de que medir el rendimiento del conjunto de entrenamiento conducía a resultados que eran demasiado optimistas. Esto llevó a modelos que se sobre-ajustaban, lo que significa que se desempeñaron muy bien en el conjunto de entrenamiento pero mal en el conjunto de prueba. Para combatir este problema, se retuvo un pequeño conjunto de datos de validación y se utilizó para medir el rendimiento del modelo mientras este está siendo entrenado. Una vez que la tasa de error del conjunto de validación comenzara a aumentar, la capacitación se detendría. En otras palabras, el conjunto de validación es un medio para tener una idea aproximada de qué tan bien se desempeñó el modelo antes del conjunto de prueba. Los conjuntos de validación se utilizan a menudo cuando el conjunto de datos original es muy grande. En este caso, una sola partición grande puede ser adecuada para caracterizar el rendimiento del modelo sin tener que realizar múltiples iteraciones de remuestreo. Con rsample, un conjunto de validación es como cualquier otro objeto de remuestreo; este tipo es diferente solo en que tiene una sola iteración set.seed(12) val_set &lt;- validation_split(ames_train, prop = 3/4, strata = NULL) val_set #val_set contiene el conjunto de entrenamiento y validación. ## # Validation Set Split (0.75/0.25) ## # A tibble: 1 × 2 ## splits id ## &lt;list&gt; &lt;chr&gt; ## 1 &lt;split [1756/586]&gt; validation Esta función regresa una columna para los objetos de división de datos y una columna llamada id que tiene una cadena de caracteres con el identificador de remuestreo. El argumento de estratos hace que el muestreo aleatorio se lleve a cabo dentro de la variable de estratificación. Esto puede ayudar a garantizar que el número de datos en los datos del análisis sea equivalente a las proporciones del conjunto de datos original. (Los estratos inferiores al 10% del total se agrupan). Otra opción de muestreo bastante común es la realizada mediante múltiples submuestras de los datos originales. Diversos métodos se revisarán a lo largo del curso. 5.4.3 Leave-one-out cross-validation La validación cruzada es una manera de predecir el ajuste de un modelo a un hipotético conjunto de datos de prueba cuando no disponemos del conjunto explícito de datos de prueba. El método LOOCV en un método iterativo que se inicia empleando como conjunto de entrenamiento todas las observaciones disponibles excepto una, que se excluye para emplearla como validación. Si se emplea una única observación para calcular el error, este varía mucho dependiendo de qué observación se haya seleccionado. Para evitarlo, el proceso se repite tantas veces como observaciones disponibles se tengan, excluyendo en cada iteración una observación distinta, ajustando el modelo con el resto y calculando el error con dicha observación. Finalmente, el error estimado por el es el promedio de todos lo \\(i\\) errores calculados. La principal desventaja de este método es su costo computacional. El proceso requiere que el modelo sea reajustado y validado tantas veces como observaciones disponibles se tengan lo que en algunos casos puede ser muy complicado. rsample contiene la función loo_cv(). set.seed(55) ames_loo &lt;- loo_cv(ames_train) ames_loo ## # Leave-one-out cross-validation ## # A tibble: 2,342 × 2 ## splits id ## &lt;list&gt; &lt;chr&gt; ## 1 &lt;split [2341/1]&gt; Resample1 ## 2 &lt;split [2341/1]&gt; Resample2 ## 3 &lt;split [2341/1]&gt; Resample3 ## 4 &lt;split [2341/1]&gt; Resample4 ## 5 &lt;split [2341/1]&gt; Resample5 ## 6 &lt;split [2341/1]&gt; Resample6 ## 7 &lt;split [2341/1]&gt; Resample7 ## 8 &lt;split [2341/1]&gt; Resample8 ## 9 &lt;split [2341/1]&gt; Resample9 ## 10 &lt;split [2341/1]&gt; Resample10 ## # … with 2,332 more rows 5.4.3.1 Cálculo del error En la validación cruzada dejando uno fuera se realizan tantas iteraciones como muestras \\((N)\\) tenga el conjunto de datos. De forma que para cada una de las \\(N\\) iteraciones se realiza un cálculo de error. El resultado final se obtiene realizando la media de los \\(N\\) errores obtenidos, según la fórmula: \\[E = \\frac{1}{N}\\sum_{i = 1}^N E_i\\] 5.4.4 V Fold Cross Validation En la validación cruzada de V iteraciones (V Fold Cross Validation) los datos de muestra se dividen en V subconjuntos. Uno de los subconjuntos se utiliza como datos de prueba y el resto \\((V-1)\\) como datos de entrenamiento. El proceso de validación cruzada es repetido durante \\(v\\) iteraciones, con cada uno de los posibles subconjuntos de datos de prueba. Finalmente se obtiene el promedio de los rendimientos de cada iteración para obtener un único resultado. Lo más común es utilizar la validación cruzada de 10 iteraciones. Este método de validación cruzada se utiliza principalmente para: Estimar el error cuando nuestro conjunto de prueba es muy pequeño. Es decir, se tiene la misma configuración de parámetros y solamente cambia el conjunto de prueba y validación. Encontrar lo mejores hiperparámetros que ajusten mejor el modelo. Es decir, en cada bloque se tiene una configuración de hiperparámetros distinto y se seleccionará aquellos hiperparámetros que hayan producido el error más pequeño. En la función vfold_cv() la entrada principal es el conjunto de entrenamiento, así como el número de bloques: set.seed(55) ames_folds &lt;- vfold_cv(ames_train, v = 10) ames_folds ## # 10-fold cross-validation ## # A tibble: 10 × 2 ## splits id ## &lt;list&gt; &lt;chr&gt; ## 1 &lt;split [2107/235]&gt; Fold01 ## 2 &lt;split [2107/235]&gt; Fold02 ## 3 &lt;split [2108/234]&gt; Fold03 ## 4 &lt;split [2108/234]&gt; Fold04 ## 5 &lt;split [2108/234]&gt; Fold05 ## 6 &lt;split [2108/234]&gt; Fold06 ## 7 &lt;split [2108/234]&gt; Fold07 ## 8 &lt;split [2108/234]&gt; Fold08 ## 9 &lt;split [2108/234]&gt; Fold09 ## 10 &lt;split [2108/234]&gt; Fold10 La columna denominada splits contiene la información sobre cómo dividir los datos (similar al objeto utilizado para crear la partición inicial de entrenamiento / prueba). Si bien cada fila de divisiones tiene una copia incrustada de todo el conjunto de entrenamiento, R es lo suficientemente inteligente como para no hacer copias de los datos en la memoria. El método de impresión dentro del tibble muestra la frecuencia de cada uno: [2K / 230] indica que aproximadamente dos mil muestras están en el conjunto de análisis y 230 están en ese conjunto de evaluación en particular. Estos objetos rsample también contienen siempre una columna de caracteres llamada id que etiqueta la partición. Algunos métodos de remuestreo requieren varios campos de identificación. Para recuperar manualmente los datos particionados, las funciones de analysis() y assessment() devuelven los de datos de análisis y evaluación respectivamente. # Primer bloque ames_folds$splits[[1]] %&gt;% analysis() %&gt;% # O assessment() head(7) ## # A tibble: 7 × 74 ## MS_SubC…¹ MS_Zo…² Lot_F…³ Lot_A…⁴ Street Alley Lot_S…⁵ Land_…⁶ Utili…⁷ Lot_C…⁸ ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; ## 1 One_Stor… Reside… 70 8400 Pave No_A… Regular Lvl AllPub Corner ## 2 Two_Stor… Reside… 21 1680 Pave No_A… Regular Lvl AllPub Inside ## 3 Two_Stor… Reside… 21 1680 Pave No_A… Regular Lvl AllPub Inside ## 4 Two_Stor… Reside… 21 1680 Pave No_A… Regular Lvl AllPub Inside ## 5 One_Stor… Reside… 53 4043 Pave No_A… Regular Lvl AllPub Inside ## 6 One_Stor… Reside… 24 2280 Pave No_A… Regular Lvl AllPub FR2 ## 7 One_Stor… Reside… 50 7175 Pave No_A… Regular Lvl AllPub Inside ## # … with 64 more variables: Land_Slope &lt;fct&gt;, Neighborhood &lt;fct&gt;, ## # Condition_1 &lt;fct&gt;, Condition_2 &lt;fct&gt;, Bldg_Type &lt;fct&gt;, House_Style &lt;fct&gt;, ## # Overall_Cond &lt;fct&gt;, Year_Built &lt;int&gt;, Year_Remod_Add &lt;int&gt;, ## # Roof_Style &lt;fct&gt;, Roof_Matl &lt;fct&gt;, Exterior_1st &lt;fct&gt;, Exterior_2nd &lt;fct&gt;, ## # Mas_Vnr_Type &lt;fct&gt;, Mas_Vnr_Area &lt;dbl&gt;, Exter_Cond &lt;fct&gt;, Foundation &lt;fct&gt;, ## # Bsmt_Cond &lt;fct&gt;, Bsmt_Exposure &lt;fct&gt;, BsmtFin_Type_1 &lt;fct&gt;, ## # BsmtFin_SF_1 &lt;dbl&gt;, BsmtFin_Type_2 &lt;fct&gt;, BsmtFin_SF_2 &lt;dbl&gt;, … 5.4.5 Medidas de ajuste Las medidas de ajuste obtenidas pueden ser utilizadas para estimar cualquier medida cuantitativa de ajuste apropiada para los datos y el modelo. En un modelo basado en clasificación binaria, para resumir el ajuste del modelo se pueden usar las medidas: Tasa de error de clasificación (Accuracy) Precisión Sensibilidad o cobertura (Recall) Especificidad Cuando el valor a predecir se distribuye de forma continua se puede calcular el error utilizando medidas como: Error porcentual absoluto medio (MAPE) Error absoluto medio (MAE) Error cuadrático medio (MSE) Raíz del error cuadrático medio (RMSE) Raíz del error logarítmico cuadrático medio (RMLSE) \\(R^2\\) (Coeficiente de determinación) \\(R^2_a\\) (Coeficiente de determinación ajustado) 5.4.5.1 Cálculo del error En cada una de las \\(v\\) iteraciones de este tipo de validación se realiza un cálculo de error. El resultado final lo obtenemos a partir de realizar la media de los \\(V\\) valores de errores obtenidos, según la fórmula: \\[E = \\frac{1}{V}\\sum_{i = 1}^vE_i\\] 5.4.6 Validación cruzada para series de tiempo En este procedimiento, hay una serie de conjuntos de prueba, cada uno de los cuales consta de una única observación. El conjunto de entrenamiento correspondiente consta solo de observaciones que ocurrieron antes de la observación que forma el conjunto de prueba. Por lo tanto, no se pueden utilizar observaciones futuras para construir el pronóstico. El siguiente diagrama ilustra la serie de conjuntos de entrenamiento y prueba, donde las observaciones azules forman los conjuntos de entrenamiento y las observaciones rojas forman los conjuntos de prueba. La precisión del pronóstico se calcula promediando los conjuntos de prueba. Este procedimiento a veces se conoce como “evaluación en un origen de pronóstico continuo” porque el “origen” en el que se basa el pronóstico avanza en el tiempo. Con los pronósticos de series de tiempo, los pronósticos de un paso pueden no ser tan relevantes como los pronósticos de varios pasos. En este caso, el procedimiento de validación cruzada basado en un origen de pronóstico continuo se puede modificar para permitir el uso de errores de varios pasos. Suponga que estamos interesados en modelos que producen buenos pronósticos de 4 pasos por delante. Entonces el diagrama correspondiente se muestra a continuación. La validación cruzada de series de tiempo se implementa con la función tsCV() del paquete forecast. 5.5 Pre-procesamiento de datos Hay varios pasos que se deben de seguir para crear un modelo útil: Recopilación de datos. Limpieza de datos. Creación de nuevas variables. Estimación de parámetros. Selección y ajuste del modelo. Evaluación del rendimiento. Al comienzo de un proyecto, generalmente hay un conjunto finito de datos disponibles para todas estas tareas. OJO: A medida que los datos se reutilizan para múltiples tareas, aumentan los riesgos de agregar sesgos o grandes efectos de errores metodológicos. Como punto de partida para nuestro flujo de trabajo de aprendizaje automático, necesitaremos datos de entrada. En la mayoría de los casos, estos datos se cargarán y almacenarán en forma de data frames o tibbles en R. Incluirán una o varias variables predictivas y, en caso de aprendizaje supervisado, también incluirán un resultado conocido. Sin embargo, no todos los modelos pueden lidiar con diferentes problemas de datos y, a menudo, necesitamos transformar los datos para obtener el mejor rendimiento posible del modelo. Este proceso se denomina pre-procesamiento y puede incluir una amplia gama de pasos, como: Dicotomización de variables: Variables cualitativas que solo pueden tomar el valor \\(0\\) o \\(1\\) para indicar la ausencia o presencia de una condición específica. Estas variables se utilizan para clasificar los datos en categorías mutuamente excluyentes o para activar comandos de encendido / apagado Near Zero Value (nzv) o Varianza Cero: En algunas situaciones, el mecanismo de generación de datos puede crear predictores que solo tienen un valor único (es decir, un “predictor de varianza cercando a cero”). Para muchos modelos (excluidos los modelos basados en árboles), esto puede hacer que el modelo se bloquee o que el ajuste sea inestable. De manera similar, los predictores pueden tener solo una pequeña cantidad de valores únicos que ocurren con frecuencias muy bajas. Imputaciones: Si faltan algunos predictores, ¿deberían estimarse mediante imputación? Des-correlacionar: Si hay predictores correlacionados, ¿debería mitigarse esta correlación? Esto podría significar filtrar predictores, usar análisis de componentes principales o una técnica basada en modelos (por ejemplo, regularización). Normalizar: ¿Deben centrarse y escalar los predictores? Transformar: ¿Es útil transformar los predictores para que sean más simétricos? (por ejemplo, escala logarítmica). Dependiendo del caso de uso, algunos pasos de pre-procesamiento pueden ser indispensables para pasos posteriores, mientras que otros solo son opcionales. Sin embargo, dependiendo de los pasos de pre-procesamiento elegidos, el rendimiento del modelo puede cambiar significativamente en pasos posteriores. Por lo tanto, es muy común probar varias configuraciones. 5.6 Ingeniería de datos La ingeniería de datos abarca actividades que dan formato a los valores de los predictores para que se puedan utilizar de manera eficaz para nuestro modelo. Esto incluye transformaciones y codificaciones de los datos para representar mejor sus características importantes. Por ejemplo: 1.- Supongamos que un conjunto de datos tiene dos predictores que se pueden representar de manera más eficaz en nuestro modelo como una proporción, así, tendríamos un nuevo predictor a partir de la proporción de los dos predictores originales. X Proporción (X) 691 0.1836789 639 0.1698565 969 0.2575758 955 0.2538543 508 0.1350346 2.- Al elegir cómo codificar nuestros datos en el modelado, podríamos elegir una opción que creemos que está más asociada con el resultado. El formato original de los datos, por ejemplo numérico (edad) versus categórico (grupo). Edad Grupo 7 Niños 78 Adultos mayores 17 Adolescentes 25 Adultos 90 Adultos mayores La ingeniería y el pre-procesamiento de datos también pueden implicar el cambio de formato requerido por el modelo. Algunos modelos utilizan métricas de distancia geométrica y, en consecuencia, los predictores numéricos deben centrarse y escalar para que estén todos en las mismas unidades. De lo contrario, los valores de distancia estarían sesgados por la escala de cada columna. 5.7 Recetas Una receta es una serie de pasos o instrucciones para el procesamiento de datos. A diferencia del método de fórmula dentro de una función de modelado, la receta define los pasos sin ejecutarlos inmediatamente; es sólo una especificación de lo que se debe hacer. La estructura de una receta sigue los siguientes pasos: Inicialización Transformación Preparación Aplicación La siguiente sección explica la estructura y flujo de transformaciones: receta &lt;- recipe(response ~ X1 + X2 + X3 + ... + Xn, data = dataset ) %&gt;% transformation_1(...) %&gt;% transformation_2(...) %&gt;% transformation_3(...) %&gt;% ... final_transformation(...) %&gt;% prep() bake(receta, new_data = new_dataset) A continuación se muestran distintos ejemplos de transformaciones realizadas comúnmente en el pre-procesamiento de modelos predictivos. Como ejemplo, utilizaremos el subconjunto de predictores disponibles en los datos de vivienda: Ames Vecindario (29 vecindarios) Superficie habitable bruta sobre el nivel del suelo Año de constricción Tipo de edificio ANTERIORMENTE… Un modelo de regresión lineal ordinario se ajustaba a los datos con la función estándar lm() de la siguiente manera: lm(Sale_Price ~ Neighborhood + log10(Gr_Liv_Area) + Year_Built + Bldg_Type, data = ames) Cuando se ejecuta esta función, los datos se convierten en a una matriz de diseño numérico (también llamada matriz de modelo) y luego se utiliza el método de mínimos cuadrados para estimar los parámetros. Lo que hace la fórmula anterior se puede descomponer en una serie de pasos: 1.- El precio de venta se define como el resultado, mientras que las variables de vecindario, superficie habitable bruta, año de construcción y tipo de edificio se definen como predictores. 2.- Se aplica una transformación logarítmica al predictor de superficie habitable bruta. 3.- Las columnas de vecindad y tipo de edificio se convierten de un formato no numérico a un formato numérico (dado que los mínimos cuadrados requieren predictores numéricos). La siguiente receta es equivalente a la fórmula anterior: simple_ames &lt;- recipe( Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type, data = ames) %&gt;% step_log(Gr_Liv_Area, base = 10) %&gt;% step_dummy(all_nominal_predictors()) simple_ames ## Recipe ## ## Inputs: ## ## role #variables ## outcome 1 ## predictor 4 ## ## Operations: ## ## Log transformation on Gr_Liv_Area ## Dummy variables from all_nominal_predictors() Ventajas de usar una receta: Los cálculos se pueden reciclar entre modelos ya que no están estrechamente acoplados a la función de modelado. Una receta permite un conjunto más amplio de opciones de procesamiento de datos que las que pueden ofrecer las fórmulas. La sintaxis puede ser muy compacta. Por ejemplo, all_nominal_predictors() se puede usar para capturar muchas variables para tipos específicos de procesamiento, mientras que una fórmula requeriría que cada una se enumere explícitamente. Todo el procesamiento de datos se puede capturar en un solo objeto en lugar de tener scripts que se repiten o incluso se distribuyen en diferentes archivos. 5.7.1 Pasos y estructura de recetas Como se mostró anteriormente, existen 4 pasos fundamentales para el procesamiento y transformación de conjuntos de datos. Estos pasos se describen de la siguiente manera: Receta: Inicializa una receta y define los roles de las variables Transformaciones: Mutaciones a los renglones y columnas hasta desear el resultado Preparación: Se realizan las estimaciones estadísticas con los datos La función prep() estima las cantidades requeridas y las estadísticas necesarias para cualquier paso declarado en la receta. prep &lt;- prep(simple_ames) prep ## Recipe ## ## Inputs: ## ## role #variables ## outcome 1 ## predictor 4 ## ## Training data contained 2930 data points and no missing data. ## ## Operations: ## ## Log transformation on Gr_Liv_Area [trained] ## Dummy variables from Neighborhood, Bldg_Type [trained] Aplicación Se llevan a cabo las transformaciones especificadas en la receta preparada a un conjunto de datos. Finalmente, la función bake() lleva a cabo la transformación de un conjunto de datos a través de las estimaciones indicadas en una receta y aplica las operaciones a un conjunto de datos para crear una matriz de diseño. La función bake(object, new_data = NULL) devolverá los datos con los que se entrenó la receta. Nota: La función juice() devolverá los resultados de una receta en la que se hayan aplicado todos los pasos a los datos. Similar a la función bake() con el comando new_data = NULL. simple_ames %&gt;% prep() %&gt;% bake(new_data = NULL) %&gt;% glimpse() ## Rows: 2,930 ## Columns: 35 ## $ Gr_Liv_Area &lt;dbl&gt; 3.219060, 2.95230… ## $ Year_Built &lt;int&gt; 1960, 1961, 1958,… ## $ Sale_Price &lt;int&gt; 215000, 105000, 1… ## $ Neighborhood_College_Creek &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Neighborhood_Old_Town &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Neighborhood_Edwards &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Neighborhood_Somerset &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Neighborhood_Northridge_Heights &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Neighborhood_Gilbert &lt;dbl&gt; 0, 0, 0, 0, 1, 1,… ## $ Neighborhood_Sawyer &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Neighborhood_Northwest_Ames &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Neighborhood_Sawyer_West &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Neighborhood_Mitchell &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Neighborhood_Brookside &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Neighborhood_Crawford &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Neighborhood_Iowa_DOT_and_Rail_Road &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Neighborhood_Timberland &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Neighborhood_Northridge &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Neighborhood_Stone_Brook &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Neighborhood_South_and_West_of_Iowa_State_University &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Neighborhood_Clear_Creek &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Neighborhood_Meadow_Village &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Neighborhood_Briardale &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Neighborhood_Bloomington_Heights &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Neighborhood_Veenker &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Neighborhood_Northpark_Villa &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Neighborhood_Blueste &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Neighborhood_Greens &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Neighborhood_Green_Hills &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Neighborhood_Landmark &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Neighborhood_Hayden_Lake &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Bldg_Type_TwoFmCon &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Bldg_Type_Duplex &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Bldg_Type_Twnhs &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… ## $ Bldg_Type_TwnhsE &lt;dbl&gt; 0, 0, 0, 0, 0, 0,… En cuanto a las transformaciones posibles, existe una gran cantidad de funciones que soportan este proceso. En esta sección se muestran algunas de las transformación más comunes, entre ellas: Normalización Dicotomización Creación de nuevas columnas Datos faltantes Imputaciones Interacciones Etc. 5.7.1.1 Normalizar columnas numéricas Quizá la transformación numérica más usada en todos los modelos es la estandarización o normalización de variables numéricas. Este proceso se realiza para homologar la escala de las variables numéricas, de modo que no predomine una sobre otra debido a la diferencia de magnitudes o escalas. Este proceso se tiene de fondo el siguiente proceso estadístico: \\[Z=\\frac{X-\\hat{\\mu}_x}{\\hat{\\sigma}_x}\\] Donde: X = Es una variable o columna numérica \\(\\hat{\\mu}_x\\) = Es la estimación de la media de la variable X \\(\\hat{\\sigma}_x\\) = Es la estimación de la desviación estándar de la variable X La librería recipes nos permite realizar este proceso ágilmente mediante la función: step_normalize(). ames %&gt;% select(Sale_Price, Neighborhood, Gr_Liv_Area, Year_Built, Bldg_Type) %&gt;% head(5) ## # A tibble: 5 × 5 ## Sale_Price Neighborhood Gr_Liv_Area Year_Built Bldg_Type ## &lt;int&gt; &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt; ## 1 215000 North_Ames 1656 1960 OneFam ## 2 105000 North_Ames 896 1961 OneFam ## 3 172000 North_Ames 1329 1958 OneFam ## 4 244000 North_Ames 2110 1968 OneFam ## 5 189900 Gilbert 1629 1997 OneFam simple_ames &lt;- recipe(Sale_Price ~ ., data = ames) %&gt;% step_normalize(all_numeric_predictors()) simple_ames ## Recipe ## ## Inputs: ## ## role #variables ## outcome 1 ## predictor 73 ## ## Operations: ## ## Centering and scaling for all_numeric_predictors() simple_ames %&gt;% prep() %&gt;% bake(new_data = NULL) %&gt;% select(Sale_Price, Neighborhood, Gr_Liv_Area, Year_Built, Bldg_Type) %&gt;% head(5) ## # A tibble: 5 × 5 ## Sale_Price Neighborhood Gr_Liv_Area Year_Built Bldg_Type ## &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 215000 North_Ames 0.309 -0.375 OneFam ## 2 105000 North_Ames -1.19 -0.342 OneFam ## 3 172000 North_Ames -0.338 -0.442 OneFam ## 4 244000 North_Ames 1.21 -0.111 OneFam ## 5 189900 Gilbert 0.256 0.848 OneFam 5.7.1.2 Dicotomización de categorías Otra transformación necesaria en la mayoría de los modelos predictivos en la creación de las variables dummy. Se mencionó anteriormente que los modelos requieren de una matriz numérica de características explicativas que permita calcular patrones estadísticos para predecir la variable de respuesta. El proceso de dicotomización consiste en crear una variable dicotómica por cada categoría de una columna con valores nominales. ames %&gt;% select(Sale_Price, Bldg_Type) %&gt;% head(5) ## # A tibble: 5 × 2 ## Sale_Price Bldg_Type ## &lt;int&gt; &lt;fct&gt; ## 1 215000 OneFam ## 2 105000 OneFam ## 3 172000 OneFam ## 4 244000 OneFam ## 5 189900 OneFam ames %&gt;% select(Bldg_Type) %&gt;% distinct() %&gt;% pull() ## [1] OneFam TwnhsE Twnhs Duplex TwoFmCon ## Levels: OneFam TwoFmCon Duplex Twnhs TwnhsE simple_ames &lt;- recipe(Sale_Price ~ Bldg_Type, data = ames) %&gt;% step_dummy(all_nominal_predictors()) %&gt;% prep() simple_ames ## Recipe ## ## Inputs: ## ## role #variables ## outcome 1 ## predictor 1 ## ## Training data contained 2930 data points and no missing data. ## ## Operations: ## ## Dummy variables from Bldg_Type [trained] simple_ames %&gt;% bake(new_data = NULL) %&gt;% head(5) ## # A tibble: 5 × 5 ## Sale_Price Bldg_Type_TwoFmCon Bldg_Type_Duplex Bldg_Type_Twnhs Bldg_Type_Twn…¹ ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 215000 0 0 0 0 ## 2 105000 0 0 0 0 ## 3 172000 0 0 0 0 ## 4 244000 0 0 0 0 ## 5 189900 0 0 0 0 ## # … with abbreviated variable name ¹​Bldg_Type_TwnhsE El proceso de dicotomización demanda que únicamente (n-1) categorías sean expresadas, mientras que la restante será considerada la categoría default o basal. Esta última categoría es la usada en el modelo cuando todas las demás se encuentran ausentes. 5.7.1.3 Codificación de datos cualitativos nuevos o faltantes Una de las tareas de ingeniería de datos más comunes es el tratamiento de datos faltantes, datos no antes vistos y datos con poca frecuencia. El problema principal con estos casos es que los modelos no saben cómo relacionar estos eventos con futuras predicciones. Es conveniente realizar las transformaciones necesarias de tratamiento de estos datos antes de pasar a la etapa de modelado. Por ejemplo: step_unknown() cambia los valores perdidos en un nivel de factor “desconocido”. step_other() analiza las frecuencias de los niveles de los factores en el conjunto de datos y convierte los valores que ocurren con poca frecuencia a un nivel general de “otro”, con un umbral que se puede especificar. step_novel() puede asignar un nuevo nivel si anticipamos que se puede encontrar un nuevo factor en datos futuros. Un buen ejemplo es el predictor de vecindad en nuestros datos. Aquí hay dos vecindarios que tienen menos de cinco propiedades. ggplot(data = ames, aes(y = Neighborhood)) + geom_bar() + labs(y = NULL) Para algunos modelos, puede resultar problemático tener variables dummy con una sola entrada distinta de cero en la columna. Como mínimo, es muy improbable que estas características sean importantes para un modelo. Si agregamos step_other (Neighborhood, threshold = 0.01) a nuestra receta, el último \\(1\\%\\) de los vecindarios se agrupará en un nuevo nivel llamado “otro”, esto atrapará a 8 vecindarios. simple_ames &lt;- recipe( Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type, data = ames) %&gt;% step_other(Neighborhood, threshold = 0.01) %&gt;% prep() ejemplo &lt;- juice(simple_ames) ggplot(ejemplo, aes(y = Neighborhood)) + geom_bar() + labs(y = NULL) 5.7.2 Imputaciones La función step_unknown crea una categoría nombrada unknown, la cual sirve como reemplazo de datos categóricos faltantes, sin embargo, para imputar datos numéricos se requiere de otra estrategia. Las imputaciones o sustituciones más comunes son realizadas a través de medidas de tendencia central tales como la media y mediana. A continuación se muestra un ejemplo: ames_na &lt;- ames ames_na[sample(nrow(ames), 5), c(&quot;Gr_Liv_Area&quot;, &quot;Lot_Area&quot;)] &lt;- NA ames_na %&gt;% filter(is.na(Gr_Liv_Area) | is.na(Lot_Area)) %&gt;% select(Sale_Price, Gr_Liv_Area, Lot_Area) ## # A tibble: 5 × 3 ## Sale_Price Gr_Liv_Area Lot_Area ## &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 130000 NA NA ## 2 143000 NA NA ## 3 189000 NA NA ## 4 302000 NA NA ## 5 111500 NA NA simple_ames &lt;- recipe(Sale_Price ~ Gr_Liv_Area + Lot_Area, data = ames_na) %&gt;% step_impute_mean(Gr_Liv_Area) %&gt;% step_impute_median(Lot_Area) %&gt;% prep() bake(simple_ames, new_data = ames_na) %&gt;% filter(is.na(Gr_Liv_Area) | is.na(Lot_Area)) ## # A tibble: 0 × 3 ## # … with 3 variables: Gr_Liv_Area &lt;int&gt;, Lot_Area &lt;int&gt;, Sale_Price &lt;int&gt; Forzamos algunos renglones a que sean omitidos aleatoriamente. Posteriormente, estos valores son imputados mediante su media y mediana. 5.7.3 Agregar o modificar columnas Quizá la transformación más usada sea la agregación o mutación de columnas existentes. Similar a la función mutate() de dplyr, la función step_mutate() se encarga de realizar esta tarea dentro de un pipeline o receta. ejemplo &lt;- recipe( Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + Year_Remod_Add, data = ames) %&gt;% step_mutate( Sale_Price_Peso = Sale_Price * 19.87, Last_Inversion = Year_Remod_Add - Year_Built ) %&gt;% step_arrange(desc(Last_Inversion)) %&gt;% prep() ejemplo ## Recipe ## ## Inputs: ## ## role #variables ## outcome 1 ## predictor 5 ## ## Training data contained 2930 data points and no missing data. ## ## Operations: ## ## Variable mutation for ~Sale_Price * 19.87, ~Year_Remod_Add - Yea... [trained] ## Row arrangement using ~desc(Last_Inversion) [trained] ejemplo %&gt;% bake(new_data = NULL) %&gt;% select(Sale_Price, Sale_Price_Peso, Year_Remod_Add, Year_Built, Last_Inversion) ## # A tibble: 2,930 × 5 ## Sale_Price Sale_Price_Peso Year_Remod_Add Year_Built Last_Inversion ## &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 131000 2602970 2007 1880 127 ## 2 265979 5285003. 2003 1880 123 ## 3 295000 5861650 2002 1880 122 ## 4 94000 1867780 1996 1875 121 ## 5 138000 2742060 2006 1890 116 ## 6 122000 2424140 1987 1872 115 ## 7 240000 4768800 2002 1890 112 ## 8 119600 2376452 2006 1895 111 ## 9 124000 2463880 1991 1880 111 ## 10 100000 1987000 1995 1885 110 ## # … with 2,920 more rows En este ejemplo se realiza la creación de una nueva variable y la modificación de una ya existente. 5.7.4 Interacciones Los efectos de interacción involucran dos o más predictores. Tal efecto ocurre cuando un predictor tiene un efecto sobre el resultado que depende de uno o más predictores. Numéricamente, un término de interacción entre predictores se codifica como su producto. Las interacciones solo se definen en términos de su efecto sobre el resultado y pueden ser combinaciones de diferentes tipos de datos (por ejemplo, numéricos, categóricos, etc.). Después de explorar el conjunto de datos de Ames, podríamos encontrar que las pendientes de regresión para el área habitable bruta difieren para los diferentes tipos de edificios: ggplot(ames, aes(x = Gr_Liv_Area, y = Sale_Price)) + geom_point(alpha = .2) + facet_wrap(~ Bldg_Type) + geom_smooth(method = lm, formula = y ~ x, se = FALSE, col = &quot;red&quot;) + scale_x_log10() + scale_y_log10() + labs(x = &quot;Gross Living Area&quot;, y = &quot;Sale Price (USD)&quot;) Con la receta actual, step_dummy() ya ha creado variables ficticias. ¿Cómo combinaríamos estos para una interacción? El paso adicional se vería como step_interact(~ términos de interacción) donde los términos en el lado derecho de la tilde son las interacciones. Estos pueden incluir selectores, por lo que sería apropiado usar: simple_ames &lt;- recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type, data = ames) %&gt;% step_other(Neighborhood, threshold = 0.05) %&gt;% step_dummy(all_nominal_predictors()) %&gt;% step_interact( ~ Gr_Liv_Area:starts_with(&quot;Bldg_Type_&quot;) ) %&gt;% prep() simple_ames %&gt;% bake(new_data = NULL) %&gt;% glimpse() ## Rows: 2,930 ## Columns: 19 ## $ Gr_Liv_Area &lt;int&gt; 1656, 896, 1329, 2110, 1629, 1604, 13… ## $ Year_Built &lt;int&gt; 1960, 1961, 1958, 1968, 1997, 1998, 2… ## $ Sale_Price &lt;int&gt; 215000, 105000, 172000, 244000, 18990… ## $ Neighborhood_College_Creek &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ Neighborhood_Old_Town &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ Neighborhood_Edwards &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ Neighborhood_Somerset &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ Neighborhood_Northridge_Heights &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ Neighborhood_Gilbert &lt;dbl&gt; 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1… ## $ Neighborhood_Sawyer &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ Neighborhood_other &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0… ## $ Bldg_Type_TwoFmCon &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ Bldg_Type_Duplex &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ Bldg_Type_Twnhs &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ Bldg_Type_TwnhsE &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0… ## $ Gr_Liv_Area_x_Bldg_Type_TwoFmCon &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ Gr_Liv_Area_x_Bldg_Type_Duplex &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ Gr_Liv_Area_x_Bldg_Type_Twnhs &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ Gr_Liv_Area_x_Bldg_Type_TwnhsE &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1338, 1280, 1616, 0… Se pueden especificar interacciones adicionales en esta fórmula separándolas con el signo \\(*\\). 5.7.5 Transformaciones generales Reflejando las operaciones originales de dplyr, los siguientes pasos se pueden usar para realizar una variedad de operaciones básicas a los datos. step_select(): Selecciona un subconjunto de variables específicas en el conjunto de datos. step_mutate(): Crea una nueva variable o modifica una existente usando dplyr::mutate(). step_mutate_at(): Lee una especificación de un paso de receta que modificará las variables seleccionadas usando una función común a través de dplyr::mutate_at(). step_filter(): Crea una especificación de un paso de receta que eliminará filas usando dplyr::filter(). step_arrange(): Ordena el conjunto de datos de acuerdo con una o más variables. step_rm(): Crea una especificación de un paso de receta que eliminará las variables según su nombre, tipo o función. step_nzv(): Realiza una selección de variables eliminando todas aquellas cuya varianza se encuentre cercana a cero. step_naomit(): Elimina todos los renglones que tengan alguna variable con valores perdidos. step_normalize(): Centra y escala las variables numéricas especificadas, generando una transformación a una distribución normal estándar. step_range(): Transforma el rango de un conjunto de variables numéricas al especificado. step_interact(): Crea un nuevo conjunto de variables basadas en la interacción entre dos variables. step_ratio(): Crea una nueva variable a partir del cociente entre dos variables. all_predictors(): Selecciona a todos los predictores del conjunto de entrenamiento para aplicarles alguna de las funciones mencionadas. all_numeric_predictors(): Selecciona a todos los predictores numéricos del conjunto de entrenamiento para aplicarles alguna de las funciones mencionadas. all_nominal_predictors(): Selecciona a todos los predictores nominales del conjunto de entrenamiento para aplicarles alguna de las funciones mencionadas. La guía completa de las familia de funciones step puede consultarse en la documentación oficial 5.8 Datos y tipos de modelos En este curso se realizarán ejemplos tanto de regresión como de clasificación. Cada uno de los modelos a estudiar se implementarán tanto para respuestas continuas como variables categóricas Regresión: Preparación de datos En esta sección, prepararemos datos para ajustar modelos de regresión y de clasificación, usando la paquetería recipes. Primero ajustaremos la receta, después obtendremos la receta actualizada con las estimaciones y al final el conjunto de datos listo para el modelo. Datos de regresión: Ames Housing Data Los datos que usaremos son los de Ames Housing Data, el conjunto de datos contiene información de la Ames Assessor’s Office utilizada para calcular valuaciones para propiedades residenciales individuales vendidas en Ames, IA, de 2006 a 2010. Podemos encontrar más información en el siguiente link Ames Housing Data. library(tidymodels) library(stringr) library(tidyverse) data(ames) glimpse(ames) ## Rows: 2,930 ## Columns: 74 ## $ MS_SubClass &lt;fct&gt; One_Story_1946_and_Newer_All_Styles, One_Story_1946… ## $ MS_Zoning &lt;fct&gt; Residential_Low_Density, Residential_High_Density, … ## $ Lot_Frontage &lt;dbl&gt; 141, 80, 81, 93, 74, 78, 41, 43, 39, 60, 75, 0, 63,… ## $ Lot_Area &lt;int&gt; 31770, 11622, 14267, 11160, 13830, 9978, 4920, 5005… ## $ Street &lt;fct&gt; Pave, Pave, Pave, Pave, Pave, Pave, Pave, Pave, Pav… ## $ Alley &lt;fct&gt; No_Alley_Access, No_Alley_Access, No_Alley_Access, … ## $ Lot_Shape &lt;fct&gt; Slightly_Irregular, Regular, Slightly_Irregular, Re… ## $ Land_Contour &lt;fct&gt; Lvl, Lvl, Lvl, Lvl, Lvl, Lvl, Lvl, HLS, Lvl, Lvl, L… ## $ Utilities &lt;fct&gt; AllPub, AllPub, AllPub, AllPub, AllPub, AllPub, All… ## $ Lot_Config &lt;fct&gt; Corner, Inside, Corner, Corner, Inside, Inside, Ins… ## $ Land_Slope &lt;fct&gt; Gtl, Gtl, Gtl, Gtl, Gtl, Gtl, Gtl, Gtl, Gtl, Gtl, G… ## $ Neighborhood &lt;fct&gt; North_Ames, North_Ames, North_Ames, North_Ames, Gil… ## $ Condition_1 &lt;fct&gt; Norm, Feedr, Norm, Norm, Norm, Norm, Norm, Norm, No… ## $ Condition_2 &lt;fct&gt; Norm, Norm, Norm, Norm, Norm, Norm, Norm, Norm, Nor… ## $ Bldg_Type &lt;fct&gt; OneFam, OneFam, OneFam, OneFam, OneFam, OneFam, Twn… ## $ House_Style &lt;fct&gt; One_Story, One_Story, One_Story, One_Story, Two_Sto… ## $ Overall_Cond &lt;fct&gt; Average, Above_Average, Above_Average, Average, Ave… ## $ Year_Built &lt;int&gt; 1960, 1961, 1958, 1968, 1997, 1998, 2001, 1992, 199… ## $ Year_Remod_Add &lt;int&gt; 1960, 1961, 1958, 1968, 1998, 1998, 2001, 1992, 199… ## $ Roof_Style &lt;fct&gt; Hip, Gable, Hip, Hip, Gable, Gable, Gable, Gable, G… ## $ Roof_Matl &lt;fct&gt; CompShg, CompShg, CompShg, CompShg, CompShg, CompSh… ## $ Exterior_1st &lt;fct&gt; BrkFace, VinylSd, Wd Sdng, BrkFace, VinylSd, VinylS… ## $ Exterior_2nd &lt;fct&gt; Plywood, VinylSd, Wd Sdng, BrkFace, VinylSd, VinylS… ## $ Mas_Vnr_Type &lt;fct&gt; Stone, None, BrkFace, None, None, BrkFace, None, No… ## $ Mas_Vnr_Area &lt;dbl&gt; 112, 0, 108, 0, 0, 20, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6… ## $ Exter_Cond &lt;fct&gt; Typical, Typical, Typical, Typical, Typical, Typica… ## $ Foundation &lt;fct&gt; CBlock, CBlock, CBlock, CBlock, PConc, PConc, PConc… ## $ Bsmt_Cond &lt;fct&gt; Good, Typical, Typical, Typical, Typical, Typical, … ## $ Bsmt_Exposure &lt;fct&gt; Gd, No, No, No, No, No, Mn, No, No, No, No, No, No,… ## $ BsmtFin_Type_1 &lt;fct&gt; BLQ, Rec, ALQ, ALQ, GLQ, GLQ, GLQ, ALQ, GLQ, Unf, U… ## $ BsmtFin_SF_1 &lt;dbl&gt; 2, 6, 1, 1, 3, 3, 3, 1, 3, 7, 7, 1, 7, 3, 3, 1, 3, … ## $ BsmtFin_Type_2 &lt;fct&gt; Unf, LwQ, Unf, Unf, Unf, Unf, Unf, Unf, Unf, Unf, U… ## $ BsmtFin_SF_2 &lt;dbl&gt; 0, 144, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1120, 0… ## $ Bsmt_Unf_SF &lt;dbl&gt; 441, 270, 406, 1045, 137, 324, 722, 1017, 415, 994,… ## $ Total_Bsmt_SF &lt;dbl&gt; 1080, 882, 1329, 2110, 928, 926, 1338, 1280, 1595, … ## $ Heating &lt;fct&gt; GasA, GasA, GasA, GasA, GasA, GasA, GasA, GasA, Gas… ## $ Heating_QC &lt;fct&gt; Fair, Typical, Typical, Excellent, Good, Excellent,… ## $ Central_Air &lt;fct&gt; Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, … ## $ Electrical &lt;fct&gt; SBrkr, SBrkr, SBrkr, SBrkr, SBrkr, SBrkr, SBrkr, SB… ## $ First_Flr_SF &lt;int&gt; 1656, 896, 1329, 2110, 928, 926, 1338, 1280, 1616, … ## $ Second_Flr_SF &lt;int&gt; 0, 0, 0, 0, 701, 678, 0, 0, 0, 776, 892, 0, 676, 0,… ## $ Gr_Liv_Area &lt;int&gt; 1656, 896, 1329, 2110, 1629, 1604, 1338, 1280, 1616… ## $ Bsmt_Full_Bath &lt;dbl&gt; 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, … ## $ Bsmt_Half_Bath &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ Full_Bath &lt;int&gt; 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 3, 2, … ## $ Half_Bath &lt;int&gt; 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, … ## $ Bedroom_AbvGr &lt;int&gt; 3, 2, 3, 3, 3, 3, 2, 2, 2, 3, 3, 3, 3, 2, 1, 4, 4, … ## $ Kitchen_AbvGr &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … ## $ TotRms_AbvGrd &lt;int&gt; 7, 5, 6, 8, 6, 7, 6, 5, 5, 7, 7, 6, 7, 5, 4, 12, 8,… ## $ Functional &lt;fct&gt; Typ, Typ, Typ, Typ, Typ, Typ, Typ, Typ, Typ, Typ, T… ## $ Fireplaces &lt;int&gt; 2, 0, 0, 2, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, … ## $ Garage_Type &lt;fct&gt; Attchd, Attchd, Attchd, Attchd, Attchd, Attchd, Att… ## $ Garage_Finish &lt;fct&gt; Fin, Unf, Unf, Fin, Fin, Fin, Fin, RFn, RFn, Fin, F… ## $ Garage_Cars &lt;dbl&gt; 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, … ## $ Garage_Area &lt;dbl&gt; 528, 730, 312, 522, 482, 470, 582, 506, 608, 442, 4… ## $ Garage_Cond &lt;fct&gt; Typical, Typical, Typical, Typical, Typical, Typica… ## $ Paved_Drive &lt;fct&gt; Partial_Pavement, Paved, Paved, Paved, Paved, Paved… ## $ Wood_Deck_SF &lt;int&gt; 210, 140, 393, 0, 212, 360, 0, 0, 237, 140, 157, 48… ## $ Open_Porch_SF &lt;int&gt; 62, 0, 36, 0, 34, 36, 0, 82, 152, 60, 84, 21, 75, 0… ## $ Enclosed_Porch &lt;int&gt; 0, 0, 0, 0, 0, 0, 170, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ Three_season_porch &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ Screen_Porch &lt;int&gt; 0, 120, 0, 0, 0, 0, 0, 144, 0, 0, 0, 0, 0, 0, 140, … ## $ Pool_Area &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ Pool_QC &lt;fct&gt; No_Pool, No_Pool, No_Pool, No_Pool, No_Pool, No_Poo… ## $ Fence &lt;fct&gt; No_Fence, Minimum_Privacy, No_Fence, No_Fence, Mini… ## $ Misc_Feature &lt;fct&gt; None, None, Gar2, None, None, None, None, None, Non… ## $ Misc_Val &lt;int&gt; 0, 0, 12500, 0, 0, 0, 0, 0, 0, 0, 0, 500, 0, 0, 0, … ## $ Mo_Sold &lt;int&gt; 5, 6, 6, 4, 3, 6, 4, 1, 3, 6, 4, 3, 5, 2, 6, 6, 6, … ## $ Year_Sold &lt;int&gt; 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 201… ## $ Sale_Type &lt;fct&gt; WD , WD , WD , WD , WD , WD , WD , WD , WD , WD , W… ## $ Sale_Condition &lt;fct&gt; Normal, Normal, Normal, Normal, Normal, Normal, Nor… ## $ Sale_Price &lt;int&gt; 215000, 105000, 172000, 244000, 189900, 195500, 213… ## $ Longitude &lt;dbl&gt; -93.61975, -93.61976, -93.61939, -93.61732, -93.638… ## $ Latitude &lt;dbl&gt; 42.05403, 42.05301, 42.05266, 42.05125, 42.06090, 4… 5.8.1 Separación de los datos El primer paso para crear un modelo de regresión es dividir nuestros datos originales en un conjunto de entrenamiento y prueba. No hay que olvidar usar siempre una semilla con la función set.seed() para que sus resultados sean reproducibles. Primero usaremos la función initial_split() de rsample para dividir los datos ames en conjuntos de entrenamiento y prueba. Usamos el parámetro prop para indicar la proporción de los conjuntos train y test. set.seed(4595) ames_split &lt;- initial_split(ames, prop = 0.75) El objeto ames_split es un objeto rsplit y solo contiene la información de partición, para obtener los conjuntos de datos resultantes, aplicamos dos funciones adicionales, training y testing. ames_train &lt;- training(ames_split) ames_test &lt;- testing(ames_split) Estos objetos son data frames con las mismas columnas que los datos originales, pero solo las filas apropiadas para cada conjunto. También existe la función vfold_cv que se usa para crear v particiones del conjunto de entrenamiento. set.seed(2453) ames_folds&lt;- vfold_cv(ames_train, v = 10) Ya con los conjuntos de entrenamiento y prueba definidos, iniciaremos con feature engineering sobre el conjunto de entrenamiento. 5.8.2 Definición de la receta Ahora usaremos la función vista en la sección anterior, recipe(), para definir los pasos de preprocesamiento antes de usar los datos para modelado. receta_casas &lt;- recipe(Sale_Price ~ . , data = ames_train) %&gt;% step_unknown(Alley) %&gt;% step_rename(Year_Remod = Year_Remod_Add) %&gt;% step_rename(ThirdSsn_Porch = Three_season_porch) %&gt;% step_ratio(Bedroom_AbvGr, denom = denom_vars(Gr_Liv_Area)) %&gt;% step_mutate( Age_House = Year_Sold - Year_Remod, TotalSF = Gr_Liv_Area + Total_Bsmt_SF, AvgRoomSF = Gr_Liv_Area / TotRms_AbvGrd, Pool = if_else(Pool_Area &gt; 0, 1, 0), Exter_Cond = forcats::fct_collapse(Exter_Cond, Good = c(&quot;Typical&quot;, &quot;Good&quot;, &quot;Excellent&quot;))) %&gt;% step_relevel(Exter_Cond, ref_level = &quot;Good&quot;) %&gt;% step_normalize(all_predictors(), -all_nominal()) %&gt;% step_dummy(all_nominal()) %&gt;% step_interact(~ Second_Flr_SF:First_Flr_SF) %&gt;% step_interact(~ matches(&quot;Bsmt_Cond&quot;):TotRms_AbvGrd) %&gt;% step_rm( First_Flr_SF, Second_Flr_SF, Year_Remod, Bsmt_Full_Bath, Bsmt_Half_Bath, Kitchen_AbvGr, BsmtFin_Type_1_Unf, Total_Bsmt_SF, Kitchen_AbvGr, Pool_Area, Gr_Liv_Area, Sale_Type_Oth, Sale_Type_VWD ) %&gt;% prep() Usamos la función step_mutate() para generar nuevas variables dentro de la receta. La función step_interact() nos ayuda a crear nuevas variables que son interacciones entre las variables especificadas. Con la función step_ratio() creamos proporciones con las variables especificadas. forcats::fct_collapse() se usa para recategorizar variables, colapsando categorías de la variable. step_relevel nos ayuda a asignar la categoria deseada de una variable como referencia. step_normalize() es de gran utilidad ya que sirve para normalizar las variables que se le indiquen. step_dummy() Nos ayuda a crear variables One Hot Encoding. Por último usamos la función step_rm() para eliminar variables que no son de utilidad para el modelo. Ahora crearemos algunas variables auxiliares que podrían ser de utilidad para el ajuste de un modelo de regresión. Recordemos que la función recipe() solo son los pasos a seguir, necesitamos usar la función prep() que nos devuelve una receta actualizada con las estimaciones y la función juice() que nos devuelve la matriz de diseño. casa_juiced &lt;- juice(receta_casas) casa_juiced ## # A tibble: 2,197 × 275 ## Lot_Frontage Lot_A…¹ Year_…² Mas_V…³ BsmtF…⁴ BsmtF…⁵ Bsmt_U…⁶ Full_…⁷ Half_…⁸ ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.214 0.604 0.848 -0.572 -0.512 -0.293 -1.03 0.795 1.25 ## 2 0.363 -0.216 -0.114 -0.572 -0.512 -0.293 -1.07 -1.03 -0.747 ## 3 1.05 -0.0159 1.08 -0.572 1.26 -0.293 1.99 0.795 -0.747 ## 4 -0.173 4.87 1.15 3.49 -0.512 -0.293 2.28 0.795 1.25 ## 5 0.512 0.256 1.11 0.526 1.26 -0.293 1.24 0.795 1.25 ## 6 -0.233 -0.696 -0.844 -0.572 1.26 -0.293 0.00546 -1.03 -0.747 ## 7 0.125 -0.261 -0.446 -0.572 -0.512 -0.293 -0.802 -1.03 -0.747 ## 8 0.571 -0.193 -0.479 -0.572 0.819 -0.293 0.140 -1.03 -0.747 ## 9 0.274 0.704 0.981 -0.572 1.26 -0.293 1.95 0.795 -0.747 ## 10 0.0651 -0.356 -0.712 -0.572 0.375 -0.293 -1.27 -1.03 -0.747 ## # … with 2,187 more rows, 266 more variables: Bedroom_AbvGr &lt;dbl&gt;, ## # TotRms_AbvGrd &lt;dbl&gt;, Fireplaces &lt;dbl&gt;, Garage_Cars &lt;dbl&gt;, ## # Garage_Area &lt;dbl&gt;, Wood_Deck_SF &lt;dbl&gt;, Open_Porch_SF &lt;dbl&gt;, ## # Enclosed_Porch &lt;dbl&gt;, ThirdSsn_Porch &lt;dbl&gt;, Screen_Porch &lt;dbl&gt;, ## # Misc_Val &lt;dbl&gt;, Mo_Sold &lt;dbl&gt;, Year_Sold &lt;dbl&gt;, Longitude &lt;dbl&gt;, ## # Latitude &lt;dbl&gt;, Sale_Price &lt;int&gt;, Bedroom_AbvGr_o_Gr_Liv_Area &lt;dbl&gt;, ## # Age_House &lt;dbl&gt;, TotalSF &lt;dbl&gt;, AvgRoomSF &lt;dbl&gt;, Pool &lt;dbl&gt;, … casa_test_bake &lt;- bake(receta_casas, new_data = ames_test) glimpse(casa_test_bake) ## Rows: 733 ## Columns: 275 ## $ Lot_Frontage &lt;dbl&gt; 0.6607556, -1.72… ## $ Lot_Area &lt;dbl&gt; 0.15995167, -0.2… ## $ Year_Built &lt;dbl&gt; -0.34660802, 0.6… ## $ Mas_Vnr_Area &lt;dbl&gt; -0.5721904, -0.5… ## $ BsmtFin_SF_1 &lt;dbl&gt; 0.81885215, -1.3… ## $ BsmtFin_SF_2 &lt;dbl&gt; 0.5851851, -0.29… ## $ Bsmt_Unf_SF &lt;dbl&gt; -0.65580176, -0.… ## $ Full_Bath &lt;dbl&gt; -1.0284858, 0.79… ## $ Half_Bath &lt;dbl&gt; -0.7465678, -0.7… ## $ Bedroom_AbvGr &lt;dbl&gt; -1.0749072, 0.15… ## $ TotRms_AbvGrd &lt;dbl&gt; -0.9221672, -0.2… ## $ Fireplaces &lt;dbl&gt; -0.9297733, -0.9… ## $ Garage_Cars &lt;dbl&gt; -1.0124349, 0.29… ## $ Garage_Area &lt;dbl&gt; 1.19047388, -0.2… ## $ Wood_Deck_SF &lt;dbl&gt; 0.3353735, 3.013… ## $ Open_Porch_SF &lt;dbl&gt; -0.70298891, -0.… ## $ Enclosed_Porch &lt;dbl&gt; -0.3536614, -0.3… ## $ ThirdSsn_Porch &lt;dbl&gt; -0.1029207, -0.1… ## $ Screen_Porch &lt;dbl&gt; 1.9128593, -0.28… ## $ Misc_Val &lt;dbl&gt; -0.09569659, 0.6… ## $ Mo_Sold &lt;dbl&gt; -0.09320608, -1.… ## $ Year_Sold &lt;dbl&gt; 1.672416, 1.6724… ## $ Longitude &lt;dbl&gt; 0.90531385, 0.27… ## $ Latitude &lt;dbl&gt; 1.00647452, 1.24… ## $ Sale_Price &lt;int&gt; 105000, 185000, … ## $ Bedroom_AbvGr_o_Gr_Liv_Area &lt;dbl&gt; 0.34141287, 0.83… ## $ Age_House &lt;dbl&gt; 1.21786294, -0.9… ## $ TotalSF &lt;dbl&gt; -0.94110413, -0.… ## $ AvgRoomSF &lt;dbl&gt; -1.11699593, -0.… ## $ Pool &lt;dbl&gt; -0.0709206, -0.0… ## $ MS_SubClass_One_Story_1945_and_Older &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ MS_SubClass_One_Story_with_Finished_Attic_All_Ages &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ MS_SubClass_One_and_Half_Story_Unfinished_All_Ages &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ MS_SubClass_One_and_Half_Story_Finished_All_Ages &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ MS_SubClass_Two_Story_1946_and_Newer &lt;dbl&gt; 0, 0, 1, 0, 0, 1… ## $ MS_SubClass_Two_Story_1945_and_Older &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ MS_SubClass_Two_and_Half_Story_All_Ages &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ MS_SubClass_Split_or_Multilevel &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ MS_SubClass_Split_Foyer &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ MS_SubClass_Duplex_All_Styles_and_Ages &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ MS_SubClass_One_Story_PUD_1946_and_Newer &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ MS_SubClass_One_and_Half_Story_PUD_All_Ages &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ MS_SubClass_Two_Story_PUD_1946_and_Newer &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ MS_SubClass_PUD_Multilevel_Split_Level_Foyer &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ MS_SubClass_Two_Family_conversion_All_Styles_and_Ages &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ MS_Zoning_Residential_High_Density &lt;dbl&gt; 1, 0, 0, 0, 0, 0… ## $ MS_Zoning_Residential_Low_Density &lt;dbl&gt; 0, 1, 1, 1, 1, 0… ## $ MS_Zoning_Residential_Medium_Density &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ MS_Zoning_A_agr &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ MS_Zoning_C_all &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ MS_Zoning_I_all &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Street_Pave &lt;dbl&gt; 1, 1, 1, 1, 1, 1… ## $ Alley_No_Alley_Access &lt;dbl&gt; 1, 1, 1, 1, 1, 1… ## $ Alley_Paved &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Alley_unknown &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Lot_Shape_Slightly_Irregular &lt;dbl&gt; 0, 1, 1, 0, 0, 0… ## $ Lot_Shape_Moderately_Irregular &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Lot_Shape_Irregular &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Land_Contour_HLS &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Land_Contour_Low &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Land_Contour_Lvl &lt;dbl&gt; 1, 1, 1, 1, 1, 1… ## $ Utilities_NoSeWa &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Utilities_NoSewr &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Lot_Config_CulDSac &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Lot_Config_FR2 &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Lot_Config_FR3 &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Lot_Config_Inside &lt;dbl&gt; 1, 1, 1, 0, 1, 1… ## $ Land_Slope_Mod &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Land_Slope_Sev &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Neighborhood_College_Creek &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Neighborhood_Old_Town &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Neighborhood_Edwards &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Neighborhood_Somerset &lt;dbl&gt; 0, 0, 0, 0, 0, 1… ## $ Neighborhood_Northridge_Heights &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Neighborhood_Gilbert &lt;dbl&gt; 0, 1, 1, 1, 0, 0… ## $ Neighborhood_Sawyer &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Neighborhood_Northwest_Ames &lt;dbl&gt; 0, 0, 0, 0, 1, 0… ## $ Neighborhood_Sawyer_West &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Neighborhood_Mitchell &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Neighborhood_Brookside &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Neighborhood_Crawford &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Neighborhood_Iowa_DOT_and_Rail_Road &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Neighborhood_Timberland &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Neighborhood_Northridge &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Neighborhood_Stone_Brook &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Neighborhood_South_and_West_of_Iowa_State_University &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Neighborhood_Clear_Creek &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Neighborhood_Meadow_Village &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Neighborhood_Briardale &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Neighborhood_Bloomington_Heights &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Neighborhood_Veenker &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Neighborhood_Northpark_Villa &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Neighborhood_Blueste &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Neighborhood_Greens &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Neighborhood_Green_Hills &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Neighborhood_Landmark &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Neighborhood_Hayden_Lake &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Condition_1_Feedr &lt;dbl&gt; 1, 0, 0, 0, 0, 0… ## $ Condition_1_Norm &lt;dbl&gt; 0, 1, 1, 1, 1, 1… ## $ Condition_1_PosA &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Condition_1_PosN &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Condition_1_RRAe &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Condition_1_RRAn &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Condition_1_RRNe &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Condition_1_RRNn &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Condition_2_Feedr &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Condition_2_Norm &lt;dbl&gt; 1, 1, 1, 1, 1, 1… ## $ Condition_2_PosA &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Condition_2_PosN &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Condition_2_RRAe &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Condition_2_RRAn &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Condition_2_RRNn &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Bldg_Type_TwoFmCon &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Bldg_Type_Duplex &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Bldg_Type_Twnhs &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Bldg_Type_TwnhsE &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ House_Style_One_and_Half_Unf &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ House_Style_One_Story &lt;dbl&gt; 1, 1, 0, 1, 1, 0… ## $ House_Style_SFoyer &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ House_Style_SLvl &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ House_Style_Two_and_Half_Fin &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ House_Style_Two_and_Half_Unf &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ House_Style_Two_Story &lt;dbl&gt; 0, 0, 1, 0, 0, 1… ## $ Overall_Cond_Poor &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Overall_Cond_Fair &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Overall_Cond_Below_Average &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Overall_Cond_Average &lt;dbl&gt; 0, 0, 1, 1, 0, 1… ## $ Overall_Cond_Above_Average &lt;dbl&gt; 1, 0, 0, 0, 1, 0… ## $ Overall_Cond_Good &lt;dbl&gt; 0, 1, 0, 0, 0, 0… ## $ Overall_Cond_Very_Good &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Overall_Cond_Excellent &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Overall_Cond_Very_Excellent &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Roof_Style_Gable &lt;dbl&gt; 1, 1, 1, 1, 1, 1… ## $ Roof_Style_Gambrel &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Roof_Style_Hip &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Roof_Style_Mansard &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Roof_Style_Shed &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Roof_Matl_CompShg &lt;dbl&gt; 1, 1, 1, 1, 1, 1… ## $ Roof_Matl_Membran &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Roof_Matl_Metal &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Roof_Matl_Roll &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Roof_Matl_Tar.Grv &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Roof_Matl_WdShake &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Roof_Matl_WdShngl &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Exterior_1st_AsphShn &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Exterior_1st_BrkComm &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Exterior_1st_BrkFace &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Exterior_1st_CBlock &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Exterior_1st_CemntBd &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Exterior_1st_HdBoard &lt;dbl&gt; 0, 1, 0, 0, 0, 0… ## $ Exterior_1st_ImStucc &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Exterior_1st_MetalSd &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Exterior_1st_Plywood &lt;dbl&gt; 0, 0, 0, 0, 1, 0… ## $ Exterior_1st_PreCast &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Exterior_1st_Stone &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Exterior_1st_Stucco &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Exterior_1st_VinylSd &lt;dbl&gt; 1, 0, 1, 1, 0, 1… ## $ Exterior_1st_Wd.Sdng &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Exterior_1st_WdShing &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Exterior_2nd_AsphShn &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Exterior_2nd_Brk.Cmn &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Exterior_2nd_BrkFace &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Exterior_2nd_CBlock &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Exterior_2nd_CmentBd &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Exterior_2nd_HdBoard &lt;dbl&gt; 0, 1, 0, 0, 0, 0… ## $ Exterior_2nd_ImStucc &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Exterior_2nd_MetalSd &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Exterior_2nd_Other &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Exterior_2nd_Plywood &lt;dbl&gt; 0, 0, 0, 0, 1, 0… ## $ Exterior_2nd_PreCast &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Exterior_2nd_Stone &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Exterior_2nd_Stucco &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Exterior_2nd_VinylSd &lt;dbl&gt; 1, 0, 1, 1, 0, 1… ## $ Exterior_2nd_Wd.Sdng &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Exterior_2nd_Wd.Shng &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Mas_Vnr_Type_BrkFace &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Mas_Vnr_Type_CBlock &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Mas_Vnr_Type_None &lt;dbl&gt; 1, 1, 1, 1, 0, 1… ## $ Mas_Vnr_Type_Stone &lt;dbl&gt; 0, 0, 0, 0, 1, 0… ## $ Exter_Cond_Fair &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Exter_Cond_Poor &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Foundation_CBlock &lt;dbl&gt; 1, 0, 0, 1, 1, 0… ## $ Foundation_PConc &lt;dbl&gt; 0, 1, 1, 0, 0, 1… ## $ Foundation_Slab &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Foundation_Stone &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Foundation_Wood &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Bsmt_Cond_Fair &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Bsmt_Cond_Good &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Bsmt_Cond_No_Basement &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Bsmt_Cond_Poor &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Bsmt_Cond_Typical &lt;dbl&gt; 1, 1, 1, 1, 1, 1… ## $ Bsmt_Exposure_Gd &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Bsmt_Exposure_Mn &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Bsmt_Exposure_No &lt;dbl&gt; 1, 1, 1, 1, 1, 1… ## $ Bsmt_Exposure_No_Basement &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ BsmtFin_Type_1_BLQ &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ BsmtFin_Type_1_GLQ &lt;dbl&gt; 0, 0, 0, 0, 0, 1… ## $ BsmtFin_Type_1_LwQ &lt;dbl&gt; 0, 0, 0, 1, 0, 0… ## $ BsmtFin_Type_1_No_Basement &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ BsmtFin_Type_1_Rec &lt;dbl&gt; 1, 0, 0, 0, 0, 0… ## $ BsmtFin_Type_2_BLQ &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ BsmtFin_Type_2_GLQ &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ BsmtFin_Type_2_LwQ &lt;dbl&gt; 1, 0, 0, 0, 0, 0… ## $ BsmtFin_Type_2_No_Basement &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ BsmtFin_Type_2_Rec &lt;dbl&gt; 0, 0, 0, 0, 1, 0… ## $ BsmtFin_Type_2_Unf &lt;dbl&gt; 0, 1, 1, 1, 0, 1… ## $ Heating_GasA &lt;dbl&gt; 1, 1, 1, 1, 1, 1… ## $ Heating_GasW &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Heating_Grav &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Heating_OthW &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Heating_Wall &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Heating_QC_Fair &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Heating_QC_Good &lt;dbl&gt; 0, 0, 1, 0, 0, 0… ## $ Heating_QC_Poor &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Heating_QC_Typical &lt;dbl&gt; 1, 0, 0, 0, 1, 0… ## $ Central_Air_Y &lt;dbl&gt; 1, 1, 1, 1, 1, 1… ## $ Electrical_FuseF &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Electrical_FuseP &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Electrical_Mix &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Electrical_SBrkr &lt;dbl&gt; 1, 1, 1, 1, 1, 1… ## $ Electrical_Unknown &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Functional_Maj2 &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Functional_Min1 &lt;dbl&gt; 0, 0, 0, 0, 1, 0… ## $ Functional_Min2 &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Functional_Mod &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Functional_Sal &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Functional_Sev &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Functional_Typ &lt;dbl&gt; 1, 1, 1, 1, 0, 1… ## $ Garage_Type_Basment &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Garage_Type_BuiltIn &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Garage_Type_CarPort &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Garage_Type_Detchd &lt;dbl&gt; 0, 0, 0, 1, 0, 0… ## $ Garage_Type_More_Than_Two_Types &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Garage_Type_No_Garage &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Garage_Finish_No_Garage &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Garage_Finish_RFn &lt;dbl&gt; 0, 0, 0, 0, 0, 1… ## $ Garage_Finish_Unf &lt;dbl&gt; 1, 0, 0, 1, 1, 0… ## $ Garage_Cond_Fair &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Garage_Cond_Good &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Garage_Cond_No_Garage &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Garage_Cond_Poor &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Garage_Cond_Typical &lt;dbl&gt; 1, 1, 1, 1, 1, 1… ## $ Paved_Drive_Partial_Pavement &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Paved_Drive_Paved &lt;dbl&gt; 1, 1, 1, 1, 1, 1… ## $ Pool_QC_Fair &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Pool_QC_Good &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Pool_QC_No_Pool &lt;dbl&gt; 1, 1, 1, 1, 1, 1… ## $ Pool_QC_Typical &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Fence_Good_Wood &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Fence_Minimum_Privacy &lt;dbl&gt; 1, 0, 0, 0, 1, 0… ## $ Fence_Minimum_Wood_Wire &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Fence_No_Fence &lt;dbl&gt; 0, 0, 1, 1, 0, 1… ## $ Misc_Feature_Gar2 &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Misc_Feature_None &lt;dbl&gt; 1, 0, 1, 1, 1, 1… ## $ Misc_Feature_Othr &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Misc_Feature_Shed &lt;dbl&gt; 0, 1, 0, 0, 0, 0… ## $ Misc_Feature_TenC &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Sale_Type_Con &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Sale_Type_ConLD &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Sale_Type_ConLI &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Sale_Type_ConLw &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Sale_Type_CWD &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Sale_Type_New &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Sale_Type_WD. &lt;dbl&gt; 1, 1, 1, 1, 1, 1… ## $ Sale_Condition_AdjLand &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Sale_Condition_Alloca &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Sale_Condition_Family &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Sale_Condition_Normal &lt;dbl&gt; 1, 1, 1, 1, 1, 1… ## $ Sale_Condition_Partial &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Second_Flr_SF_x_First_Flr_SF &lt;dbl&gt; 0.52453728, -0.0… ## $ Bsmt_Cond_Fair_x_TotRms_AbvGrd &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Bsmt_Cond_Good_x_TotRms_AbvGrd &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Bsmt_Cond_No_Basement_x_TotRms_AbvGrd &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Bsmt_Cond_Poor_x_TotRms_AbvGrd &lt;dbl&gt; 0, 0, 0, 0, 0, 0… ## $ Bsmt_Cond_Typical_x_TotRms_AbvGrd &lt;dbl&gt; -0.9221672, -0.2… Clasificación: Preparación de Datos Ahora prepararemos los datos para un ejemplo de churn, es decir, la tasa de cancelación de clientes. Usaremos datos de Telco. En este conjunto de datos, la variable a predecir será la cancelación por parte del cliente de los servicios de telecomunicaciones contratados. telco &lt;- read_csv(&quot;data/Churn.csv&quot;) glimpse(telco) ## Rows: 7,043 ## Columns: 21 ## $ customerID &lt;chr&gt; &quot;7590-VHVEG&quot;, &quot;5575-GNVDE&quot;, &quot;3668-QPYBK&quot;, &quot;7795-CFOCW… ## $ gender &lt;chr&gt; &quot;Female&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Female&quot;, &quot;Female&quot;,… ## $ SeniorCitizen &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ Partner &lt;chr&gt; &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes… ## $ Dependents &lt;chr&gt; &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;… ## $ tenure &lt;dbl&gt; 1, 34, 2, 45, 2, 8, 22, 10, 28, 62, 13, 16, 58, 49, 2… ## $ PhoneService &lt;chr&gt; &quot;No&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;No&quot;, … ## $ MultipleLines &lt;chr&gt; &quot;No phone service&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No phone service&quot;, &quot;… ## $ InternetService &lt;chr&gt; &quot;DSL&quot;, &quot;DSL&quot;, &quot;DSL&quot;, &quot;DSL&quot;, &quot;Fiber optic&quot;, &quot;Fiber opt… ## $ OnlineSecurity &lt;chr&gt; &quot;No&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;… ## $ OnlineBackup &lt;chr&gt; &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;N… ## $ DeviceProtection &lt;chr&gt; &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Y… ## $ TechSupport &lt;chr&gt; &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes… ## $ StreamingTV &lt;chr&gt; &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;Ye… ## $ StreamingMovies &lt;chr&gt; &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes… ## $ Contract &lt;chr&gt; &quot;Month-to-month&quot;, &quot;One year&quot;, &quot;Month-to-month&quot;, &quot;One … ## $ PaperlessBilling &lt;chr&gt; &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;No&quot;, … ## $ PaymentMethod &lt;chr&gt; &quot;Electronic check&quot;, &quot;Mailed check&quot;, &quot;Mailed check&quot;, &quot;… ## $ MonthlyCharges &lt;dbl&gt; 29.85, 56.95, 53.85, 42.30, 70.70, 99.65, 89.10, 29.7… ## $ TotalCharges &lt;dbl&gt; 29.85, 1889.50, 108.15, 1840.75, 151.65, 820.50, 1949… ## $ Churn &lt;chr&gt; &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Y… 5.8.3 Separación de los datos Como en el ejemplo de regresión, primero crearemos los conjuntos de entrenamiento y de prueba. set.seed(1234) telco_split &lt;- initial_split(telco, prop = .7) telco_train &lt;- training(telco_split) telco_test &lt;- testing(telco_split) 5.8.4 Definición de la receta A continuación, se presenta el desarrollo de una receta para el conjunto de datos de Telco: telco_rec &lt;- recipe(Churn ~ ., data = telco_train) %&gt;% update_role(customerID, new_role = &quot;id variable&quot;) %&gt;% step_normalize(all_numeric_predictors()) %&gt;% step_dummy(all_nominal_predictors()) %&gt;% step_impute_median(all_numeric_predictors()) %&gt;% step_rm(customerID, skip=T) %&gt;% prep() Ahora recuperamos la matriz de diseño con las funciones prep() y juice(). telco_juiced &lt;- juice(telco_rec) glimpse(telco_juiced) ## Rows: 4,930 ## Columns: 31 ## $ SeniorCitizen &lt;dbl&gt; -0.4417148, -0.4417148, -0.44171… ## $ tenure &lt;dbl&gt; 0.1915835, 0.3140505, -0.4207516… ## $ MonthlyCharges &lt;dbl&gt; -1.50429475, 0.66367718, -0.5084… ## $ TotalCharges &lt;dbl&gt; -0.66321610, 0.47214217, -0.5462… ## $ Churn &lt;fct&gt; No, No, No, Yes, Yes, No, No, No… ## $ gender_Male &lt;dbl&gt; 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,… ## $ Partner_Yes &lt;dbl&gt; 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,… ## $ Dependents_Yes &lt;dbl&gt; 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,… ## $ PhoneService_Yes &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,… ## $ MultipleLines_No.phone.service &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,… ## $ MultipleLines_Yes &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,… ## $ InternetService_Fiber.optic &lt;dbl&gt; 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,… ## $ InternetService_No &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ OnlineSecurity_No.internet.service &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ OnlineSecurity_Yes &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,… ## $ OnlineBackup_No.internet.service &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ OnlineBackup_Yes &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,… ## $ DeviceProtection_No.internet.service &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ DeviceProtection_Yes &lt;dbl&gt; 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,… ## $ TechSupport_No.internet.service &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ TechSupport_Yes &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,… ## $ StreamingTV_No.internet.service &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ StreamingTV_Yes &lt;dbl&gt; 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,… ## $ StreamingMovies_No.internet.service &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ StreamingMovies_Yes &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,… ## $ Contract_One.year &lt;dbl&gt; 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,… ## $ Contract_Two.year &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,… ## $ PaperlessBilling_Yes &lt;dbl&gt; 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,… ## $ PaymentMethod_Credit.card..automatic. &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,… ## $ PaymentMethod_Electronic.check &lt;dbl&gt; 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,… ## $ PaymentMethod_Mailed.check &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… telco_test_bake &lt;- bake(telco_rec, new_data = telco_test) glimpse(telco_test_bake) ## Rows: 2,113 ## Columns: 32 ## $ customerID &lt;chr&gt; &quot;5575-GNVDE&quot;, &quot;9305-CDSKC&quot;, &quot;671… ## $ SeniorCitizen &lt;dbl&gt; -0.4417148, -0.4417148, -0.44171… ## $ tenure &lt;dbl&gt; 0.06911644, -0.99226439, -0.9106… ## $ MonthlyCharges &lt;dbl&gt; -0.27067882, 1.14914329, -1.1751… ## $ TotalCharges &lt;dbl&gt; -0.1752116, -0.6472105, -0.87618… ## $ Churn &lt;fct&gt; No, Yes, No, No, No, No, No, No,… ## $ gender_Male &lt;dbl&gt; 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,… ## $ Partner_Yes &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,… ## $ Dependents_Yes &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,… ## $ PhoneService_Yes &lt;dbl&gt; 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,… ## $ MultipleLines_No.phone.service &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,… ## $ MultipleLines_Yes &lt;dbl&gt; 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,… ## $ InternetService_Fiber.optic &lt;dbl&gt; 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,… ## $ InternetService_No &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,… ## $ OnlineSecurity_No.internet.service &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,… ## $ OnlineSecurity_Yes &lt;dbl&gt; 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,… ## $ OnlineBackup_No.internet.service &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,… ## $ OnlineBackup_Yes &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,… ## $ DeviceProtection_No.internet.service &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,… ## $ DeviceProtection_Yes &lt;dbl&gt; 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,… ## $ TechSupport_No.internet.service &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,… ## $ TechSupport_Yes &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,… ## $ StreamingTV_No.internet.service &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,… ## $ StreamingTV_Yes &lt;dbl&gt; 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,… ## $ StreamingMovies_No.internet.service &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,… ## $ StreamingMovies_Yes &lt;dbl&gt; 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,… ## $ Contract_One.year &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,… ## $ Contract_Two.year &lt;dbl&gt; 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,… ## $ PaperlessBilling_Yes &lt;dbl&gt; 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,… ## $ PaymentMethod_Credit.card..automatic. &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,… ## $ PaymentMethod_Electronic.check &lt;dbl&gt; 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,… ## $ PaymentMethod_Mailed.check &lt;dbl&gt; 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,… Estos fueron dos ejemplos aplicados de la paquetería recipies, existen distintas funciones step que pueden implementarse en recetas para usarse con tidymodels, en las secciones siguientes les daremos su uso para ajustar un modelo completo. "],["regresión-lineal.html", "Capítulo 6 Regresión Lineal 6.1 Ajuste de modelo 6.2 Residuos del modelo 6.3 Métricas de desempeño 6.4 Implementación en R 6.5 Métodos se selección de variables", " Capítulo 6 Regresión Lineal En esta sección aprenderemos sobre regresión lineal simple y múltiple, como se ajusta un modelo de regresión en R, las métricas de desempeño para problemas de regresión y como podemos comparar modelos con estas métricas. Existen dos tipos de modelos de regresión lineal: Regresión lineal simple: En la regresión lineal simple se utiliza una variable independiente o explicativa “X” (numérica o categórica) para estimar una variable dependiente o de respuesta numérica “Y” mediante el ajuste de una recta permita conocer la relación existente entre ambas variables. Dicha relación entre variables se expresa como: \\[Y = \\beta_0 + \\beta_1X_1 + \\epsilon \\approx b + mx\\] Donde: \\(\\epsilon \\sim Norm(0,\\sigma^2)\\) (error aleatorio) \\(\\beta_0\\) = Coeficiente de regresión 0 (Ordenada al origen o intercepto) \\(\\beta_1\\) = Coeficiente de regresión 1 (Pendiente o regresor de variable \\(X_1\\)) \\(X_1\\) = Variable explicativa observada \\(Y\\) = Respuesta numérica Debido a que los valores reales de \\(\\beta_0\\) y \\(\\beta_1\\) son desconocidos, procedemos a estimarlos estadísticamente: \\[\\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1X_1\\] Con \\(\\hat{\\beta}_0\\) el estimado de la ordenada al origen y \\(\\hat{\\beta}_1\\) el estimado de la pendiente. Regresión lineal múltiple: Cuando se utiliza más de una variable independiente, el proceso se denomina regresión lineal múltiple. En este escenario no es una recta sino un hiper-plano lo que se ajusta a partir de las covariables explicativas \\(\\{X_1, X_2, X_3, ...,X_n\\}\\) El objetivo de un modelo de regresión múltiple es tratar de explicar la relación que existe entre una variable dependiente (variable respuesta) \\(&quot;Y&quot;\\) un conjunto de variables independientes (variables explicativas) \\(\\{X1,..., Xm\\}\\), el modelo es de la forma: \\[Y = \\beta_0 + \\beta_1X_1 + \\cdot \\cdot \\cdot + \\beta_mX_m + \\epsilon\\] Donde: \\(Y\\) como variable respuesta. \\(X_1,X_2,...,X_m\\) como las variables explicativas, independientes o regresoras. \\(\\beta_1, \\beta_2,...,\\beta_m\\) Se conocen como coeficientes parciales de regresión. Cada una de ellas puede interpretarse como el efecto promedio que tiene el incremento de una unidad de la variable predictora \\(X_i\\) sobre la variable dependiente \\(Y\\), manteniéndose constantes el resto de variables. 6.1 Ajuste de modelo 6.1.1 Estimación de parámetros: Regresión lineal simple En la gran mayoría de casos, los valores \\(\\beta_0\\) y \\(\\beta_1\\) poblacionales son desconocidos, por lo que, a partir de una muestra, se obtienen sus estimaciones \\(\\hat{\\beta_0}\\) y \\(\\hat{\\beta_1}\\). Estas estimaciones se conocen como coeficientes de regresión o least square coefficient estimates, ya que toman aquellos valores que minimizan la suma de cuadrados residuales, dando lugar a la recta que pasa más cerca de todos los puntos. En términos analíticos, la expresión matemática a optimizar y solución están dadas por: \\[min(\\epsilon) \\Rightarrow min(y-\\hat{y}) = min\\{y -(\\hat{\\beta}_0 + \\hat{\\beta}_1x)\\}\\] \\[\\begin{aligned} \\hat{\\beta}_0 &amp;= \\overline{y} - \\hat{\\beta}_1\\overline{x} \\\\ \\hat{\\beta}_1 &amp;= \\frac{\\sum^n_{i=1}(x_i - \\overline{x})(y_i - \\overline{y})}{\\sum^n_{i=1}(x_i - \\overline{x})^2} =\\frac{S_{xy}}{S^2_x} \\end{aligned}\\] Donde: \\(S_{xy}\\) es la covarianza entre \\(x\\) y \\(y\\). \\(S_{x}^{2}\\) es la varianza de \\(x\\). \\(\\hat{\\beta}_0\\) es el valor esperado la variable \\(Y\\) cuando \\(X = 0\\), es decir, la intersección de la recta con el eje y. 6.1.2 Estimación de parámetros: Regresión lineal múltiple En el caso de múltiples parámetros, la notación se vuelve más sencilla al expresar el modelo mediante una combinación lineal dada por la multiplicación de matrices (álgebra lineal). \\[Y = X\\beta + \\epsilon\\] Donde: \\[Y = \\begin{pmatrix}y_1\\\\y_2\\\\.\\\\.\\\\.\\\\y_n\\end{pmatrix} \\quad \\beta = \\begin{pmatrix}\\beta_0\\\\\\beta_1\\\\.\\\\.\\\\.\\\\\\beta_m\\end{pmatrix} \\quad \\epsilon = \\begin{pmatrix}\\epsilon_1\\\\\\epsilon_2\\\\.\\\\.\\\\.\\\\\\epsilon_n\\end{pmatrix} \\quad \\quad X = \\begin{pmatrix}1 &amp; x_{11} &amp; x_{12} &amp; ... &amp; x_{1m}\\\\1 &amp; x_{21} &amp; x_{22} &amp; ... &amp; x_{2m}\\\\\\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ 1 &amp; x_{n1} &amp; x_{n2} &amp; ... &amp; x_{nm}\\end{pmatrix}\\\\\\] El estimador por mínimos cuadrados está dado por: \\[\\hat{\\beta} = (X^TX)^{-1}X^TY\\] IMPORTANTE: Es necesario entender que para cada uno de los coeficientes de regresión se realiza una prueba de hipótesis. Una vez calculado el valor estimado, se procede a determinar si este valor es significativamente distinto de cero, por lo que la hipótesis de cada coeficiente se plantea de la siguiente manera: \\[H_0:\\beta_i=0 \\quad Vs \\quad H_1:\\beta_i\\neq0\\] El software R nos devuelve el p-value asociado a cada coeficiente de regresión. Recordemos que valores pequeños de p sugieren que al rechazar \\(H_0\\), la probabilidad de equivocarnos es baja, por lo que procedemos a rechazar la hipótesis nula. 6.2 Residuos del modelo El residuo de una estimación se define como la diferencia entre el valor observado y el valor esperado acorde al modelo. \\[\\epsilon_i= y_i -\\hat{y}_i\\] A la hora de contemplar el conjunto de residuos hay dos posibilidades: La suma del valor absoluto de cada residuo. \\[RAS=\\sum_{i=1}^{n}{|e_i|}=\\sum_{i=1}^{n}{|y_i-\\hat{y}_i|}\\] La suma del cuadrado de cada residuo (RSS). Esta es la aproximación más empleada (mínimos cuadrados) ya que magnifica las desviaciones más extremas. \\[RSS=\\sum_{i=1}^{n}{e_i^2}=\\sum_{i=1}^{n}{(y_i-\\hat{y}_i)^2}\\] Los residuos son muy importantes puesto que en ellos se basan las diferentes métricas de desempeño del modelo. Condiciones para el ajuste de una regresión lineal: Existen ciertas condiciones o supuestos que deben ser validados para el correcto ajuste de un modelo de regresión lineal, los cuales se enlistan a continuación: Linealidad: La relación entre ambas variables debe ser lineal. Distribución normal de los residuos: Los residuos se tiene que distribuir de forma normal, con media igual a 0. Varianza de residuos constante (homocedasticidad): La varianza de los residuos tiene que ser aproximadamente constante. Independencia: Las observaciones deben ser independientes unas de otras. Dado que las condiciones se verifican a partir de los residuos, primero se suele generar el modelo y después se valida. 6.3 Métricas de desempeño Dado que nuestra variable a predecir es numérica, podemos medir qué tan cerca o lejos estuvimos del número esperado dada una predicción. Las métricas de desempeño asociadas a los problemas de regresión ocupan esa distancia cómo cuantificación del desempeño o de los errores cometidos por el modelo. Las métricas más utilizadas son: MEA: Mean Absolute Error MAPE: Mean Absolute Percentual Error \\(\\quad \\Rightarrow \\quad\\) más usada para reportar resultados RMSE: Root Mean Squared Error \\(\\quad \\quad \\quad \\Rightarrow \\quad\\) más usada para entrenar modelos \\(R^2\\) : R cuadrada \\(R^2\\) : \\(R^2\\) ajustada MAE: Mean Absolute Error \\[MAE = \\frac{1}{N}\\sum_{i=1}^{N}{|y_{i}-\\hat{y}_{i}|}\\] Donde: \\(N:\\) Número de observaciones predichas. \\(y_{i}:\\) Valor real. \\(\\hat{y}_{i}:\\) Valor de la predicción. Esta métrica suma los errores absolutos de cada predicción y los divide entre el número de observaciones, para obtener el promedio absoluto del error del modelo. Ventajas Vs Desventajas: Todos los errores pesan lo mismo sin importar qué tan pequeños o qué tan grandes sean, es muy sensible a valores atípicos, y dado que obtiene el promedio puede ser que un solo error en la predicción que sea muy grande afecte al valor de todo el modelo, aún y cuando el modelo no tuvo errores tan malos para el resto de las observaciones. Se recomienda utilizar esta métrica cuando los errores importan lo mismo, es decir, importa lo mismo si se equivocó muy poco o se equivocó mucho. MAPE: Mean Absolute Percentage Error \\[MAPE = \\frac{1}{N}\\sum_{i=1}^{N}\\frac{{|y_{i}-\\hat{y}_{i}|}}{|y_{i}|}\\] Donde: \\(N:\\) Número de observaciones predichas. \\(y_{i}:\\) Valor real. \\(\\hat{y}_{i}:\\) Valor de la predicción. Esta métrica es la métrica MAE expresada en porcentaje, por lo que mide el error del modelo en términos de porcentaje, al igual que con MAE, no hay errores negativos por el valor absoluto, y mientras más pequeño el error es mejor. Ventajas Vs Desventajas: Cuando existe un valor real de 0 esta métrica no se puede calcular, por otro lado, una de las ventajas sobre MAE es que no es sensible a valores atípicos. Se recomienda utilizar esta métrica cuando en tu problema no haya valores a predecir que puedan ser 0, por ejemplo, en ventas puedes llegar a tener 0 ventas, en este caso no podemos ocupar esta métrica. En general a las personas de negocio les gusta esta métrica pues es fácil de comprender. RMSE: Root Mean Squared Error \\[RMSE = \\sqrt{\\frac{1}{N}\\sum_{i=1}^{N}{(y_{i}-\\hat{y}_{i})^2}}\\] Donde: \\(N:\\) Número de observaciones predichas. \\(y_{i}:\\) Valor real. \\(\\hat{y}_{i}:\\) Valor de la predicción. Esta métrica es muy parecida a MAE, solo que en lugar de sacar el valor absoluto de la diferencia entre el valor real y el valor predicho, para evitar valores negativos eleva esta diferencia al cuadrado, y saca el promedio de esa diferencia, al final, para dejar el valor en la escala inicial saca la raíz cuadrada. Esta es la métrica más utilizada en problemas de regresión, debido a que es más fácil de optimizar que el MAE. Ventajas Vs Desventaja: Todos los errores pesan lo mismo sin importar qué tan pequeños o qué tan grandes sean, es más sensible a valores atípicos que MAE pues eleva al cuadrado diferencias, y dado que obtiene el promedio puede ser que un solo error en la predicción que sea muy grande afecte al valor de todo el modelo, aún y cuando el modelo no tuvo errores tan malos para el resto de las observaciones. Se recomienda utilizar esta métrica cuando en el problema que queremos resolver es muy costoso tener equivocaciones grandes, podemos tener varios errores pequeños, pero no grandes. \\(R^2\\): R cuadrada \\[R^{2} = \\frac{\\sum_{i=1}^{N}{(\\hat{y}_{i}-\\bar{y}_{i})^2}}{\\sum_{i=1}^{N}{(y_{i}-\\bar{y}_{i})^2}}\\] Donde: \\(N:\\) Número de observaciones predichas. \\(y_{i}:\\) Valor real. \\(\\hat{y}_{i}:\\) Valor de la predicción. \\(\\bar{y}_{i}:\\) Valor promedio de la variable y. El coeficiente de determinación es la proporción de la varianza total de la variable explicada por la regresión. El coeficiente de determinación, también llamado R cuadrado, refleja la bondad del ajuste de un modelo a la variable que pretender explicar. Es importante saber que el resultado del coeficiente de determinación oscila entre 0 y 1. Cuanto más cerca de 1 se sitúe su valor, mayor será el ajuste del modelo a la variable que estamos intentando explicar. De forma inversa, cuanto más cerca de cero, menos ajustado estará el modelo y, por tanto, menos fiable será. Ventajas Vs Desventaja: El problema del coeficiente de determinación, y razón por el cual surge el coeficiente de determinación ajustado, radica en que no penaliza la inclusión de variables explicativas no significativas, es decir, el valor de \\(R^2\\) siempre será más grande cuantas más variables sean incluidas en el modelo, aún cuando estas no sean significativas en la predicción. \\(\\bar{R}^2\\): \\(R^2\\) ajustada \\[\\bar{R}^2=1-\\frac{N-1}{N-k-1}[1-R^2]\\] Donde: \\(\\bar{R}²:\\) Es el valor de R² ajustado \\(R²:\\) Es el valor de R² original \\(N:\\) Es el total de observaciones en el ajuste \\(k:\\) Es el número de variables usadas en el modelo El coeficiente de determinación ajustado (R cuadrado ajustado) es la medida que define el porcentaje explicado por la varianza de la regresión en relación con la varianza de la variable explicada. Es decir, lo mismo que el R cuadrado, pero con una diferencia: El coeficiente de determinación ajustado penaliza la inclusión de variables. En la fórmula, N es el tamaño de la muestra y k el número de variables explicativas. 6.4 Implementación en R Usaremos las recetas antes implementadas para ajustar tanto el modelo de regresión como el de clasificación. Ajustaremos un modelo de regresión usando la receta antes vista. library(tidymodels) data(ames) set.seed(4595) ames_split &lt;- initial_split(ames, prop = 0.75) ames_train &lt;- training(ames_split) ames_test &lt;- testing(ames_split) receta_casas &lt;- recipe( Sale_Price ~ Gr_Liv_Area + TotRms_AbvGrd + Exter_Cond + Bsmt_Cond + Year_Sold + Year_Remod_Add, data = ames_train) %&gt;% step_mutate( Age_House = Year_Sold - Year_Remod_Add, Exter_Cond = forcats::fct_collapse(Exter_Cond, Good = c(&quot;Typical&quot;, &quot;Good&quot;, &quot;Excellent&quot;))) %&gt;% step_relevel(Exter_Cond, ref_level = &quot;Good&quot;) %&gt;% step_normalize(all_numeric_predictors()) %&gt;% step_dummy(all_nominal_predictors()) %&gt;% step_interact(~ matches(&quot;Bsmt_Cond&quot;):TotRms_AbvGrd) %&gt;% prep() receta_casas ## Recipe ## ## Inputs: ## ## role #variables ## outcome 1 ## predictor 6 ## ## Training data contained 2197 data points and no missing data. ## ## Operations: ## ## Variable mutation for ~Year_Sold - Year_Remod_Add, ~forcats::fct... [trained] ## Re-order factor level to ref_level for Exter_Cond [trained] ## Centering and scaling for Gr_Liv_Area, TotRms_AbvGrd, Year_Sold, Year_Rem... [trained] ## Dummy variables from Exter_Cond, Bsmt_Cond [trained] ## Interactions with (Bsmt_Cond_Fair + Bsmt_Cond_Good + Bsmt_Cond_No_Ba... [trained] casa_juiced &lt;- juice(receta_casas) casa_test_bake &lt;- bake(receta_casas, new_data = ames_test) modelo1 &lt;- linear_reg() %&gt;% set_mode(&quot;regression&quot;) %&gt;% set_engine(&quot;lm&quot;) lm_fit1 &lt;- fit(modelo1, Sale_Price ~ ., casa_juiced) p_test &lt;- predict(lm_fit1, casa_test_bake) %&gt;% bind_cols(ames_test) %&gt;% select(.pred, Sale_Price) %&gt;% mutate(error = Sale_Price - .pred) %&gt;% filter(.pred &gt; 0) p_test ## # A tibble: 733 × 3 ## .pred Sale_Price error ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 94800. 105000 10200. ## 2 174700. 185000 10300. ## 3 188958. 180400 -8558. ## 4 85587. 141000 55413. ## 5 244570. 210000 -34570. ## 6 214422. 216000 1578. ## 7 163805. 149900 -13905. ## 8 122163. 105500 -16663. ## 9 122163. 88000 -34163. ## 10 164615. 146000 -18615. ## # … with 723 more rows 6.4.1 Coeficientes del modelo Podemos recuperar los coeficientes de nuestro modelo con la función tidy() y observar cuales variables explicativas son las más significativas de acuerdo con el p-value. lm_fit1 %&gt;% tidy() %&gt;% arrange(p.value) ## # A tibble: 18 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Gr_Liv_Area 56853. 1822. 31.2 5.64e-177 ## 2 Year_Remod_Add 24376. 1115. 21.9 6.21e- 96 ## 3 (Intercept) 212797. 33979. 6.26 4.55e- 10 ## 4 TotRms_AbvGrd -9494. 1823. -5.21 2.09e- 7 ## 5 Bsmt_Cond_Fair_x_TotRms_AbvGrd -24460. 5216. -4.69 2.91e- 6 ## 6 Exter_Cond_Fair -30809. 7226. -4.26 2.10e- 5 ## 7 Bsmt_Cond_No_Basement_x_TotRms_AbvGrd -19869. 5169. -3.84 1.25e- 4 ## 8 Exter_Cond_Poor -76007. 27988. -2.72 6.67e- 3 ## 9 Bsmt_Cond_No_Basement -62919. 34565. -1.82 6.88e- 2 ## 10 Bsmt_Cond_Fair -60285. 34479. -1.75 8.05e- 2 ## 11 Bsmt_Cond_Poor_x_TotRms_AbvGrd -71907. 46682. -1.54 1.24e- 1 ## 12 Year_Sold -1484. 1033. -1.44 1.51e- 1 ## 13 Bsmt_Cond_Good_x_TotRms_AbvGrd 6447. 5025. 1.28 2.00e- 1 ## 14 Bsmt_Cond_Typical -30166. 33996. -0.887 3.75e- 1 ## 15 Bsmt_Cond_Good -16809. 34336. -0.490 6.25e- 1 ## 16 Bsmt_Cond_Poor -19736. 44381. -0.445 6.57e- 1 ## 17 Age_House NA NA NA NA ## 18 Bsmt_Cond_Typical_x_TotRms_AbvGrd NA NA NA NA lm_fit1 %&gt;% tidy() %&gt;% arrange(desc(p.value)) ## # A tibble: 18 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Bsmt_Cond_Poor -19736. 44381. -0.445 6.57e- 1 ## 2 Bsmt_Cond_Good -16809. 34336. -0.490 6.25e- 1 ## 3 Bsmt_Cond_Typical -30166. 33996. -0.887 3.75e- 1 ## 4 Bsmt_Cond_Good_x_TotRms_AbvGrd 6447. 5025. 1.28 2.00e- 1 ## 5 Year_Sold -1484. 1033. -1.44 1.51e- 1 ## 6 Bsmt_Cond_Poor_x_TotRms_AbvGrd -71907. 46682. -1.54 1.24e- 1 ## 7 Bsmt_Cond_Fair -60285. 34479. -1.75 8.05e- 2 ## 8 Bsmt_Cond_No_Basement -62919. 34565. -1.82 6.88e- 2 ## 9 Exter_Cond_Poor -76007. 27988. -2.72 6.67e- 3 ## 10 Bsmt_Cond_No_Basement_x_TotRms_AbvGrd -19869. 5169. -3.84 1.25e- 4 ## 11 Exter_Cond_Fair -30809. 7226. -4.26 2.10e- 5 ## 12 Bsmt_Cond_Fair_x_TotRms_AbvGrd -24460. 5216. -4.69 2.91e- 6 ## 13 TotRms_AbvGrd -9494. 1823. -5.21 2.09e- 7 ## 14 (Intercept) 212797. 33979. 6.26 4.55e- 10 ## 15 Year_Remod_Add 24376. 1115. 21.9 6.21e- 96 ## 16 Gr_Liv_Area 56853. 1822. 31.2 5.64e-177 ## 17 Age_House NA NA NA NA ## 18 Bsmt_Cond_Typical_x_TotRms_AbvGrd NA NA NA NA 6.4.2 Métricas de desempeño Ahora para calcular las métricas de desempeño usaremos la paquetería MLmetrics y las funciones de dplyr para resumir y estructurar los resultados. library(MLmetrics) p_test %&gt;% summarise( MAE = MLmetrics::MAE(.pred, Sale_Price), MAPE = MLmetrics::MAPE(.pred, Sale_Price), RMSE = MLmetrics::RMSE(.pred, Sale_Price), R2 = MLmetrics::R2_Score(.pred, Sale_Price) ) ## # A tibble: 1 × 4 ## MAE MAPE RMSE R2 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 34069. 0.202 49398. 0.624 6.4.3 Gráfica de ajuste library(patchwork) pred_obs_plot &lt;- p_test %&gt;% ggplot(aes(x = .pred, y = Sale_Price)) + geom_point(alpha = 0.2) + geom_abline(color = &quot;red&quot;) + xlab(&quot;Predicciones&quot;) + ylab(&quot;Observaciones&quot;) + ggtitle(&quot;Predicción vs Observación&quot;) error_line &lt;- p_test %&gt;% ggplot(aes(x = Sale_Price, y = error)) + geom_line() + geom_hline(yintercept = 0, color = &quot;red&quot;) + xlab(&quot;Observaciones&quot;) + ylab(&quot;Errores&quot;) + ggtitle(&quot;Varianza de errores&quot;) pred_obs_plot + error_line error_dist &lt;- p_test %&gt;% ggplot(aes(x = error)) + geom_histogram(color = &quot;white&quot;, fill = &quot;black&quot;) + geom_vline(xintercept = 0, color = &quot;red&quot;) + ylab(&quot;Conteos de clase&quot;) + xlab(&quot;Errores&quot;) + ggtitle(&quot;Distribución de error&quot;) error_qqplot &lt;- p_test %&gt;% ggplot(aes(sample = error)) + geom_qq(alpha = 0.3) + stat_qq_line(color = &quot;red&quot;) + xlab(&quot;Distribución normal&quot;) + ylab(&quot;Distribución de errores&quot;) + ggtitle(&quot;QQ-Plot&quot;) error_dist + error_qqplot 6.5 Métodos se selección de variables Una de las preguntas clave a responder es: ¿Cómo selecciono las variables a usar en un modelo?. Existen muchas técnicas para ello. Incluso, existen modelos que se encargan de realizar esta tarea de modo automático. Analizaremos diferentes técnicas a lo largo del curso. 6.5.1 Forward selection (selección hacia adelante) Comienza sin predictores en el modelo, agrega iterativamente los predictores más contribuyentes y se detiene cuando la mejora del modelo ya no es estadísticamente significativa. 6.5.2 Backward selection (selección hacia atrás) Comienza con todos los predictores en el modelo (modelo completo), y elimina iterativamente los predictores menos contribuyentes y se detiene cuando tiene un modelo en el que todos los predictores son estadísticamente significativos. "],["regresión-logística.html", "Capítulo 7 Regresión Logística 7.1 Función sigmoide 7.2 Ajuste del modelo 7.3 Clasificación 7.4 Métricas de desempeño 7.5 Implementación en R 7.6 Matriz de Confusión", " Capítulo 7 Regresión Logística El nombre de este modelo es: Regresión Bernoulli con liga logit, pero todo mundo la conoce solo por regresión logística. Es importante saber que la liga puede ser elegida dentro de un conjunto de ligas comunes, por lo que puede dejar de ser logit y seguir siendo regresión Bernoulli, pero ya no podría ser llamada “logística”. Al igual que en regresión lineal, existe la regresión simple y regresión múltiple. La regresión logística simple se utiliza una variable independiente, mientras que cuando se utiliza más de una variable independiente, el proceso se denomina regresión logística múltiple. Objetivo: Estimar la probabilidad de pertenecer a la categoría positiva de una variable de respuesta categórica. Posteriormente, se determina el umbral de probabilidad a partir del cual se clasifica a una observación como positiva o negativa. 7.1 Función sigmoide Si una variable cualitativa con dos categorías se codifica como 1 y 0, matemáticamente es posible ajustar un modelo de regresión lineal por mínimos cuadrados. El problema de esta aproximación es que, al tratarse de una recta, para valores extremos del predictor, se obtienen valores de \\(Y\\) menores que 0 o mayores que 1, lo que entra en contradicción con el hecho de que las probabilidades siempre están dentro del rango [0,1]. Para evitar estos problemas, la regresión logística transforma el valor devuelto por la regresión lineal empleando una función cuyo resultado está siempre comprendido entre 0 y 1. Existen varias funciones que cumplen esta descripción, una de las más utilizadas es la función logística (también conocida como función sigmoide): \\[\\sigma(Z)=\\frac{e^{Z}}{1+e^{Z}}\\] Función sigmoide: Para valores de \\(Z\\) muy grandes, el valor de \\(e^{Z}\\) tiende a infinito por lo que el valor de la función sigmoide es 1. Para valores de \\(Z\\) muy negativos, el valor \\(e^{Z}\\) tiende a cero, por lo que el valor de la función sigmoide es 0. Sustituyendo la \\(Z\\) de la función sigmoide por la función lineal \\(\\beta_0+\\beta_1X\\) se obtiene que: \\[\\pi=P(Y=k|X=x)=\\frac{e^{\\beta_0+\\beta_1X}}{1+e^{\\beta_0+\\beta_1X}}\\] donde \\(P(Y=k|X=x)\\) puede interpretarse como: la probabilidad de que la variable cualitativa \\(Y\\) adquiera el valor \\(k\\), dado que el predictor \\(X\\) tiene el valor \\(x\\). 7.2 Ajuste del modelo Esta función, puede ajustarse de forma sencilla con métodos de regresión lineal si se emplea su versión logarítmica: \\[logit(\\pi)= ln(\\frac{\\pi}{1-\\pi}) = ln(\\frac{p(Y=k|X=x)}{1−p(Y=k|X=x)})=\\beta_0+\\beta_1X\\] \\[P(Y=k|X=x)=\\frac{e^{\\beta_0+\\beta_1x_1+\\beta_2x_2+...+\\beta_ix_i}}{1+e^{\\beta_0+\\beta_1x_1+\\beta_2x_2+...+\\beta_ix_i}}\\] La combinación óptima de coeficientes \\(\\beta_0\\) y \\(\\beta_1\\) será aquella que tenga la máxima verosimilitud (maximum likelihood), es decir el valor de los parámetros \\(\\beta_0\\) y \\(\\beta_1\\) con los que se maximiza la probabilidad de obtener los datos observados. El método de maximum likelihood está ampliamente extendido en la estadística aunque su implementación no siempre es trivial. Otra forma para ajustar un modelo de regresión logística es empleando descenso de gradiente. Si bien este no es el método de optimización más adecuado para resolver la regresión logística, está muy extendido en el ámbito de machine learning para ajustar otros modelos. 7.3 Clasificación Una de las principales aplicaciones de un modelo de regresión logística es clasificar la variable cualitativa en función de valor que tome el predictor. Para conseguir esta clasificación, es necesario establecer un threshold de probabilidad a partir de la cual se considera que la variable pertenece a uno de los niveles. Por ejemplo, se puede asignar una observación al grupo 1 si \\(p̂ (Y=1|X)&gt;0.3\\) y al grupo 0 si ocurre lo contrario. Es importante mencionar que el punto de corte no necesariamente tiene que ser 0.5, este puede ser seleccionado a conveniencia de la métrica a optimizar. 7.4 Métricas de desempeño Existen distintas métricas de desempeño para problemas de clasificación, debido a que contamos con la respuesta correcta podemos contar cuántos aciertos tuvimos y cuántos fallos tuvimos. Primero, por simplicidad ocuparemos un ejemplo de clasificación binaria, Cancelación (1) o No Cancelación (0). En este tipo de algoritmos definimos cuál de las categorías será nuestra etiqueta positiva y cuál será la negativa. La positiva será la categoría que queremos predecir -en nuestro ejemplo, Cancelación- y la negativa lo opuesto -en el caso binario- en nuestro ejemplo, no cancelación. Dadas estas definiciones tenemos 4 posibilidades: True positives: Nuestra predicción dijo que la transacción es fraude y la etiqueta real dice que es fraude. False positives: Nuestra predicción dijo que la transacción es fraude y la etiqueta real dice que no es fraude. True negatives: Nuestra predicción dijo que la transacción es no fraude y la etiqueta real dice que no es fraude. False negatives: Nuestra predicción dijo que la transacción es no fraude y la etiqueta real dice que es fraude. Matriz de confusión Esta métrica corresponde a una matriz en donde se plasma el conteo de los aciertos y los errores que haya hecho el modelo, esto es: los verdaderos positivos (TP), los verdaderos negativos (TN), los falsos positivos (FP) y los falsos negativos (FN). Normalmente los renglones representan las etiquetas predichas, ya sean positivas o negativas, y las columnas a las etiquetas reales, aunque esto puede cambiar en cualquier software. Accuracy Número de aciertos totales entre todas las predicciones. \\[accuracy = \\frac{TP + TN}{ TP+FP+TN+FN}\\] La métrica más utilizada, en conjuntos de datos no balanceados esta métrica no nos sirve, al contrario, nos engaña. Adicionalmente, cuando la identificación de una categoría es más importante que otra es mejor recurrir a otras métricas. Precision: Eficiencia De los que identificamos como clase positiva, cuántos identificamos correctamente. ¿Qué tan eficientes somos en la predicción? \\[precision = \\frac{TP}{TP + FP}\\] ¿Cuándo utilizar precisión? Esta es la métrica que ocuparás más, pues en un contexto de negocio, donde los recursos son finitos y tiene un costo asociado, ya sea monetario o de tiempo o de recursos, necesitarás que las predicciones de tu etiqueta positiva sean muy eficientes. Al utilizar esta métrica estaremos optimizando el modelo para minimizar el número de falsos positivos. Recall o Sensibilidad: Cobertura Del universo posible de nuestra clase positiva, cuántos identificamos correctamente. \\[recall = \\frac{TP}{TP + FN }\\] Esta métrica la ocuparás cuando en el contexto de negocio de tu problema sea más conveniente aumentar la cantidad de positivos o disminuir los falsos negativos. Esto se realiza debido al impacto que estos pueden tener en las personas en quienes se implementará la predicción. Especificidad Es el número de observaciones correctamente identificados como negativos fuera del total de negativos. \\[Specificity = \\frac{TN}{TN+FP}\\] F1-score Combina precision y recall para optimizar ambos. \\[F = 2 *\\frac{precision * recall}{precision + recall} \\] Se recomienda utilizar esta métrica de desempeño cuando quieres balancear tanto los falsos positivos como los falsos negativos. Aunque es una buena solución para tomar en cuenta ambos errores, pocas veces hay problemas reales que permiten ocuparla, esto es porque en más del 90% de los casos tenemos una restricción en recursos. Ahora con esto en mente podemos definir las siguientes métricas: AUC y ROC: Area Under the Curve y Receiver operator characteristic Una curva ROC es un gráfico que muestra el desempeño de un modelo de clasificación en todos los puntos de corte. AUC significa “Área bajo la curva ROC”. Es decir, AUC mide el área debajo de la curva ROC. 7.5 Implementación en R Ajustaremos un modelo de regresión logística usando la receta antes vista. Lectura de datos library(tidymodels) library(readr) telco &lt;- read_csv(&quot;data/Churn.csv&quot;) Segmentación de datos set.seed(1234) telco_split &lt;- initial_split(telco, prop = .7) telco_train &lt;- training(telco_split) telco_test &lt;- testing(telco_split) Pre-procesamiento de datos telco_rec &lt;- recipe(Churn ~ ., data = telco_train) %&gt;% update_role(customerID, new_role = &quot;id variable&quot;) %&gt;% step_normalize(all_numeric_predictors()) %&gt;% step_dummy(all_nominal_predictors()) %&gt;% step_impute_median(all_numeric_predictors()) %&gt;% step_rm(customerID, skip=T) %&gt;% prep() telco_juiced &lt;- juice(telco_rec) glimpse(telco_juiced) ## Rows: 4,930 ## Columns: 31 ## $ SeniorCitizen &lt;dbl&gt; -0.4417148, -0.4417148, -0.44171… ## $ tenure &lt;dbl&gt; 0.1915835, 0.3140505, -0.4207516… ## $ MonthlyCharges &lt;dbl&gt; -1.50429475, 0.66367718, -0.5084… ## $ TotalCharges &lt;dbl&gt; -0.66321610, 0.47214217, -0.5462… ## $ Churn &lt;fct&gt; No, No, No, Yes, Yes, No, No, No… ## $ gender_Male &lt;dbl&gt; 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,… ## $ Partner_Yes &lt;dbl&gt; 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1,… ## $ Dependents_Yes &lt;dbl&gt; 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,… ## $ PhoneService_Yes &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,… ## $ MultipleLines_No.phone.service &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,… ## $ MultipleLines_Yes &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,… ## $ InternetService_Fiber.optic &lt;dbl&gt; 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,… ## $ InternetService_No &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ OnlineSecurity_No.internet.service &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ OnlineSecurity_Yes &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0,… ## $ OnlineBackup_No.internet.service &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ OnlineBackup_Yes &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,… ## $ DeviceProtection_No.internet.service &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ DeviceProtection_Yes &lt;dbl&gt; 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0,… ## $ TechSupport_No.internet.service &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ TechSupport_Yes &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,… ## $ StreamingTV_No.internet.service &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ StreamingTV_Yes &lt;dbl&gt; 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,… ## $ StreamingMovies_No.internet.service &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ StreamingMovies_Yes &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,… ## $ Contract_One.year &lt;dbl&gt; 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,… ## $ Contract_Two.year &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,… ## $ PaperlessBilling_Yes &lt;dbl&gt; 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,… ## $ PaymentMethod_Credit.card..automatic. &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,… ## $ PaymentMethod_Electronic.check &lt;dbl&gt; 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,… ## $ PaymentMethod_Mailed.check &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… telco_test_bake &lt;- bake(telco_rec, new_data = telco_test) glimpse(telco_test_bake) ## Rows: 2,113 ## Columns: 32 ## $ customerID &lt;chr&gt; &quot;5575-GNVDE&quot;, &quot;9305-CDSKC&quot;, &quot;671… ## $ SeniorCitizen &lt;dbl&gt; -0.4417148, -0.4417148, -0.44171… ## $ tenure &lt;dbl&gt; 0.06911644, -0.99226439, -0.9106… ## $ MonthlyCharges &lt;dbl&gt; -0.27067882, 1.14914329, -1.1751… ## $ TotalCharges &lt;dbl&gt; -0.1752116, -0.6472105, -0.87618… ## $ Churn &lt;fct&gt; No, Yes, No, No, No, No, No, No,… ## $ gender_Male &lt;dbl&gt; 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,… ## $ Partner_Yes &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1,… ## $ Dependents_Yes &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0,… ## $ PhoneService_Yes &lt;dbl&gt; 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1,… ## $ MultipleLines_No.phone.service &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,… ## $ MultipleLines_Yes &lt;dbl&gt; 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,… ## $ InternetService_Fiber.optic &lt;dbl&gt; 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1,… ## $ InternetService_No &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,… ## $ OnlineSecurity_No.internet.service &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,… ## $ OnlineSecurity_Yes &lt;dbl&gt; 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,… ## $ OnlineBackup_No.internet.service &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,… ## $ OnlineBackup_Yes &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1,… ## $ DeviceProtection_No.internet.service &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,… ## $ DeviceProtection_Yes &lt;dbl&gt; 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,… ## $ TechSupport_No.internet.service &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,… ## $ TechSupport_Yes &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,… ## $ StreamingTV_No.internet.service &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,… ## $ StreamingTV_Yes &lt;dbl&gt; 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,… ## $ StreamingMovies_No.internet.service &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,… ## $ StreamingMovies_Yes &lt;dbl&gt; 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,… ## $ Contract_One.year &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,… ## $ Contract_Two.year &lt;dbl&gt; 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,… ## $ PaperlessBilling_Yes &lt;dbl&gt; 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,… ## $ PaymentMethod_Credit.card..automatic. &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1,… ## $ PaymentMethod_Electronic.check &lt;dbl&gt; 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,… ## $ PaymentMethod_Mailed.check &lt;dbl&gt; 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,… Creación y ajuste de modelo logistic_model &lt;- logistic_reg() %&gt;% set_engine(&quot;glm&quot;) logistic_fit1 &lt;- parsnip::fit(logistic_model, Churn ~ ., telco_juiced) Predicción de respuesta con nuevos datos logistic_p_test &lt;- predict(logistic_fit1, telco_test_bake) %&gt;% bind_cols(telco_test_bake) %&gt;% select(.pred_class, Churn) logistic_p_test ## # A tibble: 2,113 × 2 ## .pred_class Churn ## &lt;fct&gt; &lt;fct&gt; ## 1 No No ## 2 Yes Yes ## 3 No No ## 4 No No ## 5 No No ## 6 Yes No ## 7 No No ## 8 No No ## 9 Yes Yes ## 10 No No ## # … with 2,103 more rows 7.6 Matriz de Confusión logistic_p_test %&gt;% yardstick::conf_mat(truth = Churn, estimate = .pred_class) %&gt;% autoplot(type = &quot;heatmap&quot;) bind_rows( yardstick::accuracy(logistic_p_test, Churn, .pred_class, event_level = &quot;second&quot;), yardstick::precision(logistic_p_test, Churn, .pred_class, event_level = &quot;second&quot;), yardstick::recall(logistic_p_test, Churn, .pred_class, event_level = &quot;second&quot;), yardstick::specificity(logistic_p_test, Churn, .pred_class, event_level = &quot;second&quot;), yardstick::f_meas(logistic_p_test, Churn, .pred_class, event_level = &quot;second&quot;) ) ## # A tibble: 5 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 accuracy binary 0.808 ## 2 precision binary 0.678 ## 3 recall binary 0.542 ## 4 specificity binary 0.906 ## 5 f_meas binary 0.603 ¿Y si se quiere un corte diferente? ¿el negocio qué necesita? logistic_p_test_prob &lt;- predict(logistic_fit1, telco_test_bake, type = &quot;prob&quot;) %&gt;% bind_cols(telco_test_bake) %&gt;% select(.pred_Yes, .pred_No, Churn) logistic_p_test_prob ## # A tibble: 2,113 × 3 ## .pred_Yes .pred_No Churn ## &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 0.0382 0.962 No ## 2 0.769 0.231 Yes ## 3 0.309 0.691 No ## 4 0.0227 0.977 No ## 5 0.0372 0.963 No ## 6 0.551 0.449 No ## 7 0.0635 0.937 No ## 8 0.0270 0.973 No ## 9 0.520 0.480 Yes ## 10 0.0136 0.986 No ## # … with 2,103 more rows logistic_p_test_prob &lt;- logistic_p_test_prob %&gt;% mutate(.pred_class = as.factor(if_else ( .pred_Yes &gt;= 0.30, &#39;Yes&#39;, &#39;No&#39;))) %&gt;% relocate(.pred_class , .after = .pred_No) logistic_p_test_prob ## # A tibble: 2,113 × 4 ## .pred_Yes .pred_No .pred_class Churn ## &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; ## 1 0.0382 0.962 No No ## 2 0.769 0.231 Yes Yes ## 3 0.309 0.691 Yes No ## 4 0.0227 0.977 No No ## 5 0.0372 0.963 No No ## 6 0.551 0.449 Yes No ## 7 0.0635 0.937 No No ## 8 0.0270 0.973 No No ## 9 0.520 0.480 Yes Yes ## 10 0.0136 0.986 No No ## # … with 2,103 more rows bind_rows( yardstick::accuracy(logistic_p_test_prob, Churn, .pred_class, event_level = &quot;second&quot;), yardstick::precision(logistic_p_test_prob, Churn, .pred_class, event_level = &quot;second&quot;), yardstick::recall(logistic_p_test_prob, Churn, .pred_class, event_level = &quot;second&quot;), yardstick::specificity(logistic_p_test_prob, Churn, .pred_class, event_level = &quot;second&quot;), yardstick::f_meas(logistic_p_test_prob, Churn, .pred_class, event_level = &quot;second&quot;) ) ## # A tibble: 5 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 accuracy binary 0.769 ## 2 precision binary 0.548 ## 3 recall binary 0.777 ## 4 specificity binary 0.765 ## 5 f_meas binary 0.643 Para poder determinar cual es el mejor punto de corte, es indispensable conocer el comportamiento y efecto de los diferentes puntos de corte. Veamos un ejemplo visual en nuestra aplicación de Shiny: ConfusionMatrixShiny "],["k-nearest-neighbor.html", "Capítulo 8 K-Nearest-Neighbor 8.1 Clasificación 8.2 Regresión 8.3 Ajuste del modelo 8.4 Implementación en R", " Capítulo 8 K-Nearest-Neighbor KNN es un algoritmo de aprendizaje supervisado que podemos usar tanto para regresión como clasificación. Es un algoritmo fácil de interpretar y que permite ser flexible en el balance entre sesgo y varianza (dependiendo de los hiper-parámetros seleccionados). El algoritmo de K vecinos más cercanos realiza comparaciones entre un nuevo elemento y las observaciones anteriores que ya cuentan con etiqueta. La esencia de este algoritmo está en etiquetar a un nuevo elemento de manera similar a como están etiquetados aquellos K elementos que más se le parecen. Veremos este proceso para cada uno de los posibles casos: 8.1 Clasificación La idea detrás del algoritmo es sencilla, etiqueta una nueva observación en la categoría que tenga mas elementos de las k observaciones más cercanas, es decir: Seleccionamos el hiper-parámetro K como el número elegido de vecinos. Se calculará la similitud (distancia) de esta nueva observación a cada observación existente. Ordenaremos estas distancias de menor a mayor. Tomamos las K primeras entradas de la lista ordenada. La nueva observación será asignada al grupo que tenga mayor número de observaciones en estas k primeras distancias (asignación por moda) A continuación se ejemplifica este proceso: Ejemplo: Otro método que permite tener mayor control sobre las clasificaciones es asignar la probabilidad de pertenencia a cada clase de acuerdo con la proporción existente de cada una de las mismas. A partir de dichas probabilidades, el usuario puede determinar el punto de corte que sea más conveniente para el problema a resolver. 8.2 Regresión En el caso de regresión, la etiqueta de una nueva observación se realiza a través del promedio del valor en las k observaciones más cercanas, es decir: Seleccionamos el hiper-parámetro K como el número elegido de vecinos. Se calculará la similitud (distancia) de esta nueva observación a cada observación existente Ordenaremos estas distancias de menor a mayor Tomamos las K primeras entradas de la lista ordenada. La nueva observación será etiquetada mediante el promedio del valor de las observaciones en estas k primeras distancias. Considerando un modelo de 3 vecinos más cercanos, las siguientes imágenes muestran el proceso de ajuste y predicción de nuevas observaciones. Ejemplo de balance de sesgo y varianza 8.3 Ajuste del modelo En contraste con otros algoritmos de aprendizaje supervisado, K-NN no genera un modelo del aprendizaje con datos de entrenamiento, sino que el aprendizaje sucede en el mismo momento en el que se prueban los datos de prueba. A este tipo de algoritmos se les llama lazy learning methods porque no aprende del conjunto de entrenamiento inmediatamente, sino que almacena el conjunto de datos y, en el momento de la clasificación, realiza una acción en el conjunto de datos. El algoritmo KNN en la fase de entrenamiento simplemente almacena el conjunto de datos y cuando obtiene nuevos datos, clasifica esos datos en una categoría que es muy similar a los nuevos datos. 8.3.1 Selección de Hiper-parámetro K Al configurar un modelo KNN, sólo hay algunos parámetros que deben elegirse/ajustarse para mejorar el rendimiento, uno de estos parámetros es el valor de la K. No existe una forma particular de determinar el mejor valor para “K”, por lo que debemos probar algunos valores para encontrar “el mejor” de ellos. Para los modelos de clasificación, especialmente si solo hay dos clases, generalmente se elige un número impar para k. Esto es para que el algoritmo nunca llegue a un “empate” Una opción para seleccionar la K adecuada es ejecutar el algoritmo KNN varias veces con diferentes valores de K y elegimos la K que reduce la cantidad de errores mientras se mantiene la capacidad del algoritmo para hacer predicciones con precisión. Observemos lo siguiente: Estas gráficas se conoce como “gráfica de codo” y generalmente se usan para determinar el valor K. A medida que disminuimos el valor de K a 1, nuestras predicciones se vuelven menos estables. Imaginemos que tomamos K = 1 y tenemos un punto de consulta rodeado por varios rojos y uno verde, pero el verde es el vecino más cercano. Razonablemente, pensaríamos que el punto de consulta es probablemente rojo, pero como K = 1, KNN predice incorrectamente que el punto de consulta es verde. Inversamente, a medida que aumentamos el valor de K, nuestras predicciones se vuelven más estables debido a que tenemos más observaciones con quienes comparar, por lo tanto, es más probable que hagan predicciones más precisas. Eventualmente, comenzamos a presenciar un número creciente de errores, es en este punto que sabemos que hemos llevado el valor de K demasiado lejos. 8.3.2 Métodos de cálculo de la distancia entre observaciones Otro parámetro que podemos ajustar para el modelo es la distancia usada, existen diferentes formas de medir qué tan “cerca” están dos puntos entre sí, y las diferencias entre estos métodos pueden volverse significativas en dimensiones superiores. La más utilizada es la distancia euclidiana, el tipo estándar de distancia. \\[d(X,Y) = \\sqrt{\\sum_{i=1}^{n} (x_i-y_i)^2}\\] Otra métrica es la llamada distancia de Manhattan, que mide la distancia tomada en cada dirección cardinal, en lugar de a lo largo de la diagonal. \\[d(X,Y) = \\sum_{i=1}^{n} |x_i - y_i|\\] De manera más general, las anteriores son casos particulares de la distancia de Minkowski, cuya fórmula es: \\[d(X,Y) = (\\sum_{i=1}^{n} |x_i-y_i|^p)^{\\frac{1}{p}}\\] La distancia de coseno es ampliamente en análisis de texto, sistemas de recomendación \\[d(X,Y)= 1 - \\frac{\\sum_{i=1}^{n}{X_iY_i}}{\\sqrt{\\sum_{i=1}^{n}{X_i^2}}\\sqrt{\\sum_{i=1}^{n}{Y_i^2}}}\\] Un link interesante Otro link interesante 8.4 Implementación en R Usaremos las recetas antes implementadas para ajustar tanto el modelo de regresión como el de clasificación. Exploraremos un conjunto de hiperparámetros para elegir el mejor modelo. Para determinar cuáles son los hiper-parámetros que funcionan mejor, es necesario realizar experimentos mediante ensayo-error hasta determinar la mejor solución. En cada partición del método de muestreo KFCV se implementan las distintas configuraciones y se calculan predicciones. Con las predicciones hechas en cada fold, se obtienen intervalos de confianza para conocer la variación asociada al modelo a través de los hiper-parámetros implementados. Usaremos las recetas antes implementadas para ajustar tanto el modelo de regresión como el de clasificación. Exploraremos un conjunto de hiperparámetros para elegir el mejor modelo, sin embargo, para realizar este proceso de forma ágil, se inicializará un flujo de trabajo que se encargue de realizar todos los experimentos deseados y elegir el modelo adecuado. Los pasos a seguir, son los siguientes: Separación inicial de datos (test, train, KFCV). Pre-procesamiento e ingeniería de variables. Selección de tipo de modelo con hiperparámetros iniciales. Inicialización de workflow o pipeline. Creación de grid search. Entrenamiento de modelos con hiperparámetros definidos (salvar los modelos entrenados). Análisis de métricas de error e hiperparámetros (Vuelve al paso 3, si es necesario). Selección de modelo a usar. Ajuste de modelo final con todos los datos (Vuelve al paso 2, si es necesario). Validar poder predictivo con datos de prueba. 8.4.1 Regresión Paso 1: Separación inicial de datos ( test, train ) library(tidymodels) data(ames) set.seed(4595) ames_split &lt;- initial_split(ames, prop = 0.75) ames_train &lt;- training(ames_split) ames_test &lt;- testing(ames_split) ames_folds &lt;- vfold_cv(ames_train) Contando con datos de entrenamiento, procedemos a realizar el feature engineering para extraer las mejores características que permitirán realizar las estimaciones en el modelo. Paso 2: Pre-procesamiento e ingeniería de variables receta_casas &lt;- recipe( Sale_Price ~ Gr_Liv_Area + TotRms_AbvGrd + Exter_Cond + Bsmt_Cond + Year_Sold + Year_Remod_Add, data = ames_train) %&gt;% step_mutate( Age_House = Year_Sold - Year_Remod_Add, Exter_Cond = forcats::fct_collapse(Exter_Cond, Good = c(&quot;Typical&quot;, &quot;Good&quot;, &quot;Excellent&quot;))) %&gt;% step_relevel(Exter_Cond, ref_level = &quot;Good&quot;) %&gt;% step_normalize(all_numeric_predictors()) %&gt;% step_dummy(all_nominal_predictors()) %&gt;% step_interact(~ matches(&quot;Bsmt_Cond&quot;):TotRms_AbvGrd) %&gt;% prep() receta_casas ## Recipe ## ## Inputs: ## ## role #variables ## outcome 1 ## predictor 6 ## ## Training data contained 2197 data points and no missing data. ## ## Operations: ## ## Variable mutation for ~Year_Sold - Year_Remod_Add, ~forcats::fct... [trained] ## Re-order factor level to ref_level for Exter_Cond [trained] ## Centering and scaling for Gr_Liv_Area, TotRms_AbvGrd, Year_Sold, Year_Rem... [trained] ## Dummy variables from Exter_Cond, Bsmt_Cond [trained] ## Interactions with (Bsmt_Cond_Fair + Bsmt_Cond_Good + Bsmt_Cond_No_Ba... [trained] Recordemos que la función recipe() solo son los pasos a seguir, necesitamos usar la función prep() que nos devuelve una receta actualizada con las estimaciones y la función juice() que nos devuelve la matriz de diseño. Una vez que la receta de transformación de datos está lista, procedemos a implementar el pipeline del modelo de interés. Paso 3: Selección de tipo de modelo con hiperparámetros iniciales # install.packages(&quot;kknn&quot;) knn_model &lt;- nearest_neighbor( mode = &quot;regression&quot;, neighbors = tune(&quot;K&quot;), weight_func = tune()) %&gt;% set_engine(&quot;kknn&quot;) Paso 4: Inicialización de workflow o pipeline knn_workflow &lt;- workflow() %&gt;% add_recipe(receta_casas) %&gt;% add_model(knn_model) Paso 5: Creación de grid search knn_parameters_set &lt;- extract_parameter_set_dials(knn_workflow) %&gt;% update( K = dials::neighbors(c(10,80)), weight_func = weight_func(values = c(&quot;rectangular&quot;, &quot;triangular&quot;, &quot;inv&quot;, &quot;gaussian&quot;, &quot;cos&quot;)) ) set.seed(123) knn_grid &lt;- knn_parameters_set %&gt;% grid_max_entropy(size = 50) ctrl_grid &lt;- control_grid(save_pred = T, verbose = T) Paso 6: Entrenamiento de modelos con hiperparámetros definidos library(doParallel) UseCores &lt;- detectCores() - 1 cluster &lt;- makeCluster(UseCores) registerDoParallel(cluster) knnt1 &lt;- Sys.time() knn_tune_result &lt;- tune_grid( knn_workflow, resamples = ames_folds, grid = knn_grid, metrics = metric_set(rmse, mae, mape, rsq), control = ctrl_grid ) knnt2 &lt;- Sys.time(); knnt2 - knnt1 stopCluster(cluster) knn_tune_result %&gt;% saveRDS(&quot;models/knn_model_reg.rds&quot;) Podemos obtener las métricas de cada fold con el siguiente código: knn_tune_result &lt;- readRDS(&quot;models/knn_model_reg.rds&quot;) Paso 7: Análisis de métricas de error e hiperparámetros (Vuelve al paso 3, si es necesario) collect_metrics(knn_tune_result) ## # A tibble: 196 × 8 ## K weight_func .metric .estimator mean n std_err .config ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 11 cos mae standard 31367. 10 642. Preprocessor1… ## 2 11 cos mape standard 18.7 10 0.439 Preprocessor1… ## 3 11 cos rmse standard 46086. 10 1104. Preprocessor1… ## 4 11 cos rsq standard 0.669 10 0.0151 Preprocessor1… ## 5 30 cos mae standard 31355. 10 649. Preprocessor1… ## 6 30 cos mape standard 18.8 10 0.427 Preprocessor1… ## 7 30 cos rmse standard 46534. 10 1242. Preprocessor1… ## 8 30 cos rsq standard 0.665 10 0.0134 Preprocessor1… ## 9 38 cos mae standard 31528. 10 673. Preprocessor1… ## 10 38 cos mape standard 18.9 10 0.427 Preprocessor1… ## # … with 186 more rows En la siguiente gráfica observamos las distintas métricas de error asociados a los hiperparámetros elegidos: knn_tune_result %&gt;% autoplot() En la siguiente gráfica observamos el error cuadrático medio de las distintas métricas con distintos números de vecinos. En los argumentos de la función, se puede seleccionar el kernel, esto es las opciones posibles para ponderar el promedio respecto a la distancia seleccionada. “Rectangular” (que es knn estándar no ponderado), “triangular”, “cos”, “inv”, “gaussiano”, “rango” y “óptimo”. Para conocer más a cerca de las distintas métricas de distancia pueden consultar: Measures y KNN function knn_tune_result %&gt;% autoplot(metric = &quot;rmse&quot;) En la siguiente gráfica observamos el error absoluto promedio de las distintas métricas con distintos números de vecinos. knn_tune_result %&gt;% autoplot(metric = &quot;mae&quot;) Paso 8: Selección de modelo a usar Con el siguiente código obtenemos los mejores 10 modelos respecto al rmse. show_best(knn_tune_result, n = 10, metric = &quot;rmse&quot;) ## # A tibble: 10 × 8 ## K weight_func .metric .estimator mean n std_err .config ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 13 inv rmse standard 45318. 10 1070. Preprocessor1_Mode… ## 2 17 gaussian rmse standard 45678. 10 1234. Preprocessor1_Mode… ## 3 20 triangular rmse standard 45985. 10 1124. Preprocessor1_Mode… ## 4 11 cos rmse standard 46086. 10 1104. Preprocessor1_Mode… ## 5 31 gaussian rmse standard 46086. 10 1340. Preprocessor1_Mode… ## 6 25 triangular rmse standard 46165. 10 1163. Preprocessor1_Mode… ## 7 42 inv rmse standard 46296. 10 1420. Preprocessor1_Mode… ## 8 39 gaussian rmse standard 46307. 10 1386. Preprocessor1_Mode… ## 9 35 triangular rmse standard 46472. 10 1258. Preprocessor1_Mode… ## 10 47 gaussian rmse standard 46531. 10 1404. Preprocessor1_Mode… knn_tune_result %&gt;% show_best(n = 10, metric = &quot;mape&quot;) ## # A tibble: 10 × 8 ## K weight_func .metric .estimator mean n std_err .config ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 13 inv mape standard 18.2 10 0.339 Preprocessor1_Model… ## 2 17 gaussian mape standard 18.4 10 0.379 Preprocessor1_Model… ## 3 20 triangular mape standard 18.6 10 0.428 Preprocessor1_Model… ## 4 42 inv mape standard 18.6 10 0.419 Preprocessor1_Model… ## 5 25 triangular mape standard 18.6 10 0.431 Preprocessor1_Model… ## 6 31 gaussian mape standard 18.6 10 0.395 Preprocessor1_Model… ## 7 48 inv mape standard 18.7 10 0.410 Preprocessor1_Model… ## 8 11 cos mape standard 18.7 10 0.439 Preprocessor1_Model… ## 9 35 triangular mape standard 18.7 10 0.421 Preprocessor1_Model… ## 10 39 gaussian mape standard 18.7 10 0.408 Preprocessor1_Model… Ahora obtendremos el modelo que mejor desempeño tiene tomando en cuenta el rmse y haremos las predicciones del conjunto de prueba con este modelo. best_knn_model_reg &lt;- knn_tune_result %&gt;% select_best(metric = &quot;rmse&quot;) best_knn_model_reg ## # A tibble: 1 × 3 ## K weight_func .config ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 13 inv Preprocessor1_Model20 knn_regression_best_1se_model &lt;- knn_tune_result %&gt;% select_by_one_std_err(metric = &quot;mape&quot;, &quot;mape&quot;) knn_regression_best_1se_model ## # A tibble: 1 × 10 ## K weight_func .metric .estimator mean n std_err .config .best .bound ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 17 gaussian mape standard 18.4 10 0.379 Preproc… 18.2 18.5 Paso 9: Ajuste de modelo final con todos los datos (Vuelve al paso 2, si es necesario) final_knn_model_reg &lt;- knn_workflow %&gt;% finalize_workflow(best_knn_model_reg) %&gt;% parsnip::fit(data = ames_train) Este último objeto es el modelo final entrenado, el cual contiene toda la información del pre-procesamiento de datos, por lo que en caso de ponerse en producción el modelo, sólo se necesita de este último elemento para poder realizar nuevas predicciones. Antes de pasar al siguiente paso, es importante validar que hayamos hecho un uso correcto de las variables predictivas. En este momento es posible detectar variables que no estén aportando valor o variables que no debiéramos estar usando debido a que cometeríamos data leakage. Para enfrentar esto, ayuda estimar y ordenar el valor de importancia de cada variable en el modelo. # install.packages(&quot;kernlab&quot;) library(vip) ames_importance &lt;- final_knn_model_reg %&gt;% extract_fit_parsnip() %&gt;% vi( method = &quot;permute&quot;, nsim = 30, target = &quot;Sale_Price&quot;, metric = &quot;rmse&quot;, pred_wrapper = kernlab::predict, train = juice(receta_casas) ) ames_importance %&gt;% saveRDS(&quot;models/vip_ames_knn.rds&quot;) ames_importance &lt;- readRDS(&quot;models/vip_ames_knn.rds&quot;) ames_importance ## # A tibble: 17 × 3 ## Variable Importance StDev ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Gr_Liv_Area 70988. 951. ## 2 Year_Remod_Add 45452. 778. ## 3 Age_House 45334. 912. ## 4 Bsmt_Cond_Typical_x_TotRms_AbvGrd 39802. 392. ## 5 TotRms_AbvGrd 39382. 557. ## 6 Year_Sold 37657. 795. ## 7 Bsmt_Cond_Typical 17401. 863. ## 8 Bsmt_Cond_No_Basement 15895. 1750. ## 9 Bsmt_Cond_Good_x_TotRms_AbvGrd 15087. 722. ## 10 Bsmt_Cond_Fair 13961. 1478. ## 11 Exter_Cond_Fair 13681. 1726. ## 12 Bsmt_Cond_Good 13494. 917. ## 13 Bsmt_Cond_Fair_x_TotRms_AbvGrd 9742. 1372. ## 14 Bsmt_Cond_No_Basement_x_TotRms_AbvGrd 8821. 1676. ## 15 Exter_Cond_Poor 2828. 1090. ## 16 Bsmt_Cond_Poor_x_TotRms_AbvGrd 1424. 346. ## 17 Bsmt_Cond_Poor 1215. 364. ames_importance %&gt;% mutate(Variable = forcats::fct_reorder(Variable, Importance)) %&gt;% slice_max(Importance, n = 20) %&gt;% ggplot(aes(Importance, Variable, color = Variable)) + geom_errorbar(aes(xmin = Importance - StDev, xmax = Importance + StDev), alpha = 0.5, size = 1) + geom_point(size = 2) + theme(legend.position = &quot;none&quot;) + ggtitle(&quot;Variable Importance Measure&quot;) Paso 10: Validar poder predictivo con datos de prueba Imaginemos por un momento que pasa un mes de tiempo desde que hicimos nuestro modelo, es hora de ponerlo a prueba prediciendo valores de nuevos elementos: results_reg &lt;- predict(final_knn_model_reg, ames_test) %&gt;% dplyr::bind_cols(Sale_Price = ames_test$Sale_Price, .) %&gt;% dplyr::rename(pred_knn_reg = .pred) results_reg ## # A tibble: 733 × 2 ## Sale_Price pred_knn_reg ## &lt;int&gt; &lt;dbl&gt; ## 1 105000 117872. ## 2 185000 183378. ## 3 180400 184216. ## 4 141000 92359. ## 5 210000 222663. ## 6 216000 201289. ## 7 149900 170697. ## 8 105500 151156. ## 9 88000 151156. ## 10 146000 166595. ## # … with 723 more rows Métricas de desempeño Ahora para calcular las métricas de desempeño usaremos la paquetería MLmetrics. Es posible definir nuestro propio conjunto de métricas que deseamos reportar creando el objeto metric_set: library(MLmetrics) multi_metric &lt;- metric_set(mae, mape, rmse, rsq, ccc) multi_metric(results_reg, truth = Sale_Price, estimate = pred_knn_reg) %&gt;% mutate(.estimate = round(.estimate, 2)) %&gt;% select(-.estimator) ## # A tibble: 5 × 2 ## .metric .estimate ## &lt;chr&gt; &lt;dbl&gt; ## 1 mae 31870. ## 2 mape 18.6 ## 3 rmse 48366. ## 4 rsq 0.64 ## 5 ccc 0.78 results_reg %&gt;% ggplot(aes(x = pred_knn_reg, y = Sale_Price)) + geom_point() + geom_abline(color = &quot;red&quot;) + xlab(&quot;Prediction&quot;) + ylab(&quot;Observation&quot;) + ggtitle(&quot;Comparisson&quot;) 8.4.2 Clasificación Es turno de revisar la implementación de SVM con nuestro bien conocido problema de predicción de cancelación de servicios de telecomunicaciones. Los datos se encuentran disponibles en el siguiente enlace: Los pasos para implementar en R este modelo predictivo son los mismos, cambiando únicamente las especificaciones del tipo de modelo, pre-procesamiento e hiper-parámetros. library(readr) library(tidyverse) library(tidymodels) tidymodels_prefer() telco &lt;- read_csv(&quot;data/Churn.csv&quot;) glimpse(telco) ## Rows: 7,043 ## Columns: 21 ## $ customerID &lt;chr&gt; &quot;7590-VHVEG&quot;, &quot;5575-GNVDE&quot;, &quot;3668-QPYBK&quot;, &quot;7795-CFOCW… ## $ gender &lt;chr&gt; &quot;Female&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Female&quot;, &quot;Female&quot;,… ## $ SeniorCitizen &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ Partner &lt;chr&gt; &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes… ## $ Dependents &lt;chr&gt; &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;… ## $ tenure &lt;dbl&gt; 1, 34, 2, 45, 2, 8, 22, 10, 28, 62, 13, 16, 58, 49, 2… ## $ PhoneService &lt;chr&gt; &quot;No&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;No&quot;, … ## $ MultipleLines &lt;chr&gt; &quot;No phone service&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No phone service&quot;, &quot;… ## $ InternetService &lt;chr&gt; &quot;DSL&quot;, &quot;DSL&quot;, &quot;DSL&quot;, &quot;DSL&quot;, &quot;Fiber optic&quot;, &quot;Fiber opt… ## $ OnlineSecurity &lt;chr&gt; &quot;No&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;… ## $ OnlineBackup &lt;chr&gt; &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;N… ## $ DeviceProtection &lt;chr&gt; &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Y… ## $ TechSupport &lt;chr&gt; &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes… ## $ StreamingTV &lt;chr&gt; &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;Ye… ## $ StreamingMovies &lt;chr&gt; &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes… ## $ Contract &lt;chr&gt; &quot;Month-to-month&quot;, &quot;One year&quot;, &quot;Month-to-month&quot;, &quot;One … ## $ PaperlessBilling &lt;chr&gt; &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;No&quot;, … ## $ PaymentMethod &lt;chr&gt; &quot;Electronic check&quot;, &quot;Mailed check&quot;, &quot;Mailed check&quot;, &quot;… ## $ MonthlyCharges &lt;dbl&gt; 29.85, 56.95, 53.85, 42.30, 70.70, 99.65, 89.10, 29.7… ## $ TotalCharges &lt;dbl&gt; 29.85, 1889.50, 108.15, 1840.75, 151.65, 820.50, 1949… ## $ Churn &lt;chr&gt; &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Y… Paso 1: Separación inicial de datos ( test, train ) set.seed(1234) telco_split &lt;- initial_split(telco, prop = .70) telco_train &lt;- training(telco_split) telco_test &lt;- testing(telco_split) telco_folds &lt;- vfold_cv(telco_train) telco_folds ## # 10-fold cross-validation ## # A tibble: 10 × 2 ## splits id ## &lt;list&gt; &lt;chr&gt; ## 1 &lt;split [4437/493]&gt; Fold01 ## 2 &lt;split [4437/493]&gt; Fold02 ## 3 &lt;split [4437/493]&gt; Fold03 ## 4 &lt;split [4437/493]&gt; Fold04 ## 5 &lt;split [4437/493]&gt; Fold05 ## 6 &lt;split [4437/493]&gt; Fold06 ## 7 &lt;split [4437/493]&gt; Fold07 ## 8 &lt;split [4437/493]&gt; Fold08 ## 9 &lt;split [4437/493]&gt; Fold09 ## 10 &lt;split [4437/493]&gt; Fold10 Paso 2: Pre-procesamiento e ingeniería de variables telco_rec &lt;- recipe( Churn ~ customerID + TotalCharges + MonthlyCharges + SeniorCitizen + Contract, data = telco_train) %&gt;% update_role(customerID, new_role = &quot;id variable&quot;) %&gt;% step_mutate(Contract = as.factor(Contract)) %&gt;% step_impute_median(all_numeric_predictors()) %&gt;% step_normalize(all_numeric_predictors()) %&gt;% step_dummy(all_nominal_predictors()) %&gt;% prep() telco_rec ## Recipe ## ## Inputs: ## ## role #variables ## id variable 1 ## outcome 1 ## predictor 4 ## ## Training data contained 4930 data points and 10 incomplete rows. ## ## Operations: ## ## Variable mutation for ~as.factor(Contract) [trained] ## Median imputation for TotalCharges, MonthlyCharges, SeniorCitizen [trained] ## Centering and scaling for TotalCharges, MonthlyCharges, SeniorCitizen [trained] ## Dummy variables from Contract [trained] Paso 3: Selección de tipo de modelo con hiperparámetros iniciales knn_model &lt;- nearest_neighbor( mode = &quot;classification&quot;, neighbors = tune(&quot;K&quot;), weight_func = tune()) %&gt;% set_engine(&quot;kknn&quot;) knn_model ## K-Nearest Neighbor Model Specification (classification) ## ## Main Arguments: ## neighbors = tune(&quot;K&quot;) ## weight_func = tune() ## ## Computational engine: kknn Paso 4: Inicialización de workflow o pipeline knn_workflow &lt;- workflow() %&gt;% add_recipe(telco_rec) %&gt;% add_model(knn_model) knn_workflow ## ══ Workflow ══════════════════ ## Preprocessor: Recipe ## Model: nearest_neighbor() ## ## ── Preprocessor ────────────── ## 4 Recipe Steps ## ## • step_mutate() ## • step_impute_median() ## • step_normalize() ## • step_dummy() ## ## ── Model ───────────────────── ## K-Nearest Neighbor Model Specification (classification) ## ## Main Arguments: ## neighbors = tune(&quot;K&quot;) ## weight_func = tune() ## ## Computational engine: kknn Paso 5: Creación de grid search knn_parameters_set &lt;- extract_parameter_set_dials(knn_workflow) %&gt;% update(K = dials::neighbors(c(10,80)), weight_func = weight_func(values = c(&quot;rectangular&quot;, &quot;inv&quot;, &quot;gaussian&quot;, &quot;cos&quot;)) ) set.seed(123) knn_grid &lt;- knn_parameters_set %&gt;% grid_max_entropy(size = 50) ctrl_grid &lt;- control_grid(save_pred = T, verbose = T) Paso 6: Entrenamiento de modelos con hiperparámetros definidos library(doParallel) UseCores &lt;- detectCores() - 1 cluster &lt;- makeCluster(UseCores) registerDoParallel(cluster) knnt1 &lt;- Sys.time() knn_tune_result &lt;- tune_grid( knn_workflow, resamples = telco_folds, grid = knn_grid, metrics = metric_set(roc_auc, pr_auc), control = ctrl_grid ) knnt2 &lt;- Sys.time(); knnt2 - knnt1 stopCluster(cluster) knn_tune_result %&gt;% saveRDS(&quot;models/knn_model_cla.rds&quot;) knn_tune_result &lt;- readRDS(&quot;models/knn_model_cla.rds&quot;) Paso 7: Análisis de métricas de error e hiperparámetros (Vuelve al paso 3, si es necesario) collect_metrics(knn_tune_result) ## # A tibble: 96 × 8 ## K weight_func .metric .estimator mean n std_err .config ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 16 cos pr_auc binary 0.923 10 0.00238 Preprocessor1_Model… ## 2 16 cos roc_auc binary 0.814 10 0.00481 Preprocessor1_Model… ## 3 23 cos pr_auc binary 0.926 10 0.00234 Preprocessor1_Model… ## 4 23 cos roc_auc binary 0.822 10 0.00465 Preprocessor1_Model… ## 5 32 cos pr_auc binary 0.928 10 0.00218 Preprocessor1_Model… ## 6 32 cos roc_auc binary 0.827 10 0.00456 Preprocessor1_Model… ## 7 40 cos pr_auc binary 0.930 10 0.00214 Preprocessor1_Model… ## 8 40 cos roc_auc binary 0.829 10 0.00438 Preprocessor1_Model… ## 9 43 cos pr_auc binary 0.930 10 0.00219 Preprocessor1_Model… ## 10 43 cos roc_auc binary 0.830 10 0.00448 Preprocessor1_Model… ## # … with 86 more rows En la siguiente gráfica observamos las distintas métricas de error asociados a los hiperparámetros elegidos. autoplot(knn_tune_result, metric = &quot;pr_auc&quot;) autoplot(knn_tune_result, metric = &quot;roc_auc&quot;) show_best(knn_tune_result, n = 10, metric = &quot;pr_auc&quot;) ## # A tibble: 10 × 8 ## K weight_func .metric .estimator mean n std_err .config ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 79 cos pr_auc binary 0.930 10 0.00227 Preprocessor1_Model… ## 2 70 cos pr_auc binary 0.930 10 0.00221 Preprocessor1_Model… ## 3 75 gaussian pr_auc binary 0.930 10 0.00219 Preprocessor1_Model… ## 4 65 cos pr_auc binary 0.930 10 0.00220 Preprocessor1_Model… ## 5 67 gaussian pr_auc binary 0.930 10 0.00216 Preprocessor1_Model… ## 6 59 cos pr_auc binary 0.930 10 0.00224 Preprocessor1_Model… ## 7 56 cos pr_auc binary 0.930 10 0.00223 Preprocessor1_Model… ## 8 51 cos pr_auc binary 0.930 10 0.00219 Preprocessor1_Model… ## 9 48 cos pr_auc binary 0.930 10 0.00217 Preprocessor1_Model… ## 10 59 gaussian pr_auc binary 0.930 10 0.00223 Preprocessor1_Model… Paso 8: Selección de modelo a usar best_knn_model_cla &lt;- select_best(knn_tune_result, metric = &quot;pr_auc&quot;) best_knn_model_cla ## # A tibble: 1 × 3 ## K weight_func .config ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 79 cos Preprocessor1_Model12 knn_classification_best_1se_model &lt;- knn_tune_result %&gt;% select_by_one_std_err(metric = &quot;roc_auc&quot;, &quot;roc_auc&quot;) knn_classification_best_1se_model ## # A tibble: 1 × 10 ## K weight_func .metric .estimator mean n std_err .config .best .bound ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 32 cos roc_auc binary 0.827 10 0.00456 Preproc… 0.831 0.827 Paso 9: Ajuste de modelo final con todos los datos (Vuelve al paso 2, si es necesario) final_knn_model_cla &lt;- knn_workflow %&gt;% finalize_workflow(best_knn_model_cla) %&gt;% parsnip::fit(data = telco_train) Este último objeto es el modelo final entrenado, el cual contiene toda la información del pre-procesamiento de datos, por lo que en caso de ponerse en producción el modelo, sólo se necesita de este último elemento para poder realizar nuevas predicciones. Antes de pasar al siguiente paso, es importante validar que hayamos hecho un uso correcto de las variables predictivas. En este momento es posible detectar variables que no estén aportando valor o variables que no debiéramos estar usando debido a que cometeríamos data leakage. Para enfrentar esto, ayuda estimar y ordenar el valor de importancia del modelo. churn_importance &lt;- final_knn_model_cla %&gt;% extract_fit_parsnip() %&gt;% vi( method = &quot;permute&quot;, nsim = 30, target = &quot;Churn&quot;, metric = &quot;auc&quot;, reference_class = &quot;Yes&quot;, pred_wrapper = kernlab::predict, train = juice(telco_rec) ) churn_importance %&gt;% saveRDS(&quot;models/vip_telco_knn.rds&quot;) churn_importance &lt;- readRDS(&quot;models/vip_telco_knn.rds&quot;) churn_importance ## # A tibble: 5 × 3 ## Variable Importance StDev ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 TotalCharges 0.0847 0.00685 ## 2 MonthlyCharges 0.0820 0.00595 ## 3 Contract_Two.year 0.0507 0.00353 ## 4 Contract_One.year 0.0468 0.00389 ## 5 SeniorCitizen 0.0232 0.00329 churn_importance %&gt;% mutate(Variable = forcats::fct_reorder(Variable, Importance)) %&gt;% ggplot(aes(Importance, Variable, color = Variable)) + geom_errorbar(aes(xmin = Importance - StDev, xmax = Importance + StDev), alpha = 0.5, size = 1) + geom_point(size = 2) + theme(legend.position = &quot;none&quot;) + ggtitle(&quot;Variable Importance Measure&quot;) Paso 10: Validar poder predictivo con datos de prueba Imaginemos por un momento que pasa un mes de tiempo desde que hicimos nuestro modelo, es hora de ponerlo a prueba prediciendo valores de nuevos elementos: results_cla &lt;- predict(final_knn_model_cla, telco_test, type = &#39;prob&#39;) %&gt;% dplyr::bind_cols(Churn = telco_test$Churn, .) %&gt;% mutate(Churn = factor(Churn, levels = c(&#39;Yes&#39;, &#39;No&#39;), labels = c(&#39;Yes&#39;, &#39;No&#39;))) results_cla ## # A tibble: 2,113 × 3 ## Churn .pred_No .pred_Yes ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 No 0.959 0.0414 ## 2 Yes 0.268 0.732 ## 3 No 0.724 0.276 ## 4 No 1 0 ## 5 No 0.883 0.117 ## 6 No 0.515 0.485 ## 7 No 0.994 0.00557 ## 8 No 0.835 0.165 ## 9 Yes 0.642 0.358 ## 10 No 0.987 0.0126 ## # … with 2,103 more rows bind_rows( roc_auc(results_cla, truth = Churn, estimate = .pred_Yes), pr_auc(results_cla, truth = Churn, estimate = .pred_Yes) ) ## # A tibble: 2 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 roc_auc binary 0.834 ## 2 pr_auc binary 0.629 pr_curve_data &lt;- pr_curve( results_cla, truth = Churn, estimate = .pred_Yes ) pr_curve_data ## # A tibble: 1,890 × 3 ## .threshold recall precision ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Inf 0 1 ## 2 0.850 0.00177 1 ## 3 0.848 0.00353 0.667 ## 4 0.845 0.00530 0.75 ## 5 0.843 0.00883 0.833 ## 6 0.840 0.0106 0.857 ## 7 0.838 0.0141 0.889 ## 8 0.838 0.0159 0.9 ## 9 0.835 0.0177 0.909 ## 10 0.835 0.0194 0.917 ## # … with 1,880 more rows roc_curve_data &lt;- roc_curve( results_cla, truth = Churn, estimate = .pred_Yes ) roc_curve_data ## # A tibble: 1,891 × 3 ## .threshold specificity sensitivity ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -Inf 0 1 ## 2 0 0 1 ## 3 0.0000199 0.123 0.993 ## 4 0.0000704 0.124 0.993 ## 5 0.000167 0.125 0.993 ## 6 0.000288 0.125 0.993 ## 7 0.000599 0.126 0.993 ## 8 0.000921 0.127 0.993 ## 9 0.00118 0.127 0.993 ## 10 0.00126 0.128 0.993 ## # … with 1,881 more rows pr_curve_plot &lt;- pr_curve_data %&gt;% ggplot(aes(x = recall, y = precision)) + geom_abline(slope = -1, intercept = 1) + geom_path(size = 1, colour = &#39;lightblue&#39;) + ylim(0, 1) + coord_equal() + ggtitle(&quot;Precision vs Recall&quot;)+ theme_minimal() pr_curve_plot roc_curve_plot &lt;- roc_curve_data %&gt;% ggplot(aes(x = 1 - specificity, y = sensitivity)) + geom_path(size = 1, colour = &#39;lightblue&#39;) + geom_abline() + coord_equal() + ggtitle(&quot;ROC Curve&quot;)+ theme_minimal() roc_curve_plot Pueden usar la app de shiny que nos permite jugar con el threshold de clasificación para tomar la mejor decisión. "],["árboles-de-decisión.html", "Capítulo 9 Árboles de decisión 9.1 Ajuste del modelo 9.2 Regularización de árboles 9.3 Aprendizaje conjunto 9.4 Bagging 9.5 Random Forest 9.6 Implementación de RF en R 9.7 Boosting 9.8 Implementación de XGB en R", " Capítulo 9 Árboles de decisión Un árbol de decisiones es un algoritmo del aprendizaje supervisado que se puede utilizar tanto para problemas de clasificación como de regresión. Es un clasificador estructurado en árbol, donde los nodos internos representan las características de un conjunto de datos, las ramas representan las reglas de decisión y cada nodo hoja representa el resultado. La idea básica de los árboles es buscar puntos de cortes en las variables de entrada para hacer predicciones, ir dividiendo la muestra, y encontrar cortes sucesivos para refinar las predicciones. En un árbol de decisión, hay dos tipos nodos, el nodo de decisión o nodos internos (Decision Node) y el nodo hoja o nodo terminal (Leaf node). Los nodos de decisión se utilizan para tomar cualquier decisión y tienen múltiples ramas, mientras que los nodos hoja son el resultado de esas decisiones y no contienen más ramas. Regresión: En el caso de la regresión de árboles de decisión, en los nodos finales se calcula el promedio de la variable de respuesta. El promedio será la estimación del modelo. Clasificación: Por otro lado, en los árboles de clasificación se calcula la proporción de elementos de cada categoría en los nodos finales. De esta manera se calcula la probabilidad de pertenencia a la categoría. 9.1 Ajuste del modelo En un árbol de decisión, para predecir la clase del conjunto de datos, el algoritmo comienza desde el nodo raíz del árbol. Este algoritmo compara los valores de la variable raíz con la variable de registro y, según la comparación, sigue una rama y salta al siguiente nodo. Para el siguiente nodo, el algoritmo vuelve a comparar el valor de la siguiente variable con los otros sub-nodos y avanza. Continúa el proceso hasta que se llega a un nodo hoja. El proceso completo se puede comprender mejor con los siguientes pasos: Comenzamos el árbol con el nodo raíz (llamado S), que contiene el conjunto de entrenamiento completo. Encuentre la mejor variable en el conjunto de datos usando Attribute Selective Measure (ASM). Divida la S en subconjuntos que contengan valores posibles para la mejor variable. Genere el nodo del árbol de decisión, que contiene la mejor variable. Cree de forma recursiva nuevos árboles de decisión utilizando los subconjuntos del conjunto de datos creado en el paso 3. Continúe este proceso hasta que se alcance una etapa en la que no pueda particionar más los nodos y este nodo final sera un nodo hoja. Para clasificación nos quedaremos la moda de la variable respuesta del nodo hoja y para regresión usaremos la media de la variable respuesta. 9.1.1 Attribute Selective Measure (ASM) Al implementar un árbol de decisión, surge el problema principal de cómo seleccionar la mejor variable para el nodo raíz y para los sub-nodos. Para resolver este problemas existe una técnica que se llama medida de selección de atributos o ASM. Mediante esta medición, podemos seleccionar fácilmente la mejor variable para los nodos del árbol. Una de las técnicas más populares para ASM es: Índice de Gini La medida del grado de probabilidad de que una variable en particular se clasifique incorrectamente cuando se elige al azar se llama índice de Gini o impureza de Gini. Los datos se distribuyen por igual según el índice de Gini. \\[Gini = \\sum_{i=1}^{n}\\hat{p_i}(1-\\hat{p}_i)\\] Con \\(p_i\\) como la probabilidad de que un objeto se clasifique en una clase particular. Esta métrica puede analizarse como una métrica de impureza. Cuando todos o la mayoría de elementos dentro de un nodo pertenecen a una misma clase, el índice de Gini toma valores cercanos a cero. Cuando se utiliza el índice de Gini como criterio seleccionar la variable para el nodo raíz, seleccionaremos la variable con el índice de Gini menor. 9.2 Regularización de árboles Para asegurarse de que no exista sobre-ajuste en el modelo, es importante considerar algunas regularizaciones a los hiper-parámetros implementados. Posteriormente, se determinará cuál de las posibles combinaciones produce mejores resultados. 9.2.1 Nivel de profundidad de árbol Podríamos preguntarnos cuándo dejar de crecer un árbol. Pueden existir problemas que tengan un gran conjunto de variables y esto da como resultado una gran cantidad de divisiones, lo que a su vez genera un árbol de decisión muy grande. Estos árboles son complejos y pueden provocar un sobre-ajuste. Entonces, necesitamos saber cuándo parar. Una forma de hacer esto, es establecer un número mínimo de entradas de entrenamiento para dividir un nodo (min_n). Otra forma, es establecer la profundidad máxima del modelo. La profundidad máxima se refiere a la longitud del camino más largo desde el nodo raíz hasta un nodo hoja (max_depth). 9.2.2 Poda de árbol El rendimiento de un árbol se puede aumentar aún más mediante la poda del árbol. Esto se refiere a eliminar las ramas que hacen uso de variables de poca importancia. De esta manera, reducimos la complejidad del árbol y, por lo tanto, aumentamos su poder predictivo al reducir el sobre-ajuste. La poda puede comenzar en la raíz o en las hojas. El método más simple de poda comienza en las hojas y elimina cada nodo con la clase más popular en esa hoja, este cambio se mantiene si no deteriora la precisión. Se pueden usar métodos de poda más sofisticados, como la poda de complejidad de costos, donde se usa un parámetro de aprendizaje (alfa) para observar si los nodos se pueden eliminar en función del tamaño del sub-árbol. 9.3 Aprendizaje conjunto El aprendizaje conjunto da crédito a la idea de la “sabiduría de las multitudes”, lo que sugiere que la toma de decisiones de un grupo más grande de individuos (modelos) suele ser mejor que la de un individuo. El aprendizaje en conjunto es un grupo (o conjunto) de individuos o modelos, que trabajan colectivamente para lograr una mejor predicción final. Un solo modelo, también conocido como aprendiz básico puede no funcionar bien individualmente debido a una gran variación o un alto sesgo, sin embargo, cuando se agregan individuos débiles, pueden formar un individuo fuerte, ya que su combinación reduce el sesgo o la varianza, lo que produce un mejor rendimiento del modelo. Los métodos de conjunto se ilustran con frecuencia utilizando árboles de decisión, ya que este algoritmo puede ser propenso a sobre ajustar (alta varianza y bajo sesgo) y también puede prestarse a desajuste (baja varianza y alto sesgo) cuando es muy pequeño, como un árbol de decisión con un nivel. Nota: Cuando un algoritmo se adapta o no se adapta a su conjunto de entrenamiento, no se puede generalizar bien a nuevos conjuntos de datos, por lo que se utilizan métodos de conjunto para contrarrestar este comportamiento y permitir la generalización del modelo a nuevos conjuntos de datos. 9.3.1 Bagging vs. boosting Bagging y el boosting (refuerzo o impulso) son dos tipos principales de métodos de aprendizaje por conjuntos. La principal diferencia entre estos métodos de aprendizaje es la forma en que se capacitan. En bagging, los modelos se entrenan en paralelo, pero en el boosting, aprenden secuencialmente. Esto significa que se construyen una serie de modelos y con cada nueva iteración del modelo, se incrementan los pesos de los datos clasificados erróneamente en el modelo anterior. Esta redistribución de pesos ayuda al algoritmo a identificar los parámetros en los que necesita enfocarse para mejorar su desempeño. Un ejemplo de modelo secuencial es: Adaboost y significa “algoritmo de boosting adaptativo”, es uno de los algoritmos de boosting más populares, ya que fue uno de los primeros de su tipo. Otros tipos de algoritmos de boosting incluyen XGBoost, GradientBoost y BrownBoost. Otra diferencia en la que difieren bagging y boosting son los escenarios en los que se utilizan. Por ejemplo, los métodos de bagging se utilizan típicamente en modelos débiles que exhiben alta varianza y bajo sesgo, mientras que los métodos de boosting se aprovechan cuando se observa baja varianza y alto sesgo. ¡¡ RECORDAR !! Bagging realiza replicaciones bootstrap y ajusta un árbol a cada muestra de manera independiente, mientras que boosting ajusta un árbol a una versión modificada del conjunto original de datos, la cual se modifica en cada iteración de entrenamiento. 9.3.2 Error Out-Of-Bag Este error es conocido como “OOB”. Se trata de un enfoque distinto a KFCV en donde el error predictivo es calculado a través de los elementos que no fueron seleccionados en la muestra bootstrap. Recordemos que en las muestras bootstrap algunos elementos son seleccionados más de una vez, mientras que otros no aparecen en la muestra. Empíricamente, en cada replicación bootstrap se observan 2/3 partes de la muestra y el resto queda “fuera de la bolsa” (OOB) de entrenamiento. Si B es el número de replicaciones bootstrap, entonces cada observación i recibe cerca de B/3 predicciones, las cuales son usadas para estimar el error predictivo. Para obtener una única predicción en cada observación, las B/3 predicciones son promediadas. 9.4 Bagging Primero tenemos que definir qué es la Agregación de Bootstrap o Bagging. Este es un algoritmo de aprendizaje automático diseñado para mejorar la estabilidad y precisión de algoritmos de ML usados en clasificación estadística y regresión. Además reduce la varianza y ayuda a evitar el sobre-ajuste. Aunque es usualmente aplicado a métodos de árboles de decisión, puede ser usado con cualquier tipo de método. Bagging es un caso especial del promediado de modelos. Los métodos de bagging son métodos donde los algoritmos simples son usados en paralelo. El principal objetivo de los métodos en paralelo es el de aprovecharse de la independencia que hay entre los algoritmos simples, ya que el error se puede reducir bastante al promediar las salidas de los modelos simples. Es como si, queriendo resolver un problema entre varias personas independientes unas de otras, damos por bueno lo que eligiese la mayoría de las personas. Para obtener la agregación de las salidas de cada modelo simple e independiente, bagging puede usar la votación para los métodos de clasificación y el promedio para los métodos de regresión. El bagging o agregación bootstrap, es un método de aprendizaje por conjuntos que se usa comúnmente para reducir la varianza dentro de un conjunto de datos ruidoso. 9.5 Random Forest Un bosque aleatorio es un algoritmo de aprendizaje automático supervisado que se construye a partir de algoritmos de árbol de decisión. Este algoritmo se aplica en diversas industrias, como la banca y el comercio electrónico, para predecir el comportamiento y los resultados. En esta clase se dará una descripción general del algoritmo de bosque aleatorio, cómo funciona y las características del algoritmo. También se señalan las ventajas y desventajas de este algoritmo. 9.5.1 ¿Qué es? Un bosque aleatorio es una técnica de aprendizaje automático que se utiliza para resolver problemas de regresión y clasificación. Utiliza el aprendizaje por conjuntos, que es una técnica que combina muchos clasificadores para proporcionar soluciones a problemas complejos. Este algoritmo consta de muchos árboles de decisión. El “bosque” generado se entrena mediante agregación de bootstrap (bagging), el cual es es un meta-algoritmo de conjunto que mejora la precisión de los algoritmos de aprendizaje automático. El algoritmo establece el resultado en función de las predicciones de los árboles de decisión. Predice tomando el promedio o la media de la salida de varios árboles. El aumento del número de árboles aumenta la precisión del resultado. Un bosque aleatorio erradica las limitaciones de un algoritmo de árbol de decisión. Reduce el sobre-ajuste de conjuntos de datos y aumenta la precisión. Genera predicciones sin requerir muchas configuraciones. 9.5.2 Características de los bosques aleatorios Es más preciso que el algoritmo árbol de decisiones. Proporciona una forma eficaz de gestionar los datos faltantes. Puede producir una predicción razonable sin ajuste de hiperparámetros. Resuelve el problema del sobre-ajuste en los árboles de decisión. En cada árbol forestal aleatorio, se selecciona aleatoriamente un subconjunto de características en el punto de división del nodo. 9.5.3 Aplicar árboles de decisión en un bosque aleatorio La principal diferencia entre el algoritmo de árbol de decisión y el algoritmo de bosque aleatorio es que el establecimiento de nodos raíz y la desagregación de nodos se realiza de forma aleatoria en este último. El bosque aleatorio emplea el método de bagging para generar la predicción requerida. El método bagging implica el uso de diferentes muestras de datos (datos de entrenamiento) en lugar de una sola muestra. Los árboles de decisión producen diferentes resultados, dependiendo de los datos de entrenamiento alimentados al algoritmo de bosque aleatorio. Nuestro primer ejemplo todavía se puede utilizar para explicar cómo funcionan los bosques aleatorios. Supongamos que solo tenemos cuatro árboles de decisión. En este caso, los datos de entrenamiento que comprenden las observaciones y características de estudio se dividirán en cuatro nodos raíz. Supongamos que queremos modelar si un cliente compra o no compra un teléfono. Los nodos raíz podrían representar cuatro características que podrían influir en la elección de un cliente (precio, almacenamiento interno, cámara y RAM). El bosque aleatorio dividirá los nodos seleccionando características al azar. La predicción final se seleccionará en función del resultado de los cuatro árboles. El resultado elegido por la mayoría de los árboles de decisión será la elección final. Si tres árboles predicen la compra y un árbol predice que no comprará, entonces la predicción final será la compra. En este caso, se prevé que el cliente comprará. El siguiente diagrama muestra un clasificador de bosque aleatorio simple. 9.5.4 Ventajas y desventjas de bosques aleatorios Ventajas Puede realizar tareas de regresión y clasificación. Un bosque aleatorio produce buenas predicciones que se pueden entender fácilmente. Puede manejar grandes conjuntos de datos de manera eficiente. Proporciona un mayor nivel de precisión en la predicción de resultados sobre el algoritmo del árbol de decisión. Desventajas Cuando se usa un bosque aleatorio, se requieren bastantes recursos para el cálculo. Consume más tiempo en comparación con un algoritmo de árbol de decisiones. No producen buenos resultados cuando los datos son muy escasos. En este caso, el subconjunto de características y la muestra de arranque producirán un espacio invariante. Esto conducirá a divisiones improductivas, que afectarán el resultado. 9.6 Implementación de RF en R 9.6.1 Regresión Paso 1: Separación inicial de datos ( test, train ) library(tidymodels) data(ames) set.seed(4595) ames_split &lt;- initial_split(ames, prop = 0.75) ames_train &lt;- training(ames_split) ames_test &lt;- testing(ames_split) ames_folds &lt;- vfold_cv(ames_train) Contando con datos de entrenamiento, procedemos a realizar el feature engineering para extraer las mejores características que permitirán realizar las estimaciones en el modelo. Paso 2: Pre-procesamiento e ingeniería de variables receta_casas &lt;- recipe( Sale_Price ~ Gr_Liv_Area + TotRms_AbvGrd + Exter_Cond + Bsmt_Cond + Year_Sold + Year_Remod_Add, data = ames_train) %&gt;% step_mutate( Age_House = Year_Sold - Year_Remod_Add, Exter_Cond = forcats::fct_collapse(Exter_Cond, Good = c(&quot;Typical&quot;, &quot;Good&quot;, &quot;Excellent&quot;))) %&gt;% step_relevel(Exter_Cond, ref_level = &quot;Good&quot;) %&gt;% step_normalize(all_numeric_predictors()) %&gt;% step_dummy(all_nominal_predictors()) %&gt;% step_interact(~ matches(&quot;Bsmt_Cond&quot;):TotRms_AbvGrd) %&gt;% prep() receta_casas ## Recipe ## ## Inputs: ## ## role #variables ## outcome 1 ## predictor 6 ## ## Training data contained 2197 data points and no missing data. ## ## Operations: ## ## Variable mutation for ~Year_Sold - Year_Remod_Add, ~forcats::fct... [trained] ## Re-order factor level to ref_level for Exter_Cond [trained] ## Centering and scaling for Gr_Liv_Area, TotRms_AbvGrd, Year_Sold, Year_Rem... [trained] ## Dummy variables from Exter_Cond, Bsmt_Cond [trained] ## Interactions with (Bsmt_Cond_Fair + Bsmt_Cond_Good + Bsmt_Cond_No_Ba... [trained] Recordemos que la función recipe() solo son los pasos a seguir, necesitamos usar la función prep() que nos devuelve una receta actualizada con las estimaciones y la función juice() que nos devuelve la matriz de diseño. Una vez que la receta de transformación de datos está lista, procedemos a implementar el pipeline del modelo de interés. Paso 3: Selección de tipo de modelo con hiperparámetros iniciales # install.packages(&quot;ranger&quot;) library(ranger) rforest_model &lt;- rand_forest( mode = &quot;regression&quot;, trees = 1000, mtry = tune(), min_n = tune()) %&gt;% set_engine(&quot;ranger&quot;, importance = &quot;impurity&quot;) Paso 4: Inicialización de workflow o pipeline rforest_workflow &lt;- workflow() %&gt;% add_model(rforest_model) %&gt;% add_recipe(receta_casas) Paso 5: Creación de grid search set.seed(195628) rforest_param_grid &lt;- grid_random( mtry(range = c(2,15)), min_n(range = c(2,16)), size = 50 ) ctrl_grid &lt;- control_grid(save_pred = T, verbose = T) Paso 6: Entrenamiento de modelos con hiperparámetros definidos library(doParallel) UseCores &lt;- detectCores() - 1 cluster &lt;- makeCluster(UseCores) registerDoParallel(cluster) # Ajuste de parámetros rft1 &lt;- Sys.time() rforest_tune_result &lt;- tune_grid( rforest_workflow, resamples = ames_folds, grid = rforest_param_grid, metrics = metric_set(rmse, rsq, mae), control = ctrl_grid ) rft2 &lt;- Sys.time(); rft2 - rft1 stopCluster(cluster) rforest_tune_result %&gt;% saveRDS(&quot;models/random_forest_model_reg.rds&quot;) rforest_tune_result &lt;- readRDS(&quot;models/random_forest_model_reg.rds&quot;) Paso 7: Análisis de métricas de error e hiperparámetros (Vuelve al paso 3, si es necesario) Podemos obtener las métricas de cada fold con el siguiente código: collect_metrics(rforest_tune_result) ## # A tibble: 132 × 8 ## mtry min_n .metric .estimator mean n std_err .config ## &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 11 11 mae standard 28160. 10 560. Preprocessor1_Model… ## 2 11 11 rmse standard 42133. 10 1126. Preprocessor1_Model… ## 3 11 11 rsq standard 0.721 10 0.0116 Preprocessor1_Model… ## 4 11 13 mae standard 28139. 10 556. Preprocessor1_Model… ## 5 11 13 rmse standard 42183. 10 1118. Preprocessor1_Model… ## 6 11 13 rsq standard 0.720 10 0.0114 Preprocessor1_Model… ## 7 7 3 mae standard 28054. 10 576. Preprocessor1_Model… ## 8 7 3 rmse standard 41511. 10 1145. Preprocessor1_Model… ## 9 7 3 rsq standard 0.728 10 0.0115 Preprocessor1_Model… ## 10 9 16 mae standard 27991. 10 568. Preprocessor1_Model… ## # … with 122 more rows En la siguiente gráfica observamos las distintas métricas de error asociados a los hiperparámetros elegidos: rforest_tune_result %&gt;% autoplot() En la siguiente gráfica observamos el error cuadrático medio de las distintas métricas con distintos números de vecinos. rforest_tune_result %&gt;% autoplot(metric = &quot;rsq&quot;) multiparams_plot &lt;- rforest_tune_result %&gt;% collect_metrics() %&gt;% filter(.metric == &quot;rmse&quot;) %&gt;% rename(rmse = mean) %&gt;% ggplot(aes(x = mtry, y = min_n, colour = rmse)) + geom_point() + scale_color_gradientn(colours = rainbow(7)) + labs( title = &quot;Análisis de R^2 mediante ajuste de hiperparámetros&quot;, x = &quot;Número de ramas&quot;, y = &quot;Mínimo de elementos por nodo&quot; ) plotly::ggplotly(multiparams_plot) En la gráfica anterior, se aprecia la distribución conjunta de hiperparámetros y su resultados en la \\(R^2\\). De esta forma, se puede tomar una mejor decisión sobre el subconjunto óptimo de hiperparámetros a seleccionar. Paso 8: Selección de modelo a usar Con el siguiente código obtenemos los mejores 10 modelos respecto al rmse. show_best(rforest_tune_result, n = 10, metric = &quot;rmse&quot;) ## # A tibble: 10 × 8 ## mtry min_n .metric .estimator mean n std_err .config ## &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 6 10 rmse standard 41447. 10 1156. Preprocessor1_Model07 ## 2 7 3 rmse standard 41511. 10 1145. Preprocessor1_Model03 ## 3 5 6 rmse standard 41525. 10 1259. Preprocessor1_Model14 ## 4 6 8 rmse standard 41537. 10 1201. Preprocessor1_Model34 ## 5 5 7 rmse standard 41551. 10 1259. Preprocessor1_Model31 ## 6 6 9 rmse standard 41569. 10 1195. Preprocessor1_Model10 ## 7 8 7 rmse standard 41708. 10 1151. Preprocessor1_Model18 ## 8 8 4 rmse standard 41711. 10 1124. Preprocessor1_Model33 ## 9 8 12 rmse standard 41718. 10 1138. Preprocessor1_Model22 ## 10 8 3 rmse standard 41727. 10 1143. Preprocessor1_Model41 Ahora obtendremos el modelo que mejor desempeño tiene tomando en cuenta el rmse y haremos las predicciones del conjunto de prueba con este modelo. best_rforest_model &lt;- select_best(rforest_tune_result, metric = &quot;rmse&quot;) best_rforest_model ## # A tibble: 1 × 3 ## mtry min_n .config ## &lt;int&gt; &lt;int&gt; &lt;chr&gt; ## 1 6 10 Preprocessor1_Model07 Paso 9: Ajuste de modelo final con todos los datos (Vuelve al paso 2, si es necesario) final_rforest_model &lt;- rforest_workflow %&gt;% finalize_workflow(best_rforest_model) %&gt;% fit(data = ames_train) Este último objeto es el modelo final entrenado, el cual contiene toda la información del pre-procesamiento de datos, por lo que en caso de ponerse en producción el modelo, sólo se necesita de este último elemento para poder realizar nuevas predicciones. Antes de pasar al siguiente paso, es importante validar que hayamos hecho un uso correcto de las variables predictivas. En este momento es posible detectar variables que no estén aportando valor o variables que no debiéramos estar usando debido a que cometeríamos data leakage. Para enfrentar esto, ayuda estimar y ordenar el valor de importancia de cada variable en el modelo. library(vip) final_rforest_model %&gt;% extract_fit_parsnip() %&gt;% vip(geom = &quot;col&quot;) + ggtitle(&quot;Importancia de las variables&quot;) Paso 10: Validar poder predictivo con datos de prueba Imaginemos por un momento que pasa un mes de tiempo desde que hicimos nuestro modelo, es hora de ponerlo a prueba prediciendo valores de nuevos elementos: results &lt;- predict(final_rforest_model, ames_test) %&gt;% dplyr::bind_cols(Sale_Price = ames_test$Sale_Price) %&gt;% dplyr::rename(pred_rforest_reg = .pred) results ## # A tibble: 733 × 2 ## pred_rforest_reg Sale_Price ## &lt;dbl&gt; &lt;int&gt; ## 1 115022. 105000 ## 2 164222. 185000 ## 3 175564. 180400 ## 4 100135. 141000 ## 5 211380. 210000 ## 6 201731. 216000 ## 7 150731. 149900 ## 8 124723. 105500 ## 9 124723. 88000 ## 10 156835. 146000 ## # … with 723 more rows Métricas de desempeño Ahora para calcular las métricas de desempeño usaremos la paquetería MLmetrics. Es posible definir nuestro propio conjunto de métricas que deseamos reportar creando el objeto metric_set: library(MLmetrics) multi_metric &lt;- metric_set(mae, mape, rmse, rsq, ccc) multi_metric(results, truth = Sale_Price, estimate = pred_rforest_reg) %&gt;% mutate(.estimate = round(.estimate, 2)) %&gt;% select(-.estimator) ## # A tibble: 5 × 2 ## .metric .estimate ## &lt;chr&gt; &lt;dbl&gt; ## 1 mae 28848 ## 2 mape 17.0 ## 3 rmse 42780. ## 4 rsq 0.72 ## 5 ccc 0.83 results %&gt;% ggplot(aes(x = pred_rforest_reg, y = Sale_Price)) + geom_point() + geom_abline(color = &quot;red&quot;) + xlab(&quot;Prediction&quot;) + ylab(&quot;Observation&quot;) + ggtitle(&quot;Comparisson&quot;) 9.6.2 Clasificación Es turno de revisar la implementación de Random Forest con nuestro bien conocido problema de predicción de cancelación de servicios de telecomunicaciones. Los datos se encuentran disponibles en el siguiente enlace: Los pasos para implementar en R este modelo predictivo son los mismos, cambiando únicamente las especificaciones del tipo de modelo, pre-procesamiento e hiper-parámetros. library(readr) library(tidyverse) library(tidymodels) tidymodels_prefer() telco &lt;- read_csv(&quot;data/Churn.csv&quot;) glimpse(telco) ## Rows: 7,043 ## Columns: 21 ## $ customerID &lt;chr&gt; &quot;7590-VHVEG&quot;, &quot;5575-GNVDE&quot;, &quot;3668-QPYBK&quot;, &quot;7795-CFOCW… ## $ gender &lt;chr&gt; &quot;Female&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Female&quot;, &quot;Female&quot;,… ## $ SeniorCitizen &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ Partner &lt;chr&gt; &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes… ## $ Dependents &lt;chr&gt; &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;… ## $ tenure &lt;dbl&gt; 1, 34, 2, 45, 2, 8, 22, 10, 28, 62, 13, 16, 58, 49, 2… ## $ PhoneService &lt;chr&gt; &quot;No&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;No&quot;, … ## $ MultipleLines &lt;chr&gt; &quot;No phone service&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No phone service&quot;, &quot;… ## $ InternetService &lt;chr&gt; &quot;DSL&quot;, &quot;DSL&quot;, &quot;DSL&quot;, &quot;DSL&quot;, &quot;Fiber optic&quot;, &quot;Fiber opt… ## $ OnlineSecurity &lt;chr&gt; &quot;No&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;… ## $ OnlineBackup &lt;chr&gt; &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;N… ## $ DeviceProtection &lt;chr&gt; &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Y… ## $ TechSupport &lt;chr&gt; &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes… ## $ StreamingTV &lt;chr&gt; &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;Ye… ## $ StreamingMovies &lt;chr&gt; &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes… ## $ Contract &lt;chr&gt; &quot;Month-to-month&quot;, &quot;One year&quot;, &quot;Month-to-month&quot;, &quot;One … ## $ PaperlessBilling &lt;chr&gt; &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;No&quot;, … ## $ PaymentMethod &lt;chr&gt; &quot;Electronic check&quot;, &quot;Mailed check&quot;, &quot;Mailed check&quot;, &quot;… ## $ MonthlyCharges &lt;dbl&gt; 29.85, 56.95, 53.85, 42.30, 70.70, 99.65, 89.10, 29.7… ## $ TotalCharges &lt;dbl&gt; 29.85, 1889.50, 108.15, 1840.75, 151.65, 820.50, 1949… ## $ Churn &lt;chr&gt; &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Y… Paso 1: Separación inicial de datos ( test, train ) set.seed(1234) telco_split &lt;- initial_split(telco, prop = .70) telco_train &lt;- training(telco_split) telco_test &lt;- testing(telco_split) telco_folds &lt;- vfold_cv(telco_train) telco_folds ## # 10-fold cross-validation ## # A tibble: 10 × 2 ## splits id ## &lt;list&gt; &lt;chr&gt; ## 1 &lt;split [4437/493]&gt; Fold01 ## 2 &lt;split [4437/493]&gt; Fold02 ## 3 &lt;split [4437/493]&gt; Fold03 ## 4 &lt;split [4437/493]&gt; Fold04 ## 5 &lt;split [4437/493]&gt; Fold05 ## 6 &lt;split [4437/493]&gt; Fold06 ## 7 &lt;split [4437/493]&gt; Fold07 ## 8 &lt;split [4437/493]&gt; Fold08 ## 9 &lt;split [4437/493]&gt; Fold09 ## 10 &lt;split [4437/493]&gt; Fold10 Paso 2: Pre-procesamiento e ingeniería de variables telco_rec &lt;- recipe( Churn ~ customerID + TotalCharges + MonthlyCharges + SeniorCitizen + Contract, data = telco_train) %&gt;% update_role(customerID, new_role = &quot;id variable&quot;) %&gt;% step_mutate(Contract = as.factor(Contract)) %&gt;% step_impute_median(all_numeric_predictors()) %&gt;% step_normalize(all_numeric_predictors()) %&gt;% step_dummy(all_nominal_predictors()) %&gt;% prep() telco_rec ## Recipe ## ## Inputs: ## ## role #variables ## id variable 1 ## outcome 1 ## predictor 4 ## ## Training data contained 4930 data points and 10 incomplete rows. ## ## Operations: ## ## Variable mutation for ~as.factor(Contract) [trained] ## Median imputation for TotalCharges, MonthlyCharges, SeniorCitizen [trained] ## Centering and scaling for TotalCharges, MonthlyCharges, SeniorCitizen [trained] ## Dummy variables from Contract [trained] Paso 3: Selección de tipo de modelo con hiperparámetros iniciales rforest_model &lt;- rand_forest( mode = &quot;classification&quot;, trees = 1000, mtry = tune(), min_n = tune()) %&gt;% set_engine(&quot;ranger&quot;, importance = &quot;impurity&quot;) rforest_model ## Random Forest Model Specification (classification) ## ## Main Arguments: ## mtry = tune() ## trees = 1000 ## min_n = tune() ## ## Engine-Specific Arguments: ## importance = impurity ## ## Computational engine: ranger Paso 4: Inicialización de workflow o pipeline rforest_workflow &lt;- workflow() %&gt;% add_recipe(telco_rec) %&gt;% add_model(rforest_model) rforest_workflow ## ══ Workflow ══════════════════ ## Preprocessor: Recipe ## Model: rand_forest() ## ## ── Preprocessor ────────────── ## 4 Recipe Steps ## ## • step_mutate() ## • step_impute_median() ## • step_normalize() ## • step_dummy() ## ## ── Model ───────────────────── ## Random Forest Model Specification (classification) ## ## Main Arguments: ## mtry = tune() ## trees = 1000 ## min_n = tune() ## ## Engine-Specific Arguments: ## importance = impurity ## ## Computational engine: ranger Paso 5: Creación de grid search set.seed(195628) rforest_param_grid &lt;- grid_random( mtry(range = c(2,5)), min_n(range = c(2,16)), size = 20 ) ctrl_grid &lt;- control_grid(save_pred = T, verbose = T) Paso 6: Entrenamiento de modelos con hiperparámetros definidos library(doParallel) UseCores &lt;- detectCores() - 1 cluster &lt;- makeCluster(UseCores) registerDoParallel(cluster) rft1 &lt;- Sys.time() rf_tune_result &lt;- tune_grid( rforest_workflow, resamples = telco_folds, grid = rforest_param_grid, metrics = metric_set(roc_auc, pr_auc), control = ctrl_grid ) rft2 &lt;- Sys.time(); rft2 - rft1 stopCluster(cluster) rf_tune_result %&gt;% saveRDS(&quot;models/rforest_model_cla.rds&quot;) rf_tune_result &lt;- readRDS(&quot;models/rforest_model_cla.rds&quot;) Paso 7: Análisis de métricas de error e hiperparámetros (Vuelve al paso 3, si es necesario) collect_metrics(rf_tune_result) ## # A tibble: 34 × 8 ## mtry min_n .metric .estimator mean n std_err .config ## &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 3 16 pr_auc binary 0.930 10 0.00372 Preprocessor1_Model01 ## 2 3 16 roc_auc binary 0.825 10 0.00623 Preprocessor1_Model01 ## 3 3 8 pr_auc binary 0.929 10 0.00385 Preprocessor1_Model02 ## 4 3 8 roc_auc binary 0.822 10 0.00635 Preprocessor1_Model02 ## 5 3 2 pr_auc binary 0.927 10 0.00378 Preprocessor1_Model03 ## 6 3 2 roc_auc binary 0.818 10 0.00653 Preprocessor1_Model03 ## 7 5 4 pr_auc binary 0.921 10 0.00359 Preprocessor1_Model04 ## 8 5 4 roc_auc binary 0.802 10 0.00682 Preprocessor1_Model04 ## 9 3 13 pr_auc binary 0.929 10 0.00397 Preprocessor1_Model05 ## 10 3 13 roc_auc binary 0.824 10 0.00640 Preprocessor1_Model05 ## # … with 24 more rows En la siguiente gráfica observamos las distintas métricas de error asociados a los hiperparámetros elegidos. autoplot(rf_tune_result, metric = &quot;pr_auc&quot;) autoplot(rf_tune_result, metric = &quot;roc_auc&quot;) show_best(rf_tune_result, n = 10, metric = &quot;pr_auc&quot;) ## # A tibble: 10 × 8 ## mtry min_n .metric .estimator mean n std_err .config ## &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 2 10 pr_auc binary 0.934 10 0.00363 Preprocessor1_Model08 ## 2 2 3 pr_auc binary 0.934 10 0.00375 Preprocessor1_Model12 ## 3 3 15 pr_auc binary 0.930 10 0.00374 Preprocessor1_Model09 ## 4 3 16 pr_auc binary 0.930 10 0.00372 Preprocessor1_Model01 ## 5 3 13 pr_auc binary 0.929 10 0.00397 Preprocessor1_Model05 ## 6 3 8 pr_auc binary 0.929 10 0.00385 Preprocessor1_Model02 ## 7 3 2 pr_auc binary 0.927 10 0.00378 Preprocessor1_Model03 ## 8 3 4 pr_auc binary 0.927 10 0.00396 Preprocessor1_Model11 ## 9 5 14 pr_auc binary 0.925 10 0.00353 Preprocessor1_Model10 ## 10 4 12 pr_auc binary 0.924 10 0.00367 Preprocessor1_Model07 Paso 8: Selección de modelo a usar best_rf_model_cla &lt;- select_best(rf_tune_result, metric = &quot;pr_auc&quot;) best_rf_model_cla ## # A tibble: 1 × 3 ## mtry min_n .config ## &lt;int&gt; &lt;int&gt; &lt;chr&gt; ## 1 2 10 Preprocessor1_Model08 rf_classification_best_1se_model &lt;- rf_tune_result %&gt;% select_by_one_std_err(metric = &quot;roc_auc&quot;, &quot;roc_auc&quot;) rf_classification_best_1se_model ## # A tibble: 1 × 10 ## mtry min_n .metric .estimator mean n std_err .config .best .bound ## &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2 10 roc_auc binary 0.836 10 0.00624 Preprocessor1… 0.836 0.830 Paso 9: Ajuste de modelo final con todos los datos (Vuelve al paso 2, si es necesario) final_rf_model_cla &lt;- rforest_workflow %&gt;% finalize_workflow(best_rf_model_cla) %&gt;% parsnip::fit(data = telco_train) Este último objeto es el modelo final entrenado, el cual contiene toda la información del pre-procesamiento de datos, por lo que en caso de ponerse en producción el modelo, sólo se necesita de este último elemento para poder realizar nuevas predicciones. Antes de pasar al siguiente paso, es importante validar que hayamos hecho un uso correcto de las variables predictivas. En este momento es posible detectar variables que no estén aportando valor o variables que no debiéramos estar usando debido a que cometeríamos data leakage. Para enfrentar esto, ayuda estimar y ordenar el valor de importancia del modelo. Después de entrenar un modelo, es natural preguntarse qué variables tienen el mayor poder predictivo. Las variables de gran importancia son impulsoras del resultado y sus valores tienen un impacto significativo en los valores del resultado. Por el contrario, las variables con poca importancia pueden omitirse de un modelo, lo que lo hace más simple y rápido de ajustar y predecir. library(vip) final_rf_model_cla %&gt;% extract_fit_parsnip() %&gt;% vip::vip() + ggtitle(&quot;Importancia de las variables&quot;)+ theme_minimal() Paso 10: Validar poder predictivo con datos de prueba Imaginemos por un momento que pasa un mes de tiempo desde que hicimos nuestro modelo, es hora de ponerlo a prueba prediciendo valores de nuevos elementos: results_cla &lt;- predict(final_rf_model_cla, telco_test, type = &#39;prob&#39;) %&gt;% dplyr::bind_cols(Churn = telco_test$Churn, .) %&gt;% mutate(Churn = factor(Churn, levels = c(&#39;Yes&#39;, &#39;No&#39;), labels = c(&#39;Yes&#39;, &#39;No&#39;))) results_cla ## # A tibble: 2,113 × 3 ## Churn .pred_No .pred_Yes ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 No 0.936 0.0636 ## 2 Yes 0.373 0.627 ## 3 No 0.635 0.365 ## 4 No 0.982 0.0176 ## 5 No 0.905 0.0955 ## 6 No 0.576 0.424 ## 7 No 0.973 0.0268 ## 8 No 0.776 0.224 ## 9 Yes 0.551 0.449 ## 10 No 0.982 0.0183 ## # … with 2,103 more rows bind_rows( roc_auc(results_cla, truth = Churn, estimate = .pred_Yes), pr_auc(results_cla, truth = Churn, estimate = .pred_Yes) ) ## # A tibble: 2 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 roc_auc binary 0.838 ## 2 pr_auc binary 0.644 pr_curve_data &lt;- pr_curve( results_cla, truth = Churn, estimate = .pred_Yes ) pr_curve_data ## # A tibble: 1,945 × 3 ## .threshold recall precision ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Inf 0 1 ## 2 0.855 0.00177 1 ## 3 0.855 0.00353 1 ## 4 0.851 0.00530 1 ## 5 0.822 0.00707 1 ## 6 0.821 0.00707 0.8 ## 7 0.821 0.00883 0.833 ## 8 0.816 0.0106 0.857 ## 9 0.813 0.0124 0.875 ## 10 0.809 0.0141 0.889 ## # … with 1,935 more rows roc_curve_data &lt;- roc_curve( results_cla, truth = Churn, estimate = .pred_Yes ) roc_curve_data ## # A tibble: 1,946 × 3 ## .threshold specificity sensitivity ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -Inf 0 1 ## 2 0.00646 0 1 ## 3 0.00647 0.00259 1 ## 4 0.00648 0.00323 1 ## 5 0.00649 0.00582 1 ## 6 0.00652 0.0200 1 ## 7 0.00653 0.0297 1 ## 8 0.00656 0.0304 1 ## 9 0.00659 0.0310 1 ## 10 0.00661 0.0323 1 ## # … with 1,936 more rows pr_curve_plot &lt;- pr_curve_data %&gt;% ggplot(aes(x = recall, y = precision)) + geom_abline(slope = -1, intercept = 1) + geom_path(size = 1, colour = &#39;lightblue&#39;) + ylim(0, 1) + coord_equal() + ggtitle(&quot;Precision vs Recall&quot;)+ theme_minimal() pr_curve_plot roc_curve_plot &lt;- roc_curve_data %&gt;% ggplot(aes(x = 1 - specificity, y = sensitivity)) + geom_path(size = 1, colour = &#39;lightblue&#39;) + geom_abline() + coord_equal() + ggtitle(&quot;ROC Curve&quot;)+ theme_minimal() roc_curve_plot Pueden usar la app de shiny que nos permite jugar con el threshold de clasificación para tomar la mejor decisión. 9.7 Boosting Tradicionalmente, la construcción de una aplicación de aprendizaje automático consistía en tomar un solo estimador, es decir: Un regresor logístico Un árbol de decisión Una red neuronal artificial Un conjunto de vecinos cercanos Para posteriormente ser entrenado por un conjunto de datos. Luego nacieron los métodos de conjunto, los cuales pueden describirse como técnicas que utilizan un grupo de modelos “débiles” juntos, con el fin de crear uno más fuerte y agregado. El Boosting consiste en la idea de filtrar o ponderar los datos que se utilizan para capacitar a nuestro conjunto de modelos “débiles”, para que cada nuevo modelo pondere o “solo se entrene” con observaciones que han sido mal clasificadas por los anteriores modelos. Al hacer esto, nuestro conjunto de modelos aprende a hacer predicciones precisas sobre todo tipo de datos, no solo sobre las observaciones más comunes o fáciles. Además, si uno de los modelos individuales es muy malo para hacer predicciones sobre algún tipo de observación, no importa, ya que los otros \\(N - 1\\) modelos probablemente lo compensarán. Como se puede ver en la imagen anterior, en boosting el conjunto de datos se pondera (representado por los diferentes tamaños de los datos), de modo que las observaciones que fueron clasificadas incorrectamente por el clasificador \\(n\\) reciben más importancia en el entrenamiento del modelo \\(n + 1\\). En general, los métodos de conjunto reducen el sesgo y la varianza de nuestros modelos de aprendizaje automático. ¡¡ RECORDAR !! Los modelos bootstrap buscan aprender lentamente patrones relevantes a lo largo de muchas iteraciones, de forma que se vaya haciendo un ajuste lento pero preciso. El proceso de entrenamiento depende del algoritmo boosting que estemos usando (Adaboost, LigthGBM, XGBoost, \\(\\dots\\)), pero generalmente sigue este patrón: Todas las muestras de datos comienzan con los mismos pesos. Estas muestras se utilizan para entrenar un modelo individual (digamos un árbol de decisión). Se calcula el error de predicción para cada muestra, aumentando los pesos de aquellas muestras que han tenido un error mayor, para hacerlas más importantes para el entrenamiento del siguiente modelo individual. Dependiendo de qué tan bien le fue a este modelo individual en sus predicciones, se le asigna una importancia/peso. Los datos ponderados se pasan al modelo posterior y se repiten lo pasos 2) y 3). Este paso se repite hasta que se haya alcanzado un cierto número de modelos o hasta que el error esté por debajo de un cierto umbral. En algunos casos, los modelos de boosting se entrenan con un peso fijo específico para cada modelo (llamado tasa de aprendizaje) y en lugar de dar a cada muestra un peso individual, los modelos se entrenan tratando de predecir las diferencias entre las predicciones anteriores en las muestras y los valores reales de la variable objetivo. Esta diferencia es conocida como residuales. La forma de ajustar el modelo sigue los siguientes pasos: Se fija \\(\\hat{f}(x)=0\\) y \\(r_i=y_i\\) para todos los elementos del conjunto de entrenamiento Para \\(b=1,2,...,B\\), repetir: Ajustar un árbol \\(\\hat{f}^b\\) al conjunto de entrenamiento \\((X, r)\\) Actualizar el ajuste \\(\\hat{f}(x)\\) al añadir una nueva versión restringida de un nuevo árbol: \\[\\hat{F}_b(X) \\leftarrow \\hat{F}_{b-1}(X) + \\alpha_b\\hat{h}_b(X, r_{b-1})\\] c) Actualizar los residuos: \\[r_b \\leftarrow r_{b-1} - \\alpha_b\\hat{f}^b(x_i)\\] Resultado del modelo Boosting: \\[\\hat{F}=\\sum_{b=1}^{B}\\alpha_b\\hat{F}_b(x)\\] Para calcular \\(\\alpha_b\\) en cada iteración, se usa la siguiente fórmula: \\[\\underset{\\alpha}{\\operatorname{argmin}}=\\sum_{i=1}^{b}{L(Y_i, \\hat{F}_{i-1}(X_i)+\\alpha \\hat{h}_i(X_i, r_{i-1}))}\\] Donde \\(L(Y, F(X))\\) es una función de pérdida diferenciable. 9.7.1 Predicciones Boosting La forma en que un modelo de boosting hace predicciones sobre nuevos datos es muy simple. Cuando obtenemos una nueva observación con sus características, se pasa a través de cada uno de los modelos individuales, haciendo que cada modelo haga su propia predicción. Luego, teniendo en cuenta el peso de cada uno de estos modelos, todas estas predicciones se escalan y combinan, y se da una predicción global final. 9.7.2 Modelos Boosting XGBoost Abreviatura de eXtreme Gradient Boosting, como en Gradient Boosting, ajustamos los árboles a los residuos de las predicciones de árboles anteriores, sin embargo, en lugar de usar árboles de decisión de tamaño fijo convencionales, XGBoost usa un tipo diferente de árboles. Estos árboles se construyen calculando puntuaciones de similitud entre las observaciones que terminan en un nodo de salida. Además, XGBoost permite la regularización, reduciendo el posible sobre-ajuste de nuestros árboles individuales y, por lo tanto, del modelo de conjunto general. Por último, XGBoost está optimizado para superar el límite de los recursos computacionales de los algoritmos de árbol impulsados, lo que lo convierte en un algoritmo rápido y de muy alto rendimiento en términos de tiempo y cálculo. 9.8 Implementación de XGB en R 9.8.1 XGBoost para regresión Paso 1: Separación inicial de datos (test, train) library(tidymodels) data(ames) set.seed(4595) ames_split &lt;- initial_split(ames, prop = 0.75) ames_train &lt;- training(ames_split) ames_test &lt;- testing(ames_split) ames_folds &lt;- vfold_cv(ames_train) Contando con datos de entrenamiento, procedemos a realizar el feature engineering para extraer las mejores características que permitirán realizar las estimaciones en el modelo. Paso 2: Pre-procesamiento e ingeniería de variables receta_casas &lt;- recipe( Sale_Price ~ Gr_Liv_Area + TotRms_AbvGrd + Exter_Cond + Bsmt_Cond + Year_Sold + Year_Remod_Add, data = ames_train) %&gt;% step_mutate( Age_House = Year_Sold - Year_Remod_Add, Exter_Cond = forcats::fct_collapse(Exter_Cond, Good = c(&quot;Typical&quot;, &quot;Good&quot;, &quot;Excellent&quot;))) %&gt;% step_relevel(Exter_Cond, ref_level = &quot;Good&quot;) %&gt;% step_normalize(all_numeric_predictors()) %&gt;% step_dummy(all_nominal_predictors()) %&gt;% step_interact(~ matches(&quot;Bsmt_Cond&quot;):TotRms_AbvGrd) %&gt;% prep() receta_casas ## Recipe ## ## Inputs: ## ## role #variables ## outcome 1 ## predictor 6 ## ## Training data contained 2197 data points and no missing data. ## ## Operations: ## ## Variable mutation for ~Year_Sold - Year_Remod_Add, ~forcats::fct... [trained] ## Re-order factor level to ref_level for Exter_Cond [trained] ## Centering and scaling for Gr_Liv_Area, TotRms_AbvGrd, Year_Sold, Year_Rem... [trained] ## Dummy variables from Exter_Cond, Bsmt_Cond [trained] ## Interactions with (Bsmt_Cond_Fair + Bsmt_Cond_Good + Bsmt_Cond_No_Ba... [trained] Recordemos que la función recipe() solo son los pasos a seguir, necesitamos usar la función prep() que nos devuelve una receta actualizada con las estimaciones y la función juice() que nos devuelve la matriz de diseño. Paso 3: Selección de tipo de modelo con hiperparámetros iniciales xgboost_reg_model &lt;- boost_tree( mode = &quot;regression&quot;, trees = 500, tree_depth = tune(), min_n = tune(), loss_reduction = tune(), sample_size = tune(), mtry = tune(), learn_rate = tune() ) %&gt;% set_engine( &quot;xgboost&quot;, importance = &quot;impurity&quot; ) xgboost_reg_model ## Boosted Tree Model Specification (regression) ## ## Main Arguments: ## mtry = tune() ## trees = 500 ## min_n = tune() ## tree_depth = tune() ## learn_rate = tune() ## loss_reduction = tune() ## sample_size = tune() ## ## Engine-Specific Arguments: ## importance = impurity ## ## Computational engine: xgboost Paso 4: Inicialización de workflow o pipeline xgboost_reg_workflow &lt;- workflow() %&gt;% add_model(xgboost_reg_model) %&gt;% add_recipe(receta_casas) xgboost_reg_workflow ## ══ Workflow ══════════════════ ## Preprocessor: Recipe ## Model: boost_tree() ## ## ── Preprocessor ────────────── ## 5 Recipe Steps ## ## • step_mutate() ## • step_relevel() ## • step_normalize() ## • step_dummy() ## • step_interact() ## ## ── Model ───────────────────── ## Boosted Tree Model Specification (regression) ## ## Main Arguments: ## mtry = tune() ## trees = 500 ## min_n = tune() ## tree_depth = tune() ## learn_rate = tune() ## loss_reduction = tune() ## sample_size = tune() ## ## Engine-Specific Arguments: ## importance = impurity ## ## Computational engine: xgboost Paso 5: Creación de grid search xgboost_param_grid &lt;- grid_random( tree_depth(range = c(3, 50)), min_n(range = c(2,50)), loss_reduction(range = c(-10, 1.5), trans = log10_trans()), learn_rate(range = c(-6, -0.25), trans = log10_trans()), mtry(range = c(1, 70)), sample_size = sample_prop(), size = 50 ) xgboost_param_grid ## # A tibble: 50 × 6 ## tree_depth min_n loss_reduction learn_rate mtry sample_size ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 48 47 1.40e- 9 0.0000118 34 0.148 ## 2 33 14 3.14e-10 0.0000234 23 0.341 ## 3 26 31 7.83e- 6 0.276 24 0.868 ## 4 40 41 3.20e- 3 0.0523 42 0.574 ## 5 30 29 1.97e+ 0 0.000357 56 0.574 ## 6 20 25 1.03e- 6 0.000281 58 0.696 ## 7 10 34 6.78e- 6 0.00131 42 0.834 ## 8 11 33 1.13e-10 0.000630 43 0.947 ## 9 24 41 4.81e- 9 0.130 24 0.418 ## 10 23 50 9.23e- 9 0.0000449 19 0.476 ## # … with 40 more rows Paso 6: Entrenamiento de modelos con hiperparámetros definidos UseCores &lt;- detectCores() - 1 cluster &lt;- makeCluster(UseCores) registerDoParallel(cluster) ctrl_grid &lt;- control_grid(save_pred = T, verbose = T) xgb1 &lt;- Sys.time() xgboost_reg_tune_result &lt;- tune_grid( xgboost_reg_workflow, resamples = ames_folds, grid = xgboost_param_grid, metrics = metric_set(rmse, mae, mape, rsq), control = ctrl_grid ) xgb2 &lt;- Sys.time(); xgb2 - xgb1 stopCluster(cluster) xgboost_reg_tune_result %&gt;% saveRDS(&quot;models/xgboost_model_reg.rds&quot;) Paso 7: Análisis de métricas de error e hiperparámetros (Vuelve al paso 3, si es necesario) xgboost_reg_tune_result &lt;- readRDS(&quot;models/xgboost_model_reg.rds&quot;) collect_metrics(xgboost_reg_tune_result) ## # A tibble: 200 × 12 ## mtry min_n tree_depth learn_…¹ loss_…² sampl…³ .metric .esti…⁴ mean n ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; ## 1 1 12 18 0.0837 1.85e+1 0.952 mae standa… 2.88e+4 10 ## 2 1 12 18 0.0837 1.85e+1 0.952 mape standa… 1.71e+1 10 ## 3 1 12 18 0.0837 1.85e+1 0.952 rmse standa… 4.29e+4 10 ## 4 1 12 18 0.0837 1.85e+1 0.952 rsq standa… 7.13e-1 10 ## 5 18 12 49 0.00421 3.92e-7 0.457 mae standa… 3.45e+4 10 ## 6 18 12 49 0.00421 3.92e-7 0.457 mape standa… 1.85e+1 10 ## 7 18 12 49 0.00421 3.92e-7 0.457 rmse standa… 4.98e+4 10 ## 8 18 12 49 0.00421 3.92e-7 0.457 rsq standa… 7.10e-1 10 ## 9 8 9 16 0.00213 3.95e-6 0.956 mae standa… 6.49e+4 10 ## 10 8 9 16 0.00213 3.95e-6 0.956 mape standa… 3.36e+1 10 ## # … with 190 more rows, 2 more variables: std_err &lt;dbl&gt;, .config &lt;chr&gt;, and ## # abbreviated variable names ¹​learn_rate, ²​loss_reduction, ³​sample_size, ## # ⁴​.estimator En la siguiente gráfica observamos las distintas métricas de error asociados a los hiperparámetros elegidos: xgboost_reg_tune_result %&gt;% autoplot() show_best(xgboost_reg_tune_result, n = 10, metric = &quot;rsq&quot;) %&gt;% select(mtry:sample_size, mean:std_err, -n) ## # A tibble: 10 × 8 ## mtry min_n tree_depth learn_rate loss_reduction sample_size mean std_err ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 30 8 7 0.00722 1.52e- 9 0.237 0.718 0.0134 ## 2 1 12 18 0.0837 1.85e+ 1 0.952 0.713 0.0120 ## 3 50 13 41 0.00779 6.37e- 2 0.892 0.713 0.0114 ## 4 70 4 15 0.00277 2.97e- 4 0.301 0.712 0.0126 ## 5 18 12 49 0.00421 3.92e- 7 0.457 0.710 0.0138 ## 6 13 13 14 0.00331 5.70e- 8 0.406 0.706 0.0138 ## 7 8 9 16 0.00213 3.95e- 6 0.956 0.705 0.0113 ## 8 11 13 26 0.00294 2.90e- 5 0.602 0.705 0.0131 ## 9 47 19 48 0.0250 1.44e- 6 0.742 0.704 0.0129 ## 10 39 14 42 0.00337 8.06e-10 0.700 0.703 0.0137 Paso 8: Selección de modelo a usar best_xgboost_reg_model &lt;- select_best(xgboost_reg_tune_result, metric = &quot;rmse&quot;) best_xgboost_reg_model ## # A tibble: 1 × 7 ## mtry min_n tree_depth learn_rate loss_reduction sample_size .config ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 30 8 7 0.00722 0.00000000152 0.237 Preprocessor1_Mo… best_xgboost_reg_1se_model &lt;- xgboost_reg_tune_result %&gt;% select_by_one_std_err(metric = &quot;rmse&quot;, &quot;rmse&quot;) best_xgboost_reg_1se_model ## # A tibble: 1 × 14 ## mtry min_n tree_depth learn_rate loss_…¹ sampl…² .metric .esti…³ mean n ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; ## 1 1 12 18 0.0837 18.5 0.952 rmse standa… 42876. 10 ## # … with 4 more variables: std_err &lt;dbl&gt;, .config &lt;chr&gt;, .best &lt;dbl&gt;, ## # .bound &lt;dbl&gt;, and abbreviated variable names ¹​loss_reduction, ²​sample_size, ## # ³​.estimator Paso 9: Ajuste de modelo final con todos los datos (Vuelve al paso 2, si es necesario) final_xgboost_reg_model &lt;- xgboost_reg_workflow %&gt;% #finalize_workflow(best_xgboost_model) %&gt;% finalize_workflow(best_xgboost_reg_1se_model) %&gt;% fit(data = ames_train) ## [23:56:43] WARNING: amalgamation/../src/learner.cc:627: ## Parameters: { &quot;importance&quot; } might not be used. ## ## This could be a false alarm, with some parameters getting used by language bindings but ## then being mistakenly passed down to XGBoost core, or some parameter actually being used ## but getting flagged wrongly here. Please open an issue if you find any such cases. final_xgboost_reg_model ## ══ Workflow [trained] ════════ ## Preprocessor: Recipe ## Model: boost_tree() ## ## ── Preprocessor ────────────── ## 5 Recipe Steps ## ## • step_mutate() ## • step_relevel() ## • step_normalize() ## • step_dummy() ## • step_interact() ## ## ── Model ───────────────────── ## ##### xgb.Booster ## raw: 492.6 Kb ## call: ## xgboost::xgb.train(params = list(eta = 0.0836732313996573, max_depth = 18L, ## gamma = 18.5133928295454, colsample_bytree = 1, colsample_bynode = 0.0588235294117647, ## min_child_weight = 12L, subsample = 0.951664445269853), data = x$data, ## nrounds = 500, watchlist = x$watchlist, verbose = 0, importance = &quot;impurity&quot;, ## nthread = 1, objective = &quot;reg:squarederror&quot;) ## params (as set within xgb.train): ## eta = &quot;0.0836732313996573&quot;, max_depth = &quot;18&quot;, gamma = &quot;18.5133928295454&quot;, colsample_bytree = &quot;1&quot;, colsample_bynode = &quot;0.0588235294117647&quot;, min_child_weight = &quot;12&quot;, subsample = &quot;0.951664445269853&quot;, importance = &quot;impurity&quot;, nthread = &quot;1&quot;, objective = &quot;reg:squarederror&quot;, validate_parameters = &quot;TRUE&quot; ## xgb.attributes: ## niter ## callbacks: ## cb.evaluation.log() ## # of features: 17 ## niter: 500 ## nfeatures : 17 ## evaluation_log: ## iter training_rmse ## 1 183103.65 ## 2 170225.79 ## --- ## 499 37557.28 ## 500 37555.72 Como hemos hablado anteriormente, este último objeto es el modelo final entrenado, el cual contiene toda la información del pre-procesamiento de datos, por lo que en caso de ponerse en producción el modelo, sólo se necesita de este último elemento para poder realizar nuevas predicciones. Antes de pasar al siguiente paso, es importante validar que hayamos hecho un uso correcto de las variables predictivas. En este momento es posible detectar variables que no estén aportando valor o variables que no debiéramos estar usando debido a que cometeríamos data leakage. Para enfrentar esto, ayuda estimar y ordenar el valor de importancia del modelo library(vip) final_xgboost_reg_model %&gt;% extract_fit_parsnip() %&gt;% vip::vip(num_features = 25) + ggtitle(&quot;Importancia de las variables&quot;) Paso 10: Validar poder predictivo con datos de prueba Imaginemos por un momento que pasa un mes de tiempo desde que hicimos nuestro modelo, es hora de ponerlo a prueba prediciendo valores de nuevos elementos: results &lt;- predict(final_xgboost_reg_model, ames_test) %&gt;% dplyr::bind_cols(Sale_Price = ames_test$Sale_Price) %&gt;% dplyr::rename(pred_xgb_reg = .pred) results ## # A tibble: 733 × 2 ## pred_xgb_reg Sale_Price ## &lt;dbl&gt; &lt;int&gt; ## 1 121063. 105000 ## 2 172835. 185000 ## 3 177081. 180400 ## 4 95956. 141000 ## 5 224212. 210000 ## 6 201056. 216000 ## 7 184153. 149900 ## 8 133850. 105500 ## 9 133850. 88000 ## 10 159376. 146000 ## # … with 723 more rows multi_metric &lt;- metric_set(rmse, rsq, mae, mape, ccc) multi_metric(results, truth = Sale_Price, estimate = pred_xgb_reg) %&gt;% mutate(.estimate = round(.estimate, 2)) ## # A tibble: 5 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 44012. ## 2 rsq standard 0.7 ## 3 mae standard 29968. ## 4 mape standard 17.8 ## 5 ccc standard 0.82 results %&gt;% ggplot(aes(x = pred_xgb_reg, y = Sale_Price)) + geom_point() + geom_abline(color = &quot;red&quot;) + xlab(&quot;Prediction&quot;) + ylab(&quot;Observation&quot;) + ggtitle(&quot;Comparisson&quot;) 9.8.2 XGBoost para clasificación Paso 1: Separación inicial de datos (test, train) telco &lt;- read_csv(&quot;data/Churn.csv&quot;) set.seed(1234) telco_split &lt;- initial_split(telco, prop = .70) telco_train &lt;- training(telco_split) telco_test &lt;- testing(telco_split) telco_folds &lt;- vfold_cv(telco_train) telco_folds ## # 10-fold cross-validation ## # A tibble: 10 × 2 ## splits id ## &lt;list&gt; &lt;chr&gt; ## 1 &lt;split [4437/493]&gt; Fold01 ## 2 &lt;split [4437/493]&gt; Fold02 ## 3 &lt;split [4437/493]&gt; Fold03 ## 4 &lt;split [4437/493]&gt; Fold04 ## 5 &lt;split [4437/493]&gt; Fold05 ## 6 &lt;split [4437/493]&gt; Fold06 ## 7 &lt;split [4437/493]&gt; Fold07 ## 8 &lt;split [4437/493]&gt; Fold08 ## 9 &lt;split [4437/493]&gt; Fold09 ## 10 &lt;split [4437/493]&gt; Fold10 Paso 2: Pre-procesamiento e ingeniería de variables telco_rec &lt;- recipe( Churn ~ customerID + TotalCharges + MonthlyCharges + SeniorCitizen + Contract, data = telco_train) %&gt;% update_role(customerID, new_role = &quot;id variable&quot;) %&gt;% step_mutate(Contract = as.factor(Contract)) %&gt;% step_impute_median(all_numeric_predictors()) %&gt;% step_normalize(all_numeric_predictors()) %&gt;% step_dummy(all_nominal_predictors()) %&gt;% prep() telco_rec ## Recipe ## ## Inputs: ## ## role #variables ## id variable 1 ## outcome 1 ## predictor 4 ## ## Training data contained 4930 data points and 10 incomplete rows. ## ## Operations: ## ## Variable mutation for ~as.factor(Contract) [trained] ## Median imputation for TotalCharges, MonthlyCharges, SeniorCitizen [trained] ## Centering and scaling for TotalCharges, MonthlyCharges, SeniorCitizen [trained] ## Dummy variables from Contract [trained] Paso 3: Selección de tipo de modelo con hiperparámetros iniciales xgboost_model &lt;- boost_tree( mode = &quot;classification&quot;, trees = 500, tree_depth = tune(), min_n = tune(), loss_reduction = tune(), sample_size = tune(), mtry = tune(), learn_rate = tune() ) %&gt;% set_engine( &quot;xgboost&quot;, importance = &quot;impurity&quot; ) xgboost_model ## Boosted Tree Model Specification (classification) ## ## Main Arguments: ## mtry = tune() ## trees = 500 ## min_n = tune() ## tree_depth = tune() ## learn_rate = tune() ## loss_reduction = tune() ## sample_size = tune() ## ## Engine-Specific Arguments: ## importance = impurity ## ## Computational engine: xgboost Paso 4: Inicialización de workflow o pipeline xgboost_workflow &lt;- workflow() %&gt;% add_model(xgboost_model) %&gt;% add_recipe(telco_rec) xgboost_workflow ## ══ Workflow ══════════════════ ## Preprocessor: Recipe ## Model: boost_tree() ## ## ── Preprocessor ────────────── ## 4 Recipe Steps ## ## • step_mutate() ## • step_impute_median() ## • step_normalize() ## • step_dummy() ## ## ── Model ───────────────────── ## Boosted Tree Model Specification (classification) ## ## Main Arguments: ## mtry = tune() ## trees = 500 ## min_n = tune() ## tree_depth = tune() ## learn_rate = tune() ## loss_reduction = tune() ## sample_size = tune() ## ## Engine-Specific Arguments: ## importance = impurity ## ## Computational engine: xgboost Paso 5: Creación de grid search xgboost_param_grid &lt;- grid_random( tree_depth(range = c(2, 30)), min_n(range = c(2,50)), loss_reduction(range = c(-10, 1.5), trans = log10_trans()), learn_rate(range = c(-6, -0.25), trans = log10_trans()), mtry(range = c(1, 20)), sample_size = sample_prop(), size = 50 ) xgboost_param_grid ## # A tibble: 50 × 6 ## tree_depth min_n loss_reduction learn_rate mtry sample_size ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 15 3 3.25e- 5 0.00378 7 0.779 ## 2 17 34 4.81e- 3 0.00128 15 0.495 ## 3 24 49 2.80e-10 0.00503 12 0.206 ## 4 25 47 3.32e- 7 0.542 13 0.331 ## 5 19 49 5.44e- 4 0.00887 3 0.757 ## 6 15 39 1.79e+ 0 0.000173 2 0.955 ## 7 24 40 1.81e+ 0 0.0519 9 0.571 ## 8 14 23 2.11e- 4 0.337 1 0.129 ## 9 25 43 4.26e- 1 0.00000650 1 0.670 ## 10 30 14 1.61e- 2 0.00233 14 0.636 ## # … with 40 more rows Paso 6: Entrenamiento de modelos con hiperparámetros definidos library(doParallel) UseCores &lt;- detectCores() - 1 cluster &lt;- makeCluster(UseCores) registerDoParallel(cluster) ctrl_grid &lt;- control_grid(save_pred = T, verbose = T) xgbt1 &lt;- Sys.time() xgboost_tune_result &lt;- tune_grid( xgboost_workflow, resamples = telco_folds, grid = xgboost_param_grid, metrics = metric_set(roc_auc, pr_auc) ) xgb2 &lt;- Sys.time(); xgb2 - xgbt1 stopCluster(cluster) xgboost_tune_result %&gt;% saveRDS(&quot;models/xgboost_model_classification.rds&quot;) Paso 7: Análisis de métricas de error e hiperparámetros (Vuelve al paso 3, si es necesario) xgboost_tune_result &lt;- readRDS(&quot;models/xgboost_model_classification.rds&quot;) collect_metrics(xgboost_tune_result) ## # A tibble: 100 × 12 ## mtry min_n tree_depth learn_r…¹ loss_r…² sampl…³ .metric .esti…⁴ mean n ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; ## 1 7 3 15 0.00378 3.25e- 5 0.779 pr_auc binary 0.930 10 ## 2 7 3 15 0.00378 3.25e- 5 0.779 roc_auc binary 0.825 10 ## 3 15 34 17 0.00128 4.81e- 3 0.495 pr_auc binary 0.931 10 ## 4 15 34 17 0.00128 4.81e- 3 0.495 roc_auc binary 0.831 10 ## 5 12 49 24 0.00503 2.80e-10 0.206 pr_auc binary 0.918 10 ## 6 12 49 24 0.00503 2.80e-10 0.206 roc_auc binary 0.810 10 ## 7 13 47 25 0.542 3.32e- 7 0.331 pr_auc binary 0.920 10 ## 8 13 47 25 0.542 3.32e- 7 0.331 roc_auc binary 0.815 10 ## 9 3 49 19 0.00887 5.44e- 4 0.757 pr_auc binary 0.935 10 ## 10 3 49 19 0.00887 5.44e- 4 0.757 roc_auc binary 0.838 10 ## # … with 90 more rows, 2 more variables: std_err &lt;dbl&gt;, .config &lt;chr&gt;, and ## # abbreviated variable names ¹​learn_rate, ²​loss_reduction, ³​sample_size, ## # ⁴​.estimator En la siguiente gráfica observamos las distintas métricas de error asociados a los hiperparámetros elegidos: autoplot(xgboost_tune_result) show_best(xgboost_tune_result, n = 10, metric = &quot;roc_auc&quot;) ## # A tibble: 10 × 12 ## mtry min_n tree_depth learn_rate loss_…¹ sampl…² .metric .esti…³ mean n ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; ## 1 9 44 18 0.0211 2.21e-7 0.577 roc_auc binary 0.838 10 ## 2 18 19 23 0.00585 6.19e+0 0.685 roc_auc binary 0.838 10 ## 3 3 49 19 0.00887 5.44e-4 0.757 roc_auc binary 0.838 10 ## 4 18 10 26 0.00149 1.92e-9 0.454 roc_auc binary 0.838 10 ## 5 15 4 25 0.0638 1.82e+1 0.754 roc_auc binary 0.837 10 ## 6 14 14 30 0.00233 1.61e-2 0.636 roc_auc binary 0.837 10 ## 7 19 8 19 0.0000372 6.93e-6 0.362 roc_auc binary 0.837 10 ## 8 9 40 24 0.0519 1.81e+0 0.571 roc_auc binary 0.836 10 ## 9 2 3 23 0.0000608 6.74e-5 0.618 roc_auc binary 0.836 10 ## 10 9 7 29 0.00612 2.59e-3 0.551 roc_auc binary 0.836 10 ## # … with 2 more variables: std_err &lt;dbl&gt;, .config &lt;chr&gt;, and abbreviated ## # variable names ¹​loss_reduction, ²​sample_size, ³​.estimator show_best(xgboost_tune_result, n = 10, metric = &quot;pr_auc&quot;) ## # A tibble: 10 × 12 ## mtry min_n tree_depth learn_rate loss_…¹ sampl…² .metric .esti…³ mean n ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; ## 1 9 44 18 0.0211 2.21e-7 0.577 pr_auc binary 0.935 10 ## 2 3 49 19 0.00887 5.44e-4 0.757 pr_auc binary 0.935 10 ## 3 13 46 22 0.0390 1.27e-6 0.866 pr_auc binary 0.935 10 ## 4 16 10 12 0.0133 1.16e-7 0.495 pr_auc binary 0.934 10 ## 5 9 40 24 0.0519 1.81e+0 0.571 pr_auc binary 0.934 10 ## 6 12 13 7 0.0205 7.02e-1 0.561 pr_auc binary 0.934 10 ## 7 5 20 16 0.0399 5.04e-7 0.451 pr_auc binary 0.934 10 ## 8 18 19 23 0.00585 6.19e+0 0.685 pr_auc binary 0.934 10 ## 9 9 7 29 0.00612 2.59e-3 0.551 pr_auc binary 0.934 10 ## 10 14 14 30 0.00233 1.61e-2 0.636 pr_auc binary 0.934 10 ## # … with 2 more variables: std_err &lt;dbl&gt;, .config &lt;chr&gt;, and abbreviated ## # variable names ¹​loss_reduction, ²​sample_size, ³​.estimator Paso 8: Selección de modelo a usar best_xgboost_model &lt;- select_best(xgboost_tune_result, metric = &quot;pr_auc&quot;) best_xgboost_model ## # A tibble: 1 × 7 ## mtry min_n tree_depth learn_rate loss_reduction sample_size .config ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 9 44 18 0.0211 0.000000221 0.577 Preprocessor1_Mo… best_xgboost_model_1se &lt;- xgboost_tune_result %&gt;% select_by_one_std_err(metric = &quot;pr_auc&quot;, &quot;pr_auc&quot;) best_xgboost_model_1se ## # A tibble: 1 × 14 ## mtry min_n tree_depth learn_rate loss_r…¹ sampl…² .metric .esti…³ mean n ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; ## 1 3 49 19 0.00887 0.000544 0.757 pr_auc binary 0.935 10 ## # … with 4 more variables: std_err &lt;dbl&gt;, .config &lt;chr&gt;, .best &lt;dbl&gt;, ## # .bound &lt;dbl&gt;, and abbreviated variable names ¹​loss_reduction, ²​sample_size, ## # ³​.estimator Paso 9: Ajuste de modelo final con todos los datos (Vuelve al paso 2, si es necesario) final_xgboost_model &lt;- xgboost_workflow %&gt;% #finalize_workflow(best_xgboost_model) %&gt;% finalize_workflow(best_xgboost_model_1se) %&gt;% fit(data = telco_test) ## [23:56:49] WARNING: amalgamation/../src/learner.cc:627: ## Parameters: { &quot;importance&quot; } might not be used. ## ## This could be a false alarm, with some parameters getting used by language bindings but ## then being mistakenly passed down to XGBoost core, or some parameter actually being used ## but getting flagged wrongly here. Please open an issue if you find any such cases. final_xgboost_model ## ══ Workflow [trained] ════════ ## Preprocessor: Recipe ## Model: boost_tree() ## ## ── Preprocessor ────────────── ## 4 Recipe Steps ## ## • step_mutate() ## • step_impute_median() ## • step_normalize() ## • step_dummy() ## ## ── Model ───────────────────── ## ##### xgb.Booster ## raw: 448.7 Kb ## call: ## xgboost::xgb.train(params = list(eta = 0.00887062462674066, max_depth = 19L, ## gamma = 0.000544461013295823, colsample_bytree = 1, colsample_bynode = 0.6, ## min_child_weight = 49L, subsample = 0.756512506399304), data = x$data, ## nrounds = 500, watchlist = x$watchlist, verbose = 0, importance = &quot;impurity&quot;, ## nthread = 1, objective = &quot;binary:logistic&quot;) ## params (as set within xgb.train): ## eta = &quot;0.00887062462674066&quot;, max_depth = &quot;19&quot;, gamma = &quot;0.000544461013295823&quot;, colsample_bytree = &quot;1&quot;, colsample_bynode = &quot;0.6&quot;, min_child_weight = &quot;49&quot;, subsample = &quot;0.756512506399304&quot;, importance = &quot;impurity&quot;, nthread = &quot;1&quot;, objective = &quot;binary:logistic&quot;, validate_parameters = &quot;TRUE&quot; ## xgb.attributes: ## niter ## callbacks: ## cb.evaluation.log() ## # of features: 5 ## niter: 500 ## nfeatures : 5 ## evaluation_log: ## iter training_logloss ## 1 0.6895728 ## 2 0.6863896 ## --- ## 499 0.4275463 ## 500 0.4275110 Como hemos hablado anteriormente, este último objeto es el modelo final entrenado, el cual contiene toda la información del pre-procesamiento de datos, por lo que en caso de ponerse en producción el modelo, sólo se necesita de este último elemento para poder realizar nuevas predicciones. Antes de pasar al siguiente paso, es importante validar que hayamos hecho un uso correcto de las variables predictivas. En este momento es posible detectar variables que no estén aportando valor o variables que no debiéramos estar usando debido a que cometeríamos data leakage. Para enfrentar esto, ayuda estimar y ordenar el valor de importancia del modelo. library(vip) final_xgboost_model %&gt;% extract_fit_parsnip() %&gt;% vip::vip() + ggtitle(&quot;Importancia de las variables&quot;) Paso 10: Validar poder predictivo con datos de prueba Imaginemos por un momento que pasa un mes de tiempo desde que hicimos nuestro modelo, es hora de ponerlo a prueba prediciendo valores de nuevos elementos: class_results &lt;- predict(final_xgboost_model, telco_test, type = &quot;prob&quot;) %&gt;% bind_cols(Churn = telco_test$Churn) %&gt;% mutate(Churn = factor(Churn, levels = c(&#39;Yes&#39;, &#39;No&#39;), labels = c(&#39;Yes&#39;, &#39;No&#39;))) class_results ## # A tibble: 2,113 × 3 ## .pred_No .pred_Yes Churn ## &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 0.900 0.100 No ## 2 0.313 0.687 Yes ## 3 0.730 0.270 No ## 4 0.921 0.0788 No ## 5 0.884 0.116 No ## 6 0.576 0.424 No ## 7 0.879 0.121 No ## 8 0.846 0.154 No ## 9 0.537 0.463 Yes ## 10 0.922 0.0779 No ## # … with 2,103 more rows bind_rows( roc_auc(class_results, truth = Churn, estimate = .pred_Yes), pr_auc(class_results, truth = Churn, estimate = .pred_Yes) ) ## # A tibble: 2 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 roc_auc binary 0.845 ## 2 pr_auc binary 0.666 A continuación, conoceremos el nivel de sensitividad y especificidad para cada punto de corte: roc_curve_data &lt;- roc_curve( class_results, truth = Churn, estimate = .pred_Yes ) roc_curve_data ## # A tibble: 1,376 × 3 ## .threshold specificity sensitivity ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -Inf 0 1 ## 2 0.0499 0 1 ## 3 0.0499 0.00129 1 ## 4 0.0501 0.00194 1 ## 5 0.0502 0.00517 1 ## 6 0.0504 0.00582 1 ## 7 0.0511 0.00646 1 ## 8 0.0513 0.00711 1 ## 9 0.0515 0.00776 1 ## 10 0.0516 0.00840 1 ## # … with 1,366 more rows A través de estas métricas es posible crear la curva ROC: roc_curve_plot &lt;- roc_curve_data %&gt;% ggplot(aes(x = 1 - specificity, y = sensitivity)) + geom_path(size = 1, colour = &#39;lightblue&#39;) + geom_abline() + coord_equal() + ggtitle(&quot;ROC Curve&quot;)+ theme_minimal() roc_curve_plot De igual manera, podemos calcular la precisión y cobertura para cada punte de corte: pr_curve_data &lt;- pr_curve( class_results, truth = Churn, estimate = .pred_Yes ) pr_curve_data ## # A tibble: 1,375 × 3 ## .threshold recall precision ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Inf 0 1 ## 2 0.753 0.00177 1 ## 3 0.750 0.00707 1 ## 4 0.749 0.0106 1 ## 5 0.746 0.0124 1 ## 6 0.744 0.0141 1 ## 7 0.740 0.0159 1 ## 8 0.736 0.0194 1 ## 9 0.736 0.0212 1 ## 10 0.736 0.0230 0.929 ## # … with 1,365 more rows Y graficar su respectiva curva: pr_curve_plot &lt;- pr_curve_data %&gt;% ggplot(aes(x = recall, y = precision)) + ylim(0, 1) + geom_abline(slope = -1, intercept = 1) + geom_path(size = 1, colour = &#39;lightblue&#39;) + coord_equal() + ggtitle(&quot;Precision vs Recall&quot;)+ theme_minimal() pr_curve_plot "],["regularización-elasticnet.html", "Capítulo 10 Regularización Elasticnet 10.1 Regularización Ridge 10.2 Regularización Lasso 10.3 Comparación entre Ridge y Lasso 10.4 ElasticNet 10.5 Implementación en R", " Capítulo 10 Regularización Elasticnet En muchas técnicas de aprendizaje automático, el aprendizaje consiste en encontrar los coeficientes que minimizan una función de costo. Un modelo estándar de mínimos cuadrados tiende a tener alguna variación, es decir, este modelo no se generalizará bien para un conjunto de datos diferente a sus datos de entrenamiento. La regularización consiste en añadir una penalización a la función de costo. Esta penalización produce modelos más simples que generalizan mejor y evita el riesgo de sobre-ajuste. El procedimiento de ajuste implica una función de pérdida, conocida como suma de cuadrados residual o RSS. Los coeficientes \\(\\beta\\) se eligen de manera que minimicen esta función de pérdida. \\[RSS = \\sum_{i=1}^n\\left(y_i - \\beta_0- \\sum_{i=1}^p \\beta_jx_{ij}\\right)^2\\] Esto ajustará los coeficientes en función de sus datos de entrenamiento. Si hay ruido en los datos de entrenamiento, los coeficientes estimados no se generalizarán bien a los datos futuros. Aquí es donde entra la regularización y reduce o regulariza estas estimaciones aprendidas hacia cero. En esta sección se verán las regularizaciones más usadas en machine learning: Ridge (conocida también como L2) Lasso (también conocida como L1) ElasticNet que combina tanto Lasso como Ridge. Para cada una de estas regularizaciones ajustaremos un modelo de regresión lineal al conjunto de datos de viviendas de Ames con ayuda del paquete de tidymodels llamado parsnip. 10.1 Regularización Ridge En este tipo de regularización RSS se modifica agregando una cantidad de contracción a los coeficientes, los cuales se estiman minimizando esta función. \\(\\lambda\\) es el parámetro de ajuste que decide cuánto queremos penalizar la flexibilidad de el modelo. \\[\\sum_{i=1}^n\\left(y_i - \\beta_0- \\sum_{i=1}^p \\beta_jx_{ij}\\right)^2 + \\lambda \\sum_{j=1}^p \\beta_j^2 = RSS + \\lambda \\sum_{j=1}^p \\beta_j^2\\] El aumento de la flexibilidad de un modelo está representado por el aumento de sus coeficientes, si se desea minimizar la función anterior, los coeficientes deben ser pequeños. Así es como la técnica de regresión de Ridge evita que los coeficientes aumenten demasiado. Además, reduce la asociación estimada de cada variable con la respuesta excepto la intersección \\(\\beta_0\\). Esta intersección es una medida del valor medio de la respuesta cuando \\(x_{i1} = x_{i2} =\\dots= x_{ip} = 0\\). Cuando \\(\\lambda = 0\\), el término de penalización no tiene efecto y las estimaciones serán iguales a mínimos cuadrados. A medida que \\(\\lambda \\rightarrow \\infty\\), el impacto de la penalización por contracción aumenta, y las estimaciones se acercarán a cero. La selección de un buen valor de \\(\\lambda\\) es fundamental. Las estimaciones de coeficientes producidas por este método también se conocen como la norma L2. Nota: Es necesario estandarizar los predictores o llevarlos a la misma escala antes de aplicar esta regularización. 10.2 Regularización Lasso Lasso es otra variación, en la que se minimiza la función RSS. Utiliza \\(|\\beta_j|\\) en lugar de los cuadrados de \\(\\beta\\) como penalización. Las estimaciones de coeficientes producidas por este método también se conocen como la norma L1. \\[\\sum_{i=1}^n\\left(y_i - \\beta_0- \\sum_{i=1}^p \\beta_jx_{ij}\\right)^2 + \\lambda \\sum_{j=1}^p |\\beta_j| = RSS + \\lambda \\sum_{j=1}^p |\\beta_j|\\] Cuando \\(\\lambda = 0\\), el término de penalización no tiene efecto y las estimaciones serán iguales a mínimos cuadrados. A medida que \\(\\lambda \\rightarrow \\infty\\), el impacto de la penalización por contracción aumenta, y las estimaciones se convierten en cero (eliminando variables). Este método de regularización permite eliminar coeficientes con alta variación, lo cual ayuda a la selección de variables. 10.3 Comparación entre Ridge y Lasso La regresión Ridge se puede considerar como la solución de una ecuación, donde la suma de los cuadrados de los coeficientes es menor o igual que \\(s\\), donde \\(s\\) es una constante que existe para cada valor del factor de contracción \\(\\lambda\\) \\[\\beta_1^2 + \\beta_2^2 \\leq s\\] Esto implica que los coeficientes de la regresión Ridge tienen el RSS (función de pérdida) más pequeño para todos los puntos que se encuentran dentro del círculo dado por la función de restricción \\(\\beta_1^2 + \\beta_2^2 \\leq s\\). Y en la regresión Lasso se puede considerar como una ecuación en la que la suma del módulo de coeficientes es menor o igual que \\(s\\). \\[|\\beta_1| + |\\beta_2| \\leq s\\] Esto implica que los coeficientes de lasso tienen la RSS (función de pérdida) más pequeña para todos los puntos que se encuentran dentro del diamante dado por la función de restricción \\(|\\beta_1| + |\\beta_2| \\leq s\\) La imagen de arriba muestra las funciones de restricción (áreas verdes) para Lasso (izquierda) y Ridge (derecha), junto con contornos para RSS (elipse roja). Para un valor muy grande de \\(s\\), las regiones verdes contendrán el centro de la elipse, lo que hará que las estimaciones de los coeficientes de ambas técnicas de regresión sean iguales a las estimaciones de mínimos cuadrados. Pero este no es el caso en la imagen de arriba. En este caso, las estimaciones del coeficiente de regresión de Lasso y Ridge vienen dadas por el primer punto en el que una elipse contacta con la región de restricción. Dado que la regresión Ridge tiene una restricción circular sin puntos agudos, esta intersección generalmente no ocurrirá en un eje, por lo que las estimaciones del coeficiente de regresión de Ridge serán exclusivamente distintas de cero. Sin embargo, la restricción de Lasso tiene esquinas en cada uno de los ejes, por lo que la elipse a menudo intersectará la región de restricción en un eje. Cuando esto ocurre, uno de los coeficientes será igual a cero. En dimensiones más altas, muchas de las estimaciones de coeficientes pueden ser iguales a cero simultáneamente. Desventajas Regresión Ridge: Reducirá los coeficientes de los predictores menos importantes, muy cerca de cero. Pero nunca los hará exactamente cero. En otras palabras, el modelo final incluirá todos los predictores. Regresión Lasso: La penalización L1 tiene el efecto de forzar algunas de las estimaciones de coeficientes a ser exactamente iguales a cero cuando el parámetro de ajuste \\(\\lambda\\) es suficientemente grande. Por lo tanto, este método realiza una selección de variables. 10.4 ElasticNet ElasticNet surgió por primera vez como resultado de la crítica a Lasso, cuya selección de variables puede ser demasiado dependiente de los datos y, por lo tanto, inestable. La solución es combinar las penalizaciones de la regresión de Ridge y Lasso para obtener lo mejor de ambas regularizaciones. ElasticNet tiene como objetivo minimizar la siguiente función de pérdida: \\[\\frac{\\sum_{i=1}^n\\left(y_i - \\beta_0- \\sum_{i=1}^p \\beta_jx_{ij}\\right)^2}{2n} + \\lambda\\left( ({1-\\alpha}) \\sum_{j=1}^p|\\beta_j| + \\alpha \\sum_{j=1}^p \\beta_j ^2\\right)\\] \\[= \\frac{RSS}{2n}+ \\lambda\\left( ({1-\\alpha}) \\sum_{j=1}^p|\\beta_j| + \\alpha \\sum_{j=1}^p \\beta_j ^2\\right)\\] donde \\(\\alpha \\in [0,1]\\) es el parámetro de mezcla entre la regularización Ridge \\((\\alpha = 0)\\) y la regularización Lasso \\((\\alpha = 1)\\). La combinación de ambas penalizaciones suele dar lugar a buenos resultados. Una estrategia frecuentemente utilizada es asignarle casi todo el peso a la penalización L1 ( \\(\\alpha \\approx 1\\)) para conseguir seleccionar predictores y menos peso a la regularización \\(L2\\) para dar cierta estabilidad en el caso de que algunos predictores estén correlacionados. 10.5 Implementación en R 10.5.1 Regresión Utilizando el modelo linear_reg() del paquete parsnip. Hay varios mecanismos que pueden realizar la regularización/penalización, los paquetes glmnet, sparklyr, keras o stan. Usemos el primero aquí. El paquete glmnet solo implementa un método que no es de fórmula, pero parsnip permitirá que se use cualquiera de ellos. Cuando se utiliza la regularización, los predictores deben de centrarse y escalarse primero antes de pasar al modelo. El método de la fórmula no lo hará automáticamente, por lo que tendremos que hacerlo nosotros mismos como se hizo en la sección 4.6 Preparación de conjunto de datos con la receta receta_casas. En R existen dos parámetros que nos permiten hacer la regularización: penalty: Es un número no negativo que representa la cantidad total de regularización (solo glmnet, keras y spark). mixture: Es un número entre cero y uno (inclusivo) que es la proporción de regularización L1 en el modelo. Cuando mixture = 1, es un modelo de Lasso puro, mientras que mixture = 0 indica que se está utilizando un modelo Ridge. Paso 1: Separación inicial de datos ( test, train, KFCV ) library(tidymodels) data(ames) set.seed(4595) ames_split &lt;- initial_split(ames, prop = 0.75) ames_train &lt;- training(ames_split) ames_test &lt;- testing(ames_split) ames_folds &lt;- vfold_cv(ames_train) Contando con datos de entrenamiento, procedemos a realizar el feature engineering para extraer las mejores características que permitirán realizar las estimaciones en el modelo. Paso 2: Pre-procesamiento e ingeniería de variables receta_casas &lt;- recipe( Sale_Price ~ Gr_Liv_Area + TotRms_AbvGrd + Exter_Cond + Bsmt_Cond + Year_Sold + Year_Remod_Add, data = ames_train) %&gt;% step_mutate( Age_House = Year_Sold - Year_Remod_Add, Exter_Cond = forcats::fct_collapse(Exter_Cond, Good = c(&quot;Typical&quot;, &quot;Good&quot;, &quot;Excellent&quot;))) %&gt;% step_relevel(Exter_Cond, ref_level = &quot;Good&quot;) %&gt;% step_normalize(all_numeric_predictors()) %&gt;% step_dummy(all_nominal_predictors()) %&gt;% step_interact(~ matches(&quot;Bsmt_Cond&quot;):TotRms_AbvGrd) %&gt;% prep() receta_casas ## Recipe ## ## Inputs: ## ## role #variables ## outcome 1 ## predictor 6 ## ## Training data contained 2197 data points and no missing data. ## ## Operations: ## ## Variable mutation for ~Year_Sold - Year_Remod_Add, ~forcats::fct... [trained] ## Re-order factor level to ref_level for Exter_Cond [trained] ## Centering and scaling for Gr_Liv_Area, TotRms_AbvGrd, Year_Sold, Year_Rem... [trained] ## Dummy variables from Exter_Cond, Bsmt_Cond [trained] ## Interactions with (Bsmt_Cond_Fair + Bsmt_Cond_Good + Bsmt_Cond_No_Ba... [trained] Recordemos que la función recipe() solo son los pasos a seguir, necesitamos usar la función prep() que nos devuelve una receta actualizada con las estimaciones y la función juice() que nos devuelve la matriz de diseño. Una vez que la receta de transformación de datos está lista, procedemos a implementar el pipeline del modelo de interés. Paso 3: Selección de tipo de modelo con hiperparámetros iniciales # install.packages(&quot;glmnet&quot;) elasticnet_regression_model &lt;- linear_reg( mixture = tune(), penalty = tune()) %&gt;% set_mode(&quot;regression&quot;) %&gt;% set_engine(&quot;glmnet&quot;) Paso 4: Inicialización de workflow o pipeline elasticnet_workflow &lt;- workflow() %&gt;% add_model(elasticnet_regression_model) %&gt;% add_recipe(receta_casas) Paso 5: Creación de grid search set.seed(195628) elasticnet_param_grid &lt;- grid_random( penalty(range = c(-15, 0), trans = log10_trans()), mixture(range = c(0,1)), size = 100 ) ctrl_grid &lt;- control_grid(save_pred = T, verbose = T) Paso 6: Entrenamiento de modelos con hiperparámetros definidos library(doParallel) UseCores &lt;- detectCores() - 1 cluster &lt;- makeCluster(UseCores) registerDoParallel(cluster) # Ajuste de parámetros elt1 &lt;- Sys.time() elasticnet_tune_result &lt;- tune_grid( elasticnet_workflow, resamples = ames_folds, grid = elasticnet_param_grid, metrics = metric_set(mae, mape, rmse, rsq), control = ctrl_grid ) elt2 &lt;- Sys.time(); elt2 - elt1 stopCluster(cluster) elasticnet_tune_result %&gt;% saveRDS(&quot;models/elasticnet_model_reg.rds&quot;) elasticnet_tune_result &lt;- readRDS(&quot;models/elasticnet_model_reg.rds&quot;) Paso 7: Análisis de métricas de error e hiperparámetros (Vuelve al paso 3, si es necesario) Podemos obtener las métricas de cada fold con el siguiente código: collect_metrics(elasticnet_tune_result) ## # A tibble: 400 × 8 ## penalty mixture .metric .estimator mean n std_err .config ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 3.81e-12 0.0139 mae standard 32607. 10 544. Preprocessor1_… ## 2 3.81e-12 0.0139 mape standard 19.1 10 0.268 Preprocessor1_… ## 3 3.81e-12 0.0139 rmse standard 48347. 10 1024. Preprocessor1_… ## 4 3.81e-12 0.0139 rsq standard 0.641 10 0.0176 Preprocessor1_… ## 5 4.32e- 2 0.0160 mae standard 32609. 10 543. Preprocessor1_… ## 6 4.32e- 2 0.0160 mape standard 19.1 10 0.268 Preprocessor1_… ## 7 4.32e- 2 0.0160 rmse standard 48347. 10 1025. Preprocessor1_… ## 8 4.32e- 2 0.0160 rsq standard 0.641 10 0.0177 Preprocessor1_… ## 9 7.31e- 8 0.0361 mae standard 32617. 10 541. Preprocessor1_… ## 10 7.31e- 8 0.0361 mape standard 19.1 10 0.268 Preprocessor1_… ## # … with 390 more rows En la siguiente gráfica observamos las distintas métricas de error asociados a los hiperparámetros elegidos: elasticnet_tune_result %&gt;% autoplot() En la siguiente gráfica observamos el error cuadrático medio de las distintas métricas con distintos números de vecinos. elasticnet_tune_result %&gt;% autoplot(metric = &quot;rsq&quot;) Paso 8: Selección de modelo a usar Con el siguiente código obtenemos los mejores 10 modelos respecto al rmse. show_best(elasticnet_tune_result, n = 10, metric = &quot;rmse&quot;) ## # A tibble: 10 × 8 ## penalty mixture .metric .estimator mean n std_err .config ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 6.18e- 2 0.306 rmse standard 48342. 10 1029. Preprocessor1_Model… ## 2 3.03e-11 0.301 rmse standard 48344. 10 1030. Preprocessor1_Model… ## 3 8.01e- 6 0.301 rmse standard 48344. 10 1030. Preprocessor1_Model… ## 4 9.54e-12 0.246 rmse standard 48346. 10 1030. Preprocessor1_Model… ## 5 1.27e- 8 0.278 rmse standard 48346. 10 1030. Preprocessor1_Model… ## 6 1.62e-15 0.252 rmse standard 48346. 10 1029. Preprocessor1_Model… ## 7 1.91e- 9 0.283 rmse standard 48346. 10 1030. Preprocessor1_Model… ## 8 2.17e-15 0.269 rmse standard 48346. 10 1030. Preprocessor1_Model… ## 9 6.99e- 5 0.283 rmse standard 48346. 10 1030. Preprocessor1_Model… ## 10 3.81e-12 0.0139 rmse standard 48347. 10 1024. Preprocessor1_Model… Ahora obtendremos el modelo que mejor desempeño tiene tomando en cuenta el rmse y haremos las predicciones del conjunto de prueba con este modelo. best_elasticnet_regression_model &lt;- select_best(elasticnet_tune_result, metric = &quot;rmse&quot;) best_elasticnet_regression_model ## # A tibble: 1 × 3 ## penalty mixture .config ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 0.0618 0.306 Preprocessor1_Model027 Paso 9: Ajuste de modelo final con todos los datos (Vuelve al paso 2, si es necesario) final_elasticnet_regression_model &lt;- elasticnet_workflow %&gt;% finalize_workflow(best_elasticnet_regression_model) %&gt;% fit(data = ames_train) Este último objeto es el modelo final entrenado, el cual contiene toda la información del pre-procesamiento de datos, por lo que en caso de ponerse en producción el modelo, sólo se necesita de este último elemento para poder realizar nuevas predicciones. Antes de pasar al siguiente paso, es importante validar que hayamos hecho un uso correcto de las variables predictivas. En este momento es posible detectar variables que no estén aportando valor o variables que no debiéramos estar usando debido a que cometeríamos data leakage. Para enfrentar esto, ayuda estimar y ordenar el valor de importancia de cada variable en el modelo. library(vip) final_elasticnet_regression_model %&gt;% extract_fit_parsnip() %&gt;% vip(geom = &quot;col&quot;) + ggtitle(&quot;Importancia de las variables&quot;) Paso 10: Validar poder predictivo con datos de prueba Imaginemos por un momento que pasa un mes de tiempo desde que hicimos nuestro modelo, es hora de ponerlo a prueba prediciendo valores de nuevos elementos: results &lt;- predict(final_elasticnet_regression_model, ames_test) %&gt;% dplyr::bind_cols(Sale_Price = ames_test$Sale_Price) %&gt;% dplyr::rename(pred_elasticnet_reg = .pred) results ## # A tibble: 733 × 2 ## pred_elasticnet_reg Sale_Price ## &lt;dbl&gt; &lt;int&gt; ## 1 95091. 105000 ## 2 175072. 185000 ## 3 189332. 180400 ## 4 85542. 141000 ## 5 244179. 210000 ## 6 214542. 216000 ## 7 164313. 149900 ## 8 122589. 105500 ## 9 122589. 88000 ## 10 164965. 146000 ## # … with 723 more rows Métricas de desempeño Ahora para calcular las métricas de desempeño usaremos la paquetería MLmetrics. Es posible definir nuestro propio conjunto de métricas que deseamos reportar creando el objeto metric_set: library(MLmetrics) multi_metric &lt;- metric_set(mae, mape, rmse, rsq, ccc) multi_metric(results, truth = Sale_Price, estimate = pred_elasticnet_reg) %&gt;% mutate(.estimate = round(.estimate, 2)) %&gt;% select(-.estimator) ## # A tibble: 5 × 2 ## .metric .estimate ## &lt;chr&gt; &lt;dbl&gt; ## 1 mae 34092 ## 2 mape 20.2 ## 3 rmse 49397 ## 4 rsq 0.63 ## 5 ccc 0.76 results %&gt;% ggplot(aes(x = pred_elasticnet_reg, y = Sale_Price)) + geom_point() + geom_abline(color = &quot;red&quot;) + xlab(&quot;Prediction&quot;) + ylab(&quot;Observation&quot;) + ggtitle(&quot;Comparisson&quot;) 10.5.2 Clasificación Paso 1: Separación inicial de datos (test, train) telco &lt;- read_csv(&quot;data/Churn.csv&quot;) set.seed(1234) telco_split &lt;- initial_split(telco, prop = .70) telco_train &lt;- training(telco_split) telco_test &lt;- testing(telco_split) telco_folds &lt;- vfold_cv(telco_train) telco_folds ## # 10-fold cross-validation ## # A tibble: 10 × 2 ## splits id ## &lt;list&gt; &lt;chr&gt; ## 1 &lt;split [4437/493]&gt; Fold01 ## 2 &lt;split [4437/493]&gt; Fold02 ## 3 &lt;split [4437/493]&gt; Fold03 ## 4 &lt;split [4437/493]&gt; Fold04 ## 5 &lt;split [4437/493]&gt; Fold05 ## 6 &lt;split [4437/493]&gt; Fold06 ## 7 &lt;split [4437/493]&gt; Fold07 ## 8 &lt;split [4437/493]&gt; Fold08 ## 9 &lt;split [4437/493]&gt; Fold09 ## 10 &lt;split [4437/493]&gt; Fold10 Paso 2: Pre-procesamiento e ingeniería de variables telco_rec &lt;- recipe( Churn ~ customerID + TotalCharges + MonthlyCharges + SeniorCitizen + Contract, data = telco_train) %&gt;% update_role(customerID, new_role = &quot;id variable&quot;) %&gt;% step_mutate(Contract = as.factor(Contract)) %&gt;% step_impute_median(all_numeric_predictors()) %&gt;% step_normalize(all_numeric_predictors()) %&gt;% step_dummy(all_nominal_predictors()) %&gt;% prep() telco_rec ## Recipe ## ## Inputs: ## ## role #variables ## id variable 1 ## outcome 1 ## predictor 4 ## ## Training data contained 4930 data points and 10 incomplete rows. ## ## Operations: ## ## Variable mutation for ~as.factor(Contract) [trained] ## Median imputation for TotalCharges, MonthlyCharges, SeniorCitizen [trained] ## Centering and scaling for TotalCharges, MonthlyCharges, SeniorCitizen [trained] ## Dummy variables from Contract [trained] Paso 3: Selección de tipo de modelo con hiperparámetros iniciales elasticnet_class_model &lt;- logistic_reg( mixture = tune(), penalty = tune()) %&gt;% set_mode(&#39;classification&#39;) %&gt;% set_engine(&quot;glmnet&quot;) elasticnet_class_model ## Logistic Regression Model Specification (classification) ## ## Main Arguments: ## penalty = tune() ## mixture = tune() ## ## Computational engine: glmnet Paso 4: Inicialización de workflow o pipeline elasticnet_workflow &lt;- workflow() %&gt;% add_model(elasticnet_class_model) %&gt;% add_recipe(telco_rec) elasticnet_workflow ## ══ Workflow ══════════════════ ## Preprocessor: Recipe ## Model: logistic_reg() ## ## ── Preprocessor ────────────── ## 4 Recipe Steps ## ## • step_mutate() ## • step_impute_median() ## • step_normalize() ## • step_dummy() ## ## ── Model ───────────────────── ## Logistic Regression Model Specification (classification) ## ## Main Arguments: ## penalty = tune() ## mixture = tune() ## ## Computational engine: glmnet Paso 5: Creación de grid search elasticnet_param_grid &lt;- grid_random( penalty( range = c(-4, 0.1), trans = log10_trans() ), dials::mixture(range = c(0, 1)), size = 50 ) elasticnet_param_grid ## # A tibble: 50 × 2 ## penalty mixture ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.000172 0.756 ## 2 0.194 0.460 ## 3 0.0430 0.978 ## 4 0.0102 0.909 ## 5 0.000735 0.635 ## 6 0.00285 0.129 ## 7 0.172 0.965 ## 8 0.0433 0.723 ## 9 0.000138 0.643 ## 10 0.000504 0.239 ## # … with 40 more rows Paso 6: Entrenamiento de modelos con hiperparámetros definidos library(doParallel) UseCores &lt;- detectCores() - 1 cluster &lt;- makeCluster(UseCores) registerDoParallel(cluster) ctrl_grid &lt;- control_grid(save_pred = T, verbose = T) etnt1 &lt;- Sys.time() elasticnet_tune_result &lt;- tune_grid( elasticnet_workflow, resamples = telco_folds, grid = elasticnet_param_grid, metrics = metric_set(roc_auc, pr_auc) ) etn2 &lt;- Sys.time(); etn2 - etnt1 stopCluster(cluster) elasticnet_tune_result %&gt;% saveRDS(&quot;models/elasticnet_class_model.rds&quot;) Paso 7: Análisis de métricas de error e hiperparámetros (Vuelve al paso 3, si es necesario) elasticnet_tune_result &lt;- readRDS(&quot;models/elasticnet_class_model.rds&quot;) collect_metrics(elasticnet_tune_result) ## # A tibble: 100 × 8 ## penalty mixture .metric .estimator mean n std_err .config ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 0.0494 0.000985 pr_auc binary 0.928 10 0.00365 Preprocessor1_Model… ## 2 0.0494 0.000985 roc_auc binary 0.822 10 0.00629 Preprocessor1_Model… ## 3 0.000837 0.0152 pr_auc binary 0.928 10 0.00362 Preprocessor1_Model… ## 4 0.000837 0.0152 roc_auc binary 0.823 10 0.00626 Preprocessor1_Model… ## 5 0.181 0.0240 pr_auc binary 0.927 10 0.00381 Preprocessor1_Model… ## 6 0.181 0.0240 roc_auc binary 0.819 10 0.00643 Preprocessor1_Model… ## 7 0.155 0.0244 pr_auc binary 0.927 10 0.00380 Preprocessor1_Model… ## 8 0.155 0.0244 roc_auc binary 0.819 10 0.00645 Preprocessor1_Model… ## 9 0.0147 0.0368 pr_auc binary 0.929 10 0.00359 Preprocessor1_Model… ## 10 0.0147 0.0368 roc_auc binary 0.823 10 0.00626 Preprocessor1_Model… ## # … with 90 more rows En la siguiente gráfica observamos las distintas métricas de error asociados a los hiperparámetros elegidos: autoplot(elasticnet_tune_result) show_best(elasticnet_tune_result, n = 10, metric = &quot;roc_auc&quot;) ## # A tibble: 10 × 8 ## penalty mixture .metric .estimator mean n std_err .config ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 0.00333 0.0764 roc_auc binary 0.823 10 0.00627 Preprocessor1_Model07 ## 2 0.00561 0.174 roc_auc binary 0.823 10 0.00628 Preprocessor1_Model11 ## 3 0.00513 0.710 roc_auc binary 0.823 10 0.00626 Preprocessor1_Model34 ## 4 0.00283 0.739 roc_auc binary 0.823 10 0.00626 Preprocessor1_Model38 ## 5 0.000202 0.792 roc_auc binary 0.823 10 0.00626 Preprocessor1_Model40 ## 6 0.000216 0.674 roc_auc binary 0.823 10 0.00626 Preprocessor1_Model31 ## 7 0.00272 0.442 roc_auc binary 0.823 10 0.00627 Preprocessor1_Model19 ## 8 0.00383 0.717 roc_auc binary 0.823 10 0.00625 Preprocessor1_Model35 ## 9 0.000101 0.682 roc_auc binary 0.823 10 0.00625 Preprocessor1_Model32 ## 10 0.000232 0.731 roc_auc binary 0.823 10 0.00626 Preprocessor1_Model37 show_best(elasticnet_tune_result, n = 10, metric = &quot;pr_auc&quot;) ## # A tibble: 10 × 8 ## penalty mixture .metric .estimator mean n std_err .config ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 0.0197 0.369 pr_auc binary 0.929 10 0.00357 Preprocessor1_Model18 ## 2 0.0201 0.613 pr_auc binary 0.929 10 0.00358 Preprocessor1_Model29 ## 3 0.0132 0.120 pr_auc binary 0.929 10 0.00359 Preprocessor1_Model08 ## 4 0.0187 0.702 pr_auc binary 0.929 10 0.00357 Preprocessor1_Model33 ## 5 0.00513 0.710 pr_auc binary 0.929 10 0.00361 Preprocessor1_Model34 ## 6 0.0272 0.916 pr_auc binary 0.929 10 0.00357 Preprocessor1_Model46 ## 7 0.00942 0.851 pr_auc binary 0.929 10 0.00360 Preprocessor1_Model43 ## 8 0.0147 0.0368 pr_auc binary 0.929 10 0.00359 Preprocessor1_Model05 ## 9 0.00561 0.174 pr_auc binary 0.929 10 0.00360 Preprocessor1_Model11 ## 10 0.0127 0.923 pr_auc binary 0.929 10 0.00360 Preprocessor1_Model47 Paso 8: Selección de modelo a usar best_elasticnet_class_model &lt;- select_best(elasticnet_tune_result, metric = &quot;pr_auc&quot;) best_elasticnet_class_model ## # A tibble: 1 × 3 ## penalty mixture .config ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 0.0197 0.369 Preprocessor1_Model18 best_elasticnet_class_model_1se &lt;- elasticnet_tune_result %&gt;% select_by_one_std_err(metric = &quot;pr_auc&quot;, &quot;pr_auc&quot;) best_elasticnet_class_model_1se ## # A tibble: 1 × 10 ## penalty mixture .metric .estimator mean n std_err .config .best .bound ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.0494 0.000985 pr_auc binary 0.928 10 0.00365 Preproce… 0.929 0.925 Paso 9: Ajuste de modelo final con todos los datos (Vuelve al paso 2, si es necesario) final_elasticnet_class_model &lt;- elasticnet_workflow %&gt;% #finalize_workflow(best_elasticnet_class_model) %&gt;% finalize_workflow(best_elasticnet_class_model_1se) %&gt;% fit(data = telco_test) Como hemos hablado anteriormente, este último objeto es el modelo final entrenado, el cual contiene toda la información del pre-procesamiento de datos, por lo que en caso de ponerse en producción el modelo, sólo se necesita de este último elemento para poder realizar nuevas predicciones. Antes de pasar al siguiente paso, es importante validar que hayamos hecho un uso correcto de las variables predictivas. En este momento es posible detectar variables que no estén aportando valor o variables que no debiéramos estar usando debido a que cometeríamos data leakage. Para enfrentar esto, ayuda estimar y ordenar el valor de importancia del modelo. library(vip) final_elasticnet_class_model %&gt;% extract_fit_parsnip() %&gt;% vip::vip() + ggtitle(&quot;Importancia de las variables&quot;) Paso 10: Validar poder predictivo con datos de prueba Imaginemos por un momento que pasa un mes de tiempo desde que hicimos nuestro modelo, es hora de ponerlo a prueba prediciendo valores de nuevos elementos: class_results &lt;- predict(final_elasticnet_class_model, telco_test, type = &quot;prob&quot;) %&gt;% bind_cols(Churn = telco_test$Churn) %&gt;% mutate(Churn = factor(Churn, levels = c(&#39;Yes&#39;, &#39;No&#39;), labels = c(&#39;Yes&#39;, &#39;No&#39;))) class_results ## # A tibble: 2,113 × 3 ## .pred_No .pred_Yes Churn ## &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 0.848 0.152 No ## 2 0.442 0.558 Yes ## 3 0.715 0.285 No ## 4 0.922 0.0782 No ## 5 0.921 0.0788 No ## 6 0.543 0.457 No ## 7 0.883 0.117 No ## 8 0.726 0.274 No ## 9 0.700 0.300 Yes ## 10 0.926 0.0738 No ## # … with 2,103 more rows bind_rows( roc_auc(class_results, truth = Churn, estimate = .pred_Yes), pr_auc(class_results, truth = Churn, estimate = .pred_Yes) ) ## # A tibble: 2 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 roc_auc binary 0.824 ## 2 pr_auc binary 0.609 A continuación, conoceremos el nivel de sensitividad y especificidad para cada punto de corte: roc_curve_data &lt;- roc_curve( class_results, truth = Churn, estimate = .pred_Yes ) roc_curve_data ## # A tibble: 2,085 × 3 ## .threshold specificity sensitivity ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -Inf 0 1 ## 2 0.0617 0 1 ## 3 0.0622 0.000646 1 ## 4 0.0625 0.00129 1 ## 5 0.0626 0.00194 1 ## 6 0.0627 0.00259 1 ## 7 0.0629 0.00323 1 ## 8 0.0629 0.00388 1 ## 9 0.0630 0.00452 1 ## 10 0.0630 0.00517 1 ## # … with 2,075 more rows A través de estas métricas es posible crear la curva ROC: roc_curve_plot &lt;- roc_curve_data %&gt;% ggplot(aes(x = 1 - specificity, y = sensitivity)) + geom_path(size = 1, colour = &#39;lightblue&#39;) + geom_abline() + coord_equal() + ggtitle(&quot;ROC Curve&quot;)+ theme_minimal() roc_curve_plot De igual manera, podemos calcular la precisión y cobertura para cada punte de corte: pr_curve_data &lt;- pr_curve( class_results, truth = Churn, estimate = .pred_Yes ) pr_curve_data ## # A tibble: 2,084 × 3 ## .threshold recall precision ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Inf 0 1 ## 2 0.695 0.00177 1 ## 3 0.666 0.00353 1 ## 4 0.660 0.00530 1 ## 5 0.644 0.00530 0.75 ## 6 0.640 0.00707 0.8 ## 7 0.638 0.00883 0.833 ## 8 0.633 0.0106 0.857 ## 9 0.632 0.0124 0.875 ## 10 0.629 0.0141 0.889 ## # … with 2,074 more rows Y graficar su respectiva curva: pr_curve_plot &lt;- pr_curve_data %&gt;% ggplot(aes(x = recall, y = precision)) + ylim(0, 1) + geom_abline(slope = -1, intercept = 1) + geom_path(size = 1, colour = &#39;lightblue&#39;) + coord_equal() + ggtitle(&quot;Precision vs Recall&quot;)+ theme_minimal() pr_curve_plot "],["workflowsets-stacking.html", "Capítulo 11 Workflowsets &amp; Stacking 11.1 Múltiples recetas 11.2 Múltiples modelos 11.3 Creación de workflowset 11.4 Ajuste y evaluación de modelos 11.5 Extracción de modelos 11.6 Stacking", " Capítulo 11 Workflowsets &amp; Stacking Es común no sepamos ni remotamente cuál es el mejor modelo que podríamos implementar al iniciar un proyecto con datos que nunca antes hemos visto. Es posible que un profesional de datos deba seleccionar muchas combinaciones de modelos y pre-procesadores. También es posible tener poco o ningún conocimiento a priori sobre qué método funcionará mejor con un nuevo conjunto de datos. Una buena estrategia es dedicar un esfuerzo inicial a probar una variedad de enfoques de modelado, determinar qué funciona mejor y luego invertir tiempo adicional ajustando / optimizando un pequeño conjunto de modelos. 11.1 Múltiples recetas Algunos modelos requieren predictores que se han centrado y escalado, por lo que algunos flujos de trabajo de modelos requerirán recetas con estos pasos de pre-procesamiento. Para otros modelos, crear interacciones cuadráticas y bi-direccionales. Para estos fines, creamos múltiples recetas: library(tidymodels) data(ames) set.seed(4595) ames_split &lt;- initial_split(ames, prop = 0.75) ames_train &lt;- training(ames_split) ames_test &lt;- testing(ames_split) ames_folds &lt;- vfold_cv(ames_train) Receta Original receta_base &lt;- recipe( Sale_Price ~ Gr_Liv_Area + TotRms_AbvGrd + Exter_Cond + Bsmt_Cond + Year_Sold + Year_Remod_Add, data = ames_train) %&gt;% step_mutate( Age_House = Year_Sold - Year_Remod_Add, Exter_Cond = forcats::fct_collapse(Exter_Cond, Good = c(&quot;Typical&quot;, &quot;Good&quot;, &quot;Excellent&quot;))) %&gt;% step_relevel(Exter_Cond, ref_level = &quot;Good&quot;) %&gt;% step_normalize(all_numeric_predictors()) %&gt;% step_dummy(all_nominal_predictors()) %&gt;% step_interact(~ matches(&quot;Bsmt_Cond&quot;):TotRms_AbvGrd) receta_base_prep &lt;- receta_base %&gt;% prep() Receta KNN: receta_extendida &lt;- recipe( Sale_Price ~ Gr_Liv_Area + TotRms_AbvGrd + Exter_Cond + Bsmt_Cond + Year_Sold + Year_Remod_Add + Year_Remod_Add + Bedroom_AbvGr +Total_Bsmt_SF + Pool_Area + Second_Flr_SF + First_Flr_SF, data = ames_train) %&gt;% step_ratio(Bedroom_AbvGr, denom = denom_vars(Gr_Liv_Area)) %&gt;% step_mutate( Age_House = Year_Sold - Year_Remod_Add, TotalSF = Gr_Liv_Area + Total_Bsmt_SF, AvgRoomSF = Gr_Liv_Area / TotRms_AbvGrd, Pool = ifelse(Pool_Area &gt; 0, 1, 0), Exter_Cond = forcats::fct_collapse(Exter_Cond, Good = c(&quot;Typical&quot;, &quot;Good&quot;, &quot;Excellent&quot;))) %&gt;% step_relevel(Exter_Cond, ref_level = &quot;Good&quot;) %&gt;% step_normalize(all_numeric_predictors()) %&gt;% step_dummy(all_nominal_predictors()) %&gt;% step_interact(~ Second_Flr_SF:First_Flr_SF) %&gt;% step_interact(~ matches(&quot;Bsmt_Cond&quot;):TotRms_AbvGrd) receta_extendida_prep &lt;- receta_extendida %&gt;% prep() Receta Experta: receta_experta &lt;- recipe( Sale_Price ~ Gr_Liv_Area + TotRms_AbvGrd + Exter_Cond + Bsmt_Cond + Condition_1 + Year_Sold + Year_Remod_Add + Year_Remod_Add + Bedroom_AbvGr +Total_Bsmt_SF + Pool_Area + Second_Flr_SF + First_Flr_SF + Full_Bath + Bsmt_Full_Bath + Neighborhood, data = ames_train) %&gt;% step_ratio(Bedroom_AbvGr, denom = denom_vars(Gr_Liv_Area)) %&gt;% step_ratio(Second_Flr_SF, denom = denom_vars(First_Flr_SF)) %&gt;% step_mutate( TotalBaths = Full_Bath + Bsmt_Full_Bath, Age_House = Year_Sold - Year_Remod_Add, Pool = ifelse(Pool_Area &gt; 0,1,0), Exter_Cond = forcats::fct_collapse(Exter_Cond, Good = c(&quot;Typical&quot;, &quot;Good&quot;, &quot;Excellent&quot;)), Neighborhood = forcats::fct_collapse(Neighborhood, NoRidge_GrnHill = c(&quot;Northridge&quot;, &quot;Green_Hills&quot;), ClearCr_Somerst = c(&quot;Clear_Creek&quot;, &quot;Somerset&quot;))) %&gt;% step_relevel(Condition_1, ref_level = &quot;Norm&quot;) %&gt;% step_normalize(all_numeric_predictors()) %&gt;% step_dummy(all_nominal_predictors()) %&gt;% step_interact(~ Age_House:TotRms_AbvGrd) %&gt;% step_interact(~ matches(&quot;Bsmt_Cond&quot;):TotRms_AbvGrd) %&gt;% step_rm( First_Flr_SF, Second_Flr_SF, Year_Remod_Add, Bsmt_Full_Bath, Total_Bsmt_SF, Pool_Area, Gr_Liv_Area ) receta_experta_prep &lt;- receta_experta %&gt;% prep() 11.2 Múltiples modelos Una vez que tenemos suficientes recetas, podemos experimentar con múltiples modelos para poner a prueba. Usaremos los modelos que hemos aprendido a implementar en todo el curso: pacman::p_load( rules, baguette, tune, stacks ) knn_model &lt;- nearest_neighbor( mode = &quot;regression&quot;, neighbors = tune(&quot;K&quot;), dist_power = tune(), weight_func = tune()) %&gt;% set_engine(&quot;kknn&quot;) rforest_model &lt;- rand_forest( mode = &quot;regression&quot;, trees = 500, mtry = tune(), min_n = tune()) %&gt;% set_engine( &quot;ranger&quot;, importance = &quot;impurity&quot; ) xgboost_model &lt;- boost_tree( mode = &quot;regression&quot;, trees = 500, tree_depth = tune(), min_n = tune(), loss_reduction = tune(), sample_size = tune(), mtry = tune(), learn_rate = tune()) %&gt;% set_engine( &quot;xgboost&quot;, importance = &quot;impurity&quot; ) elasticnet_model &lt;- linear_reg( mode = &quot;regression&quot;, penalty = tune(), mixture = tune()) %&gt;% set_engine(&quot;glmnet&quot;) ¿Cómo podemos hacer coincidir estos modelos con las recetas desarrolladas, ajustarlos y luego evaluar su rendimiento de manera eficiente? WORKFLOWSETS ofrece una solución. 11.3 Creación de workflowset Los conjuntos de flujo de trabajo toman listas nombradas de pre-procesadores y especificaciones de modelos y las combinan en un objeto que contiene múltiples flujos de trabajo. Como primer ejemplo de conjunto de flujo de trabajo, combinemos las recetas creadas en la sección anterior. workflow_set_models &lt;- workflow_set( preproc = list( receta_base = receta_base_prep, receta_extendida = receta_extendida_prep, receta_experta = receta_experta_prep ), models = list( knn = knn_model, rf = rforest_model, boost = xgboost_model, elasticnet = elasticnet_model ) ) 11.4 Ajuste y evaluación de modelos Casi todos estos flujos de trabajo contienen parámetros de ajuste. Para evaluar su rendimiento, podemos utilizar las funciones estándar de ajuste o remuestreo (por ejemplo, tune_grid()). La función workflow_map() aplicará la misma función a todos los flujos de trabajo del conjunto; el valor predeterminado es tune_grid(). A continuación se declaran los parámetros para cada modelo y el grid: elasticnet_params &lt;- elasticnet_model %&gt;% parameters() %&gt;% update( penalty = penalty( range = c(-10, 1), trans = log10_trans()), mixture = dials::mixture(range = c(0, 1)) ) knn_params &lt;- knn_model %&gt;% parameters() %&gt;% update( K = dials::neighbors(c(5, 150)), dist_power = dist_power(range = c(1, 3)), weight_func = weight_func(values = c(&quot;rectangular&quot;, &quot;inv&quot;, &quot;gaussian&quot;, &quot;cos&quot;)) ) rforest_params &lt;- rforest_model %&gt;% parameters() %&gt;% update( mtry = finalize(mtry(range = c(15, 80))), min_n = min_n(range = c(3,15)) ) xgboost_params &lt;- xgboost_model %&gt;% parameters() %&gt;% update( min_n = min_n(range = c(5,15)), mtry = finalize(mtry(range = c(5, 80))), tree_depth = tree_depth(range = c(3, 50)), loss_reduction = loss_reduction(range = c(-10, 1.5), trans = log10_trans()), learn_rate = learn_rate(range = c(-6, -0.25), trans = log10_trans()), sample_size = sample_prop() ) # Declaración del grid workflow_tunning_set_models &lt;- workflow_set_models %&gt;% option_add(param_info = knn_params, id = &quot;knn&quot;) %&gt;% option_add(param_info = rforest_params, id = &quot;rf&quot;) %&gt;% option_add(param_info = xgboost_params, id = &quot;xgboost&quot;) %&gt;% option_add(param_info = elasticnet_params, id = &quot;elasticnet&quot;) workflow_tunning_set_models ## # A workflow set/tibble: 12 × ## # 4 ## wflow_id info option result ## &lt;chr&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; ## 1 receta_base_knn &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt; ## 2 receta_base_rf &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt; ## 3 receta_base_boost &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt; ## 4 receta_base_elasticnet &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt; ## 5 receta_extendida_knn &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt; ## 6 receta_extendida_rf &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt; ## 7 receta_extendida_boost &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt; ## 8 receta_extendida_elasticnet &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt; ## 9 receta_experta_knn &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt; ## 10 receta_experta_rf &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt; ## 11 receta_experta_boost &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt; ## 12 receta_experta_elasticnet &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt; Dado que el pre-procesador contiene más de una entrada, la función crea todas las combinaciones de pre-procesadores y modelos. info: Contiene un tibble con algunos identificadores y el objeto de flujo de trabajo. option: Es un marcador de posición para cualquier argumento que se utilice cuando evaluamos el flujo de trabajo. result: Es un marcador de posición para la salida de las funciones de ajuste o remuestreo. Para este ejemplo, la búsqueda del grid se aplica al flujo de trabajo. library(doParallel) UseCores &lt;- detectCores() - 1 cluster &lt;- makeCluster(UseCores) registerDoParallel(cluster) grid_ctrl &lt;- control_grid( save_pred = TRUE, save_workflow = TRUE, parallel_over = &quot;everything&quot; ) set.seed(536) tunning_models_result &lt;- workflow_tunning_set_models %&gt;% workflow_map( fn = &quot;tune_grid&quot;, seed = 20221022, resamples = ames_folds, grid = 30, metrics = metric_set(mae, mape, rmse, rsq), control = grid_ctrl, verbose = TRUE ) stopCluster(cluster) tunning_models_result %&gt;% saveRDS(&quot;models/ensemble_model.rds&quot;) tunning_models_result &lt;- readRDS(&quot;models/ensemble_model.rds&quot;) tunning_models_result %&gt;% rank_results(select_best = T) %&gt;% select(-c(.config, n, preprocessor, std_err)) %&gt;% pivot_wider(names_from = .metric, values_from = mean) ## # A tibble: 12 × 7 ## wflow_id model rank mae mape rmse rsq ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 receta_experta_boost boost_tree 1 19244. 11.8 29474. 0.863 ## 2 receta_extendida_rf rand_forest 2 21171. 13.2 32709. 0.829 ## 3 receta_extendida_boost boost_tree 3 21179. 13.0 32203. 0.836 ## 4 receta_experta_rf rand_forest 4 21836. 13.6 33843. 0.820 ## 5 receta_extendida_knn nearest_neighbor 5 23273. 14.4 36006. 0.795 ## 6 receta_experta_knn nearest_neighbor 6 24639. 14.5 38809. 0.768 ## 7 receta_experta_elasticnet linear_reg 7 25602. 15.0 38309. 0.771 ## 8 receta_extendida_elasticnet linear_reg 8 27114. 16.1 45859. 0.687 ## 9 receta_base_boost boost_tree 9 27621. 16.0 42057. 0.726 ## 10 receta_base_rf rand_forest 10 27863. 16.8 41351. 0.731 ## 11 receta_base_knn nearest_neighbor 11 31133. 18.5 45870. 0.673 ## 12 receta_base_elasticnet linear_reg 12 32603. 19.1 48345. 0.641 autoplot( tunning_models_result, rank_metric = &quot;rsq&quot;, metric = &quot;rsq&quot;, select_best = F) + lims(y = c(0, 1)) + ggtitle(&quot;Model Comparisson&quot;) autoplot( tunning_models_result, rank_metric = &quot;rsq&quot;, metric = &quot;rsq&quot;, select_best = T) + geom_text(aes(y = mean - 0.10 , label = wflow_id), angle = 90, hjust = 1) + lims(y = c(0, 1)) + ggtitle(&quot;Model Comparisson&quot;) 11.5 Extracción de modelos Una vez que hemos realizado una exploración sobre el desempeño de todas las combinaciones de pre-procesamientos con modelos, es posible tomar varios caminos hacia adelante. Algunas de las opciones más comunes son: Realizar múltiples iteraciones de modificaciones a las recetas para extraer lo mejor de cada una. Realizar mejoras a los hiperparámetros. Eliminar modelos y/o recetas que no tuvieron buen desempeño. Crear un modelo a partir de la combinación de los modelos más competentes. Para empezar, se realiza una exploración del resultado del desempeño de los mejores modelos y los hiperparámetros usado en cada caso. Resultados de XGBoost tunning_models_result %&gt;% extract_workflow_set_result(&quot;receta_experta_boost&quot;) %&gt;% autoplot(metric = &quot;rsq&quot;) tunning_models_result %&gt;% extract_workflow_set_result(&quot;receta_experta_boost&quot;) %&gt;% show_best(n = 10, metric = &quot;rsq&quot;) %&gt;% select(-c(.estimator, .metric, .config, n)) ## # A tibble: 10 × 8 ## mtry min_n tree_depth learn_rate loss_reduction sample_size mean std_err ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 29 10 6 0.0676 1.79e- 5 0.636 0.863 0.00972 ## 2 17 15 15 0.0338 7.80e- 9 0.872 0.854 0.00874 ## 3 25 23 8 0.0443 9.46e- 1 0.986 0.851 0.00787 ## 4 18 6 13 0.160 4.10e-10 0.772 0.849 0.0101 ## 5 39 2 3 0.285 4.25e- 2 0.593 0.847 0.0112 ## 6 33 13 10 0.0799 9.73e- 9 0.526 0.846 0.0106 ## 7 48 27 2 0.128 3.03e+ 1 0.939 0.844 0.0104 ## 8 12 12 5 0.0534 1.45e- 6 0.366 0.842 0.00874 ## 9 53 38 4 0.120 2.03e- 7 0.692 0.836 0.0106 ## 10 32 30 12 0.0191 1.10e-10 0.943 0.836 0.00982 Resultados de Random Forest tunning_models_result %&gt;% extract_workflow_set_result(&quot;receta_extendida_rf&quot;) %&gt;% autoplot(metric = &quot;rmse&quot;) tunning_models_result %&gt;% extract_workflow_set_result(&quot;receta_extendida_rf&quot;) %&gt;% show_best(n = 10, metric = &quot;rmse&quot;) %&gt;% select(-c(.estimator, .metric, .config, n)) ## # A tibble: 10 × 4 ## mtry min_n mean std_err ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 9 7 32513. 823. ## 2 10 4 32608. 864. ## 3 5 5 32627. 800. ## 4 15 16 32693. 799. ## 5 13 14 32709. 809. ## 6 12 7 32721. 858. ## 7 17 13 32764. 724. ## 8 23 3 32796. 714. ## 9 18 18 32803. 694. ## 10 20 11 32828. 694. 11.5.1 Selección de modelo Habiendo determinado los hiperparámetros adecuados para la configuración del modelo, procedemos a seleccionar la configuración adecuada para nosotros. Estos pasos son los mismos que corresponden a la selección del modelo con un único workflow. Mejor modelo XGBoost best_xgb_model &lt;- tunning_models_result %&gt;% extract_workflow_set_result(&quot;receta_experta_boost&quot;) %&gt;% select_best(metric = &quot;rsq&quot;, &quot;rsq&quot;) best_xgb_model %&gt;% mutate_all(as.character) %&gt;% pivot_longer(everything(), names_to = &quot;metric&quot;, values_to = &quot;value&quot;) ## # A tibble: 7 × 2 ## metric value ## &lt;chr&gt; &lt;chr&gt; ## 1 mtry 29 ## 2 min_n 10 ## 3 tree_depth 6 ## 4 learn_rate 0.0676325249032833 ## 5 loss_reduction 1.78986298406448e-05 ## 6 sample_size 0.636355151841417 ## 7 .config Preprocessor1_Model12 Mejor modelo XGBoost a menos de una desviación estándar best_regularized_xgb_model_1se &lt;- tunning_models_result %&gt;% extract_workflow_set_result(&quot;receta_experta_boost&quot;) %&gt;% select_by_one_std_err(metric = &quot;rsq&quot;, &quot;rsq&quot;) best_regularized_xgb_model_1se %&gt;% mutate_all(as.character) %&gt;% pivot_longer(everything(), names_to = &quot;metric&quot;, values_to = &quot;value&quot;) ## # A tibble: 14 × 2 ## metric value ## &lt;chr&gt; &lt;chr&gt; ## 1 mtry 29 ## 2 min_n 10 ## 3 tree_depth 6 ## 4 learn_rate 0.0676325249032833 ## 5 loss_reduction 1.78986298406448e-05 ## 6 sample_size 0.636355151841417 ## 7 .metric rsq ## 8 .estimator standard ## 9 mean 0.862749773434049 ## 10 n 10 ## 11 std_err 0.00971605219814996 ## 12 .config Preprocessor1_Model12 ## 13 .best 0.862749773434049 ## 14 .bound 0.853033721235899 Ajuste del modelo seleccionado final_regularized_xgb_model &lt;- tunning_models_result %&gt;% extract_workflow(&quot;receta_experta_boost&quot;) %&gt;% finalize_workflow(best_regularized_xgb_model_1se) %&gt;% parsnip::fit(data = ames_train) ## [23:57:18] WARNING: amalgamation/../src/learner.cc:627: ## Parameters: { &quot;importance&quot; } might not be used. ## ## This could be a false alarm, with some parameters getting used by language bindings but ## then being mistakenly passed down to XGBoost core, or some parameter actually being used ## but getting flagged wrongly here. Please open an issue if you find any such cases. final_regularized_xgb_model ## ══ Workflow [trained] ════════ ## Preprocessor: Recipe ## Model: boost_tree() ## ## ── Preprocessor ────────────── ## 9 Recipe Steps ## ## • step_ratio() ## • step_ratio() ## • step_mutate() ## • step_relevel() ## • step_normalize() ## • step_dummy() ## • step_interact() ## • step_interact() ## • step_rm() ## ## ── Model ───────────────────── ## ##### xgb.Booster ## raw: 958.3 Kb ## call: ## xgboost::xgb.train(params = list(eta = 0.0676325249032833, max_depth = 6L, ## gamma = 1.78986298406448e-05, colsample_bytree = 1, colsample_bynode = 0.517857142857143, ## min_child_weight = 10L, subsample = 0.636355151841417), data = x$data, ## nrounds = 500, watchlist = x$watchlist, verbose = 0, importance = &quot;impurity&quot;, ## nthread = 1, objective = &quot;reg:squarederror&quot;) ## params (as set within xgb.train): ## eta = &quot;0.0676325249032833&quot;, max_depth = &quot;6&quot;, gamma = &quot;1.78986298406448e-05&quot;, colsample_bytree = &quot;1&quot;, colsample_bynode = &quot;0.517857142857143&quot;, min_child_weight = &quot;10&quot;, subsample = &quot;0.636355151841417&quot;, importance = &quot;impurity&quot;, nthread = &quot;1&quot;, objective = &quot;reg:squarederror&quot;, validate_parameters = &quot;TRUE&quot; ## xgb.attributes: ## niter ## callbacks: ## cb.evaluation.log() ## # of features: 56 ## niter: 500 ## nfeatures : 56 ## evaluation_log: ## iter training_rmse ## 1 184817.81 ## 2 173355.72 ## --- ## 499 13685.27 ## 500 13675.65 Como hemos hablado anteriormente, este último objeto es el modelo final entrenado, el cual contiene toda la información del pre-procesamiento de datos, por lo que en caso de ponerse en producción el modelo, sólo se necesita de este último elemento para poder realizar nuevas predicciones. Es importante validar que hayamos hecho un uso correcto de las variables predictivas. En este momento es posible detectar variables que no estén aportando valor o variables que no debiéramos estar usando debido a que cometeríamos data leakage. Para enfrentar esto, ayuda estimar y ordenar el valor de importancia del modelo library(vip) final_regularized_xgb_model %&gt;% extract_fit_parsnip() %&gt;% vip::vip(num_features = 20) + ggtitle(&quot;Importancia de las variables&quot;) Por último… Imaginemos por un momento que pasa un mes de tiempo desde que hicimos nuestro modelo, es hora de ponerlo a prueba prediciendo valores de nuevos elementos: results &lt;- predict(final_regularized_xgb_model, ames_test) %&gt;% dplyr::bind_cols(truth = ames_test$Sale_Price) %&gt;% dplyr::rename(pred_xgb_reg = .pred, Sale_Price = truth) results ## # A tibble: 733 × 2 ## pred_xgb_reg Sale_Price ## &lt;dbl&gt; &lt;int&gt; ## 1 109202. 105000 ## 2 186124. 185000 ## 3 169706. 180400 ## 4 114391. 141000 ## 5 237815. 210000 ## 6 205541. 216000 ## 7 152242. 149900 ## 8 110091. 105500 ## 9 110091. 88000 ## 10 147398. 146000 ## # … with 723 more rows multi_metric &lt;- metric_set(rmse, mae, mape, rsq, ccc) multi_metric(results, truth = Sale_Price, estimate = pred_xgb_reg) %&gt;% mutate(.estimate = round(.estimate, 2)) ## # A tibble: 5 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 rmse standard 32077. ## 2 mae standard 21173. ## 3 mape standard 12.9 ## 4 rsq standard 0.84 ## 5 ccc standard 0.91 results %&gt;% ggplot(aes(x = pred_xgb_reg, y = Sale_Price)) + geom_point() + geom_abline(color = &quot;red&quot;) + xlab(&quot;Prediction&quot;) + ylab(&quot;Observation&quot;) + ggtitle(&quot;Comparisson&quot;) 11.6 Stacking El ensamblaje de modelos es un proceso en el que se utilizan varios modelos base para predecir un resultado. La motivación para usar modelos de conjunto es reducir el error de generalización de la predicción. Siempre que los modelos base sean diversos e independientes, el error de predicción disminuye cuando se utiliza el enfoque de conjunto. Aunque el modelo de conjunto tiene varios modelos base dentro del modelo, actúa y funciona como un solo modelo. Un conjunto de modelos, donde las predicciones de varios modelos individuales se agregan para hacer una predicción, puede producir un modelo final de alto rendimiento. Los métodos más populares para crear modelos de conjuntos son: Bagging Bosques aleatorios Boosting Cada uno de estos métodos combina las predicciones de múltiples versiones del mismo tipo de modelo. Uno de los primeros métodos para crear conjuntos es el apilamiento de modelos (stacking). Stacking combina las predicciones de múltiples modelos de cualquier tipo. Por ejemplo, una regresión logística, un árbol de clasificación y una máquina de vectores de soporte se pueden incluir en un conjunto de apilamiento, así como diferentes configuraciones de un mismo modelo. El proceso de construcción de un conjunto apilado es: Reunir el conjunto de entrenamiento de predicciones (producidas mediante remuestreo). Crear un modelo para combinar estas predicciones. Para cada modelo del conjunto, ajustar el modelo en el conjunto de entrenamiento original. 11.6.1 Elección de modelos Para cada observación en el conjunto de entrenamiento, el apilamiento (stacking) requiere una predicción fuera de la muestra de algún tipo. Para comenzar a ensamblar con el paquete stacks, se crea una pila de datos vacía usando la función stacks() y luego se agrega el flujo de trabajo para ajustar una amplia variedad de modelos a estos datos. library(tidymodels) library(stacks) tidymodels_prefer() concrete_stack &lt;- stacks() %&gt;% add_candidates(tunning_models_result) concrete_stack concrete_stack %&gt;% saveRDS(&quot;models/concrete_stack.rds&quot;) concrete_stack &lt;- readRDS(&quot;models/concrete_stack.rds&quot;) La regularización mediante la penalización de lazo tiene varias ventajas: El uso de la penalización de lazo puede eliminar modelos (y, a veces, tipos de modelos completos) del conjunto. La correlación entre los candidatos del conjunto tiende a ser muy alta y la regularización ayuda a mitigar este problema. Dado que nuestro resultado es numérico, se utiliza la regresión lineal para el meta-modelo. library(doParallel) UseCores &lt;- detectCores() - 1 cluster &lt;- makeCluster(UseCores) registerDoParallel(cluster) set.seed(20220612) assembly &lt;- concrete_stack %&gt;% blend_predictions(metric = metric_set(rmse, mae, mape, rsq)) stopCluster(cluster) assembly %&gt;% saveRDS(&quot;models/stack_predictions.rds&quot;) assembly &lt;- readRDS(&quot;models/stack_predictions.rds&quot;) assembly ## # A tibble: 10 × 3 ## member type weight ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 receta_experta_boost_1_12 boost_tree 0.303 ## 2 receta_extendida_boost_1_05 boost_tree 0.172 ## 3 receta_extendida_boost_1_24 boost_tree 0.157 ## 4 receta_experta_boost_1_24 boost_tree 0.145 ## 5 receta_extendida_boost_1_01 boost_tree 0.0777 ## 6 receta_experta_knn_1_15 nearest_neighbor 0.0558 ## 7 receta_experta_knn_1_27 nearest_neighbor 0.0382 ## 8 receta_experta_boost_1_04 boost_tree 0.0345 ## 9 receta_experta_knn_1_24 nearest_neighbor 0.0299 ## 10 receta_experta_boost_1_01 boost_tree 0.0226 autoplot(assembly, &quot;weights&quot;) + theme_minimal() Esto evalúa el modelo de meta aprendizaje sobre un grid predefinido de valores de penalización de lazo y utiliza un método de remuestreo interno para determinar el mejor valor. El método autoplot() nos ayuda a comprender si el método de penalización predeterminado fue suficiente: autoplot(assembly) El panel de en medio muestra el número promedio de modelos del conjunto retenidos por el modelo de meta aprendizaje. Es posible que el rango predeterminado no nos haya servido bien aquí. Para evaluar el modelo de meta aprendizaje con penalizaciones mayores. UseCores &lt;- detectCores() - 1 cluster &lt;- makeCluster(UseCores) registerDoParallel(cluster) set.seed(20220612) assembly_v2 &lt;- concrete_stack %&gt;% blend_predictions( metric = metric_set(rmse, mae, mape, rsq), penalty = 10^seq(-8, 4, length = 50) ) stopCluster(cluster) assembly_v2 %&gt;% saveRDS(&quot;models/stack_predictions_v2.rds&quot;) assembly_v2 &lt;- readRDS(&quot;models/stack_predictions_v2.rds&quot;) autoplot(assembly_v2) El valor de penalización asociado a las curvas fue de 1456.34. La impresión del objeto muestra los detalles del modelo de meta aprendizaje: assembly_v2 ## # A tibble: 10 × 3 ## member type weight ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 receta_experta_boost_1_12 boost_tree 0.314 ## 2 receta_extendida_boost_1_05 boost_tree 0.179 ## 3 receta_extendida_boost_1_24 boost_tree 0.157 ## 4 receta_experta_boost_1_24 boost_tree 0.146 ## 5 receta_experta_knn_1_15 nearest_neighbor 0.0870 ## 6 receta_extendida_boost_1_01 boost_tree 0.0654 ## 7 receta_experta_knn_1_27 nearest_neighbor 0.0301 ## 8 receta_experta_boost_1_01 boost_tree 0.0280 ## 9 receta_experta_boost_1_04 boost_tree 0.0225 ## 10 receta_base_boost_1_24 boost_tree 0.0107 El modelo de meta aprendizaje contenía cuatro coeficientes de combinación. El método autoplot() se puede usar nuevamente para mostrar las contribuciones de cada tipo de modelo: autoplot(assembly_v2, &quot;weights&quot;) + theme_minimal() El modelo de bosques aleatorios tiene la mayor contribución al conjunto. Para este conjunto, el resultado se predice con la ecuación: \\[\\begin{aligned} \\text{Predicción ensamblada} = &amp;- 7172.1435 \\\\ &amp;+ 0.0107 * \\text{receta_base_boost_1_24} \\\\ &amp;+ 0.0051 * \\text{receta_extendida_boost_1_08} \\\\ &amp;+ 0.1568 * \\text{receta_extendida_boost_1_24} \\\\ &amp;+ 0.0653 * \\text{receta_extendida_boost_1_01} \\\\ &amp;+ 0.1788 * \\text{receta_extendida_boost_1_05} \\\\ &amp;+ 0.0010 * \\text{receta_experta_knn_1_14} \\\\ &amp;+ 0.0301 * \\text{receta_experta_knn_1_27} \\\\ &amp;+ 0.0870 * \\text{receta_experta_knn_1_15} \\\\ &amp;+ 0.0034 * \\text{receta_experta_knn_1_24} \\\\ &amp;+ 0.0224 * \\text{receta_experta_boost_1_04} \\\\ &amp;+ 0.3143 * \\text{receta_experta_boost_1_12} \\\\ &amp;+ 0.1463 * \\text{receta_experta_boost_1_24} \\\\ &amp;+ 0.0279 * \\text{receta_experta_boost_1_01} \\\\ &amp;+ 0.0003 * \\text{receta_experta_elasticnet_1_06} \\\\ &amp;+ 3.74e-05 * \\text{receta_experta_elasticnet_1_27} \\\\ &amp;+ 4.06e-05 * \\text{receta_experta_elasticnet_1_03} \\\\ &amp;+ 0.0001 * \\text{receta_experta_elasticnet_1_02} \\\\ &amp;+ 0.0014 * \\text{receta_experta_elasticnet_1_01} \\end{aligned}\\] Ahora sabemos cómo se pueden combinar sus predicciones en una predicción final para el conjunto. Sin embargo, estos ajustes de modelos individuales aún no se han creado. 11.6.2 Ajuste final Para poder usar el modelo de stacking, se requieren los ajustes de todos los modelos candidatos. Estos utilizan todo el conjunto de entrenamiento con los predictores originales. library(tidyverse) library(tidymodels) library(stacks) tidymodels_prefer() UseCores &lt;- 1 cluster &lt;- makeCluster(UseCores) registerDoParallel(cluster) fit_assembly &lt;- fit_members(assembly_v2) stopCluster(cluster) fit_assembly %&gt;% saveRDS(&quot;models/stack_fit_members.rds&quot;) fit_assembly &lt;- readRDS(&quot;models/stack_fit_members.rds&quot;) fit_assembly ## # A tibble: 10 × 3 ## member type weight ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 receta_experta_boost_1_12 boost_tree 0.314 ## 2 receta_extendida_boost_1_05 boost_tree 0.179 ## 3 receta_extendida_boost_1_24 boost_tree 0.157 ## 4 receta_experta_boost_1_24 boost_tree 0.146 ## 5 receta_experta_knn_1_15 nearest_neighbor 0.0870 ## 6 receta_extendida_boost_1_01 boost_tree 0.0654 ## 7 receta_experta_knn_1_27 nearest_neighbor 0.0301 ## 8 receta_experta_boost_1_01 boost_tree 0.0280 ## 9 receta_experta_boost_1_04 boost_tree 0.0225 ## 10 receta_base_boost_1_24 boost_tree 0.0107 Esto actualiza el objeto de apilamiento con los objetos de flujo de trabajo ajustados para cada miembro. En este punto, el modelo de stacking se puede utilizar para la predicción. regression_metrics &lt;- metric_set(rmse, mae, mape, rsq, ccc) fit_assembly_pred_test &lt;- predict(fit_assembly, ames_test) %&gt;% bind_cols(ames_test) stacking_metrics &lt;- fit_assembly_pred_test %&gt;% regression_metrics(Sale_Price, .pred) %&gt;% select(-.estimator) %&gt;% rename(stacking = .estimate) stacking_metrics ## # A tibble: 5 × 2 ## .metric stacking ## &lt;chr&gt; &lt;dbl&gt; ## 1 rmse 29932. ## 2 mae 19800. ## 3 mape 12.0 ## 4 rsq 0.862 ## 5 ccc 0.925 11.6.3 Comparación de métricas Para ver la efectividad del ensamblaje, realizamos una comparación con el mejor modelo entrenado anteriormente (XGBoost). Es importante que la comparación se realice utilizando los mismos datos de prueba. results %&gt;% regression_metrics(truth = Sale_Price, estimate = pred_xgb_reg) %&gt;% select(-.estimator) %&gt;% rename(xgboost = .estimate) %&gt;% left_join(stacking_metrics, by = &quot;.metric&quot;) %&gt;% map_if(is.numeric, round, 3) %&gt;% as_tibble() ## # A tibble: 5 × 3 ## .metric xgboost stacking ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 rmse 32077. 29932. ## 2 mae 21173. 19800. ## 3 mape 12.9 12.0 ## 4 rsq 0.842 0.862 ## 5 ccc 0.914 0.925 Este capítulo demuestra cómo combinar diferentes modelos en un conjunto para un mejor desempeño predictivo. El proceso de creación del conjunto puede eliminar automáticamente los modelos candidatos para encontrar un pequeño subconjunto que mejore el rendimiento. "],["sesgo-e-inequidad.html", "Capítulo 12 Sesgo e Inequidad 12.1 Propósito Vs Error 12.2 Métricas", " Capítulo 12 Sesgo e Inequidad Machine Learning por naturaleza es discriminante, pues justo lo que hacemos es discriminar datos a través del uso de la estadística. Sin embargo, esta discriminación puede ser un problema cuando brinda ventajas sistemáticas a grupos privilegiados y desventajas sistemáticas a grupos no privilegiados. Por ejemplo: Privilegiar la atención médica a pacientes blancos sobre pacientes afroamericanos. Un sesgo en el conjunto de entrenamiento ya sea por prejuicio o por un sobre/sub muestreo lleva a tener modelos sesgados. Un mal entendido común al hacer modelos de machine learning consiste en evitar utilizar características que pueden generar una inequidad por ejemplo: sexo, edad, etnia, etc. Sin embargo, no ocuparlos nos lleva a tener puntos ciegos en nuestros modelos para cuantificar si efectivamente tenemos un sesgo o inequidad en algunos grupos. Deberemos de ocupar estas características en los modelos, justo porque queremos evitar estos sesgos. Para ello, identificaremos y cuantificaremos estos sesgos e inequidades en diferentes grupos para después mitigarlos y cuantificar la consecuencia en nuestras métricas de desempeño off line. 12.1 Propósito Vs Error El siguiente árbol de decisión está desarrollado pensando desde el punto de vista del tomador de decisiones -operativas- al que ayudamos desarrollando un modelo de machine learning para identificar en qué métricas deberíamos de concentrarnos para cuantificar el sesgo y la inequidad (bias y fairness). Métricas FP/GS: False Positive over Group Size. Es el riesgo de ser incorrectamente clasificado como positivo, dado el grupo de pertenencia FDR: False Discovery Rate. Es similar al error tipo 1 en pruebas de hipótesis estadísticas. FPR: False Positive Rate. Recall: Cobertura del modelo respecto al total de positivos. FN/GS: False Negative over Group Size. Es el riesgo de ser incorrectamente clasificado como negativo, dado el grupo de pertenencia FOR: False Omission Rate. Similar al error tipo 2 en pruebas de hipótesis estadísticas. FNR: False Negative Rate Tipo de modelo | Aplicación Modelo Punitivo: Corresponde a modelos en donde al menos una de las acciones asociadas a nuestro modelo de predicción está relacionada con un “castigo”. Por ejemplo: Algoritmos donde se predice la probabilidad de reincidencia en algún delito y que es tomada como variable para decidir si dan libertad provisional o no. Modelo Asistivo: Corresponde a modelos en donde la acción asociada al modelo son del estilo de preventivo. Por ejemplo: Priorización de inspecciones a realizar: médicas, a hogares, a estaciones de generación de energía, etc. RECORDATORIO PREDICTED REAL TP 1 1 FP 1 0 TN 0 0 FN 0 1 \\[(1-\\tau) \\leq \\text{Medida de disparidad}_\\text{grupo i} \\leq \\frac{1}{(1 - \\tau)}, \\] donde \\(\\tau\\) es el fairness threshold definido por nosotros. En los siguientes ejemplos utilizaremos \\(\\tau=0.20%\\) por lo que cualquier métrica de paridad que se encuentre entre \\(0.8\\) y \\(1.25\\) va a ser tratado como justo (sin sesgo). 12.2 Métricas El paquete fairness implementa \\(11\\) métricas de equidad. Muchos de estos son mutuamente excluyentes: los resultados para un problema de clasificación, a menudo no pueden ser justos en términos de todas las métricas. Dependiendo del contexto, es importante seleccionar una métrica adecuada para evaluar la equidad. A continuación, se describen las funciones utilizadas para calcular las métricas implementadas. Cada función tiene un conjunto similar de argumentos: data: data.frame que contiene los datos de entrada y las predicciones del modelo grupo: nombre de la columna que indica el grupo base (variable de factor) base: nivel base del grupo base para el cálculo de métricas de equidad resultado: nombre de la columna que indica la variable de resultado binaria result_base: nivel base de la variable de resultado (es decir, clase negativa) para el cálculo de métricas de equidad También necesitamos proporcionar predicciones de modelos, estas predicciones se pueden agregar al data.frame original o se pueden proporcionar como un vector. Cuando se trabaja con predicciones probabilísticas, algunas métricas requieren un valor de corte para convertir probabilidades en predicciones de clase proporcionadas como límite. 12.2.1 Equal Parity or Demographic or Statistical Parity Cuando nos interesa que cada grupo de la variable “protegida” tenga la misma proporción de etiquetas positivas predichas (TP). Por ejemplo: En un modelo que predice si darte o no un crédito, nos gustaría que sin importar el género de la persona tuvieran la misma oportunidad. La paridad demográfica se calcula en base a la comparación del número absoluto de todos los individuos clasificados positivamente en todos los subgrupos de datos. En el resultado del vector con nombres, al grupo de referencia se le asignará 1, mientras que a todos los demás grupos se les asignarán valores según si su proporción de observaciones pronosticadas positivamente es menor o mayor en comparación con el grupo de referencia. Las proporciones más bajas se reflejarán en números inferiores a 1 en el vector con resultado de nombre. Fórmula: \\(TP + FP\\) Se utiliza esta métrica cuando: Queremos cambiar el estado actual para “mejorarlo”. Por ejemplo: Ver más personas de grupos desfavorecidos con mayor oportunidad de tener un préstamo. Conocemos que ha habido una ventaja histórica que afecta los datos con los que construiremos el modelo. Al querer eliminar las desventajas podríamos poner en más desventaja al grupo que históricamente ha tenido desventaja, ya que no está preparado (literalmente) para recibir esa ventaja. Por ejemplo, si damos créditos a grupos a los que antes de hacer fairness no lo hacíamos, sin ninguna educación financiera o apoyo de educación financiera de nuestra parte, muy probablemente esas personas caerán en default aumentando el sesgo que ya teníamos inicialmente. pacman::p_load( tidymodels, fairness, magrittr ) data(compas) compas %&lt;&gt;% mutate(Two_yr_Recidivism_01 = if_else(Two_yr_Recidivism == &#39;yes&#39;, 1, 0)) glimpse(compas) ## Rows: 6,172 ## Columns: 10 ## $ Two_yr_Recidivism &lt;fct&gt; no, yes, no, no, no, no, yes, yes, yes, no, no, n… ## $ Number_of_Priors &lt;dbl&gt; -0.6843578, 2.2668817, -0.6843578, -0.6843578, -0… ## $ Age_Above_FourtyFive &lt;fct&gt; no, no, no, no, no, no, no, no, no, no, no, no, n… ## $ Age_Below_TwentyFive &lt;fct&gt; no, no, no, no, no, no, no, no, no, yes, no, no, … ## $ Female &lt;fct&gt; Male, Male, Female, Male, Male, Male, Male, Male,… ## $ Misdemeanor &lt;fct&gt; yes, no, yes, no, yes, yes, no, no, no, no, no, y… ## $ ethnicity &lt;fct&gt; Other, Caucasian, Caucasian, African_American, Hi… ## $ probability &lt;dbl&gt; 0.3151557, 0.8854616, 0.2552680, 0.4173908, 0.320… ## $ predicted &lt;dbl&gt; 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1… ## $ Two_yr_Recidivism_01 &lt;dbl&gt; 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1… dem_parity( data = compas, outcome = &#39;Two_yr_Recidivism_01&#39;, group = &#39;ethnicity&#39;, probs = &#39;probability&#39;, cutoff = 0.4, base = &#39;Caucasian&#39; ) ## $Metric ## Caucasian African_American Asian Hispanic ## Positively classified 1070 2599.000000 7.000000000 214.0 ## Demographic Parity 1 2.428972 0.006542056 0.2 ## Group size 2103 3175.000000 31.000000000 509.0 ## Native_American Other ## Positively classified 7.000000000 152.0000000 ## Demographic Parity 0.006542056 0.1420561 ## Group size 11.000000000 343.0000000 ## ## $Metric_plot ## ## $Probability_plot 12.2.2 Proportional Parity o Impact Parity o Minimizing Disparate Impact Cuando nos interesa que cada grupo de la variable “protegida” tenga el mismo impacto. La paridad proporcional se logra si la proporción de predicciones positivas en los subgrupos es cercana entre sí. Similar a la paridad demográfica, esta medida tampoco depende de las etiquetas verdaderas. Fórmula: \\(\\frac{TP + FP}{TP + FP + TN + FN}\\) prop_parity( data = compas, outcome = &#39;Two_yr_Recidivism_01&#39;, group = &#39;ethnicity&#39;, probs = &#39;probability&#39;, cutoff = 0.4, base = &#39;Caucasian&#39; ) ## $Metric ## Caucasian African_American Asian Hispanic ## Proportion 0.508797 0.8185827 0.2258065 0.4204322 ## Proportional Parity 1.000000 1.6088592 0.4438046 0.8263261 ## Group size 2103.000000 3175.0000000 31.0000000 509.0000000 ## Native_American Other ## Proportion 0.6363636 0.4431487 ## Proportional Parity 1.2507222 0.8709735 ## Group size 11.0000000 343.0000000 ## ## $Metric_plot ## ## $Probability_plot 12.2.3 Equalized odds Las probabilidades igualadas se logran si las sensibilidades en los subgrupos están cerca unas de otras. Las sensibilidades específicas del grupo indican el número de verdaderos positivos dividido por el número total de positivos en ese grupo. Fórmula: \\(\\frac{TP}{TP + FN}\\) equal_odds( data = compas, outcome = &#39;Two_yr_Recidivism_01&#39;, group = &#39;ethnicity&#39;, probs = &#39;probability&#39;, cutoff = 0.4, base = &#39;Caucasian&#39; ) ## $Metric ## Caucasian African_American Asian Hispanic ## Sensitivity 0.676399 0.910295 0.2500000 0.5978836 ## Equalized odds 1.000000 1.345796 0.3696043 0.8839214 ## Group size 2103.000000 3175.000000 31.0000000 509.0000000 ## Native_American Other ## Sensitivity 0.6000000 0.6532258 ## Equalized odds 0.8870504 0.9657403 ## Group size 11.0000000 343.0000000 ## ## $Metric_plot ## ## $Probability_plot 12.2.4 Predictive rate parity La paridad de tasa predictiva se logra si las precisiones (o valores predictivos positivos) en los subgrupos están cerca unas de otras. La precisión representa el número de verdaderos positivos dividido por el número total de ejemplos predichos positivos dentro de un grupo. Fórmula: \\(\\frac{TP}{TP + FP}\\) pred_rate_parity( data = compas, outcome = &#39;Two_yr_Recidivism_01&#39;, group = &#39;ethnicity&#39;, probs = &#39;probability&#39;, cutoff = 0.4, base = &#39;Caucasian&#39; ) ## $Metric ## Caucasian African_American Asian Hispanic ## Precision 0.5196262 0.5817622 0.2857143 0.5280374 ## Predictive Rate Parity 1.0000000 1.1195784 0.5498458 1.0161871 ## Group size 2103.0000000 3175.0000000 31.0000000 509.0000000 ## Native_American Other ## Precision 0.4285714 0.5328947 ## Predictive Rate Parity 0.8247688 1.0255348 ## Group size 11.0000000 343.0000000 ## ## $Metric_plot ## ## $Probability_plot 12.2.5 Accuracy parity La paridad de precisión se logra si las precisiones (todos los ejemplos clasificados con precisión divididos por el número total de ejemplos) en los subgrupos están cerca entre sí. Fórmula: \\(\\frac{TP + TN}{TP + FP + TN + FN}\\) acc_parity( data = compas, outcome = &#39;Two_yr_Recidivism_01&#39;, group = &#39;ethnicity&#39;, probs = &#39;probability&#39;, cutoff = 0.4, base = &#39;Caucasian&#39; ) ## $Metric ## Caucasian African_American Asian Hispanic ## Accuracy 0.6291013 0.6107087 0.6451613 0.6522593 ## Accuracy Parity 1.0000000 0.9707637 1.0255285 1.0368113 ## Group size 2103.0000000 3175.0000000 31.0000000 509.0000000 ## Native_American Other ## Accuracy 0.4545455 0.6676385 ## Accuracy Parity 0.7225314 1.0612575 ## Group size 11.0000000 343.0000000 ## ## $Metric_plot ## ## $Probability_plot 12.2.6 False Negative Parity o Equal Oppportunity La paridad de tasas de falsos negativos se logra si las tasas de falsos negativos (la relación entre el número de falsos negativos y el número total de positivos) en los subgrupos están cerca entre sí. Fórmula: \\(\\frac{FN}{TP + FN}\\) Se utiliza esta métrica cuando: El modelo necesita ser muy bueno en detectar la etiqueta positiva. No hay -mucho- costo en introducir falsos negativos al sistema -tanto al usuario como a la empresa-. Por ejemplo: Generar FPs en tarjeta de crédito. La definición de la variable target no es subjetiva. Por ejemplo: Fraude o No Fraude no es algo subjetivo, buen empleado o no, puede ser muy subjetivo. Para poder cumplir con tener el mismo porcentaje de TPR en todos los grupos de la variable protegida, incurriremos en agregar más falsos positivos, lo que puede afectar más a ese grupo a largo plazo. fnr_parity( data = compas, outcome = &#39;Two_yr_Recidivism_01&#39;, group = &#39;ethnicity&#39;, probs = &#39;probability&#39;, cutoff = 0.4, base = &#39;Caucasian&#39; ) ## $Metric ## Caucasian African_American Asian Hispanic Native_American ## FNR 0.323601 0.0897050 0.750000 0.4021164 0.40000 ## FNR Parity 1.000000 0.2772087 2.317669 1.2426304 1.23609 ## Group size 2103.000000 3175.0000000 31.000000 509.0000000 11.00000 ## Other ## FNR 0.3467742 ## FNR Parity 1.0716105 ## Group size 343.0000000 ## ## $Metric_plot ## ## $Probability_plot 12.2.7 False Positive Parity Cuando queremos que todos los grupos de la variable protegida tengan la misma tasa de falsos positivos. Es decir, nos equivocamos en las mismas proporciones para etiquetas positivas que eran negativas. Fórmula: \\(\\frac{FP}{TN + FP}\\) fpr_parity( data = compas, outcome = &#39;Two_yr_Recidivism_01&#39;, group = &#39;ethnicity&#39;, probs = &#39;probability&#39;, cutoff = 0.4, base = &#39;Caucasian&#39; ) ## $Metric ## Caucasian African_American Asian Hispanic Native_American ## FPR 0.401249 0.7179657 0.2173913 0.3156250 0.6666667 ## FPR Parity 1.000000 1.7893269 0.5417865 0.7866063 1.6614786 ## Group size 2103.000000 3175.0000000 31.0000000 509.0000000 11.0000000 ## Other ## FPR 0.3242009 ## FPR Parity 0.8079793 ## Group size 343.0000000 ## ## $Metric_plot ## ## $Probability_plot 12.2.8 Negative predictive value parity La paridad de valor predictivo negativo se logra si los valores predictivos negativos en los subgrupos están cerca unos de otros. El valor predictivo negativo se calcula como una relación entre el número de negativos verdaderos y el número total de negativos previstos. Esta función puede considerarse la “inversa” de la paridad de tasa predictiva (predictive rate parity). Fórmula: \\(\\frac{TN}{TN + FN}\\) npv_parity( data = compas, outcome = &#39;Two_yr_Recidivism_01&#39;, group = &#39;ethnicity&#39;, probs = &#39;probability&#39;, cutoff = 0.4, base = &#39;Caucasian&#39; ) ## $Metric ## Caucasian African_American Asian Hispanic Native_American ## NPV 0.7424976 0.7413194 0.750000 0.7423729 0.5000000 ## NPV Parity 1.0000000 0.9984133 1.010104 0.9998321 0.6734029 ## Group size 2103.0000000 3175.0000000 31.000000 509.0000000 11.0000000 ## Other ## NPV 0.7748691 ## NPV Parity 1.0435982 ## Group size 343.0000000 ## ## $Metric_plot ## ## $Probability_plot 12.2.9 Specificity parity La paridad de especificidad se logra si las especificidades (la relación entre el número de verdaderos negativos y el número total de negativos) en los subgrupos están próximas entre sí. Esta función puede considerarse la “inversa” de las probabilidades igualadas (equalized odds). Fórmula: \\(\\frac{TN}{TN + FP}\\) spec_parity( data = compas, outcome = &#39;Two_yr_Recidivism_01&#39;, group = &#39;ethnicity&#39;, probs = &#39;probability&#39;, cutoff = 0.4, base = &#39;Caucasian&#39; ) ## $Metric ## Caucasian African_American Asian Hispanic ## Specificity 0.598751 0.2820343 0.7826087 0.684375 ## Specificity Parity 1.000000 0.4710378 1.3070688 1.143004 ## Group size 2103.000000 3175.0000000 31.0000000 509.000000 ## Native_American Other ## Specificity 0.3333333 0.6757991 ## Specificity Parity 0.5567145 1.1286814 ## Group size 11.0000000 343.0000000 ## ## $Metric_plot ## ## $Probability_plot "],["interpretabilidad-local.html", "Capítulo 13 Interpretabilidad local 13.1 Interpretabilidad LIME 13.2 Interpretabilidad DALEXtra", " Capítulo 13 Interpretabilidad local Interpretabilidad: En Machine Learning, nos referimos a interpretabilidad al grado en el que un humano puede entender la causa de una decisión o clasificación, la cual nos permite identificar y evitar tener sesgo, injusticia, inequidad en los modelos que generamos. Poder interpretar nuestros modelos nos brinda más confianza en que lo que estamos haciendo es correcto (además de las métricas de desempeño). Por otro lado, para la gente que lo ocupa, permite transparentar y tener más confianza al modelo. Ejemplos Etiquetado: Google photos (2015) etiqueta incorrectamente personas afroamericanas como gorilas. Facial Recognition (IBM, Microsoft, Megvii): Reconocimiento para hombres blancos 99%, mujeres afroamericanas 35%. Facebook automatic translation: Arresto de un palestino por traducción incorrecta de “buenos días” en hebreo a “atácalos”. Interpretabilidad en ML General Data Protection Regulation (GDPR): Desde mayo de 2018 existe el derecho a una explicación Por ejemplo: Algoritmos de predicción de riesgo en créditos hipotecarios. Se puede tener interpretabilidad de modelos de aprendizaje supervisado. Se tiene la creencia equivocada de que en Europa no se puede ocupar Deep Learning debido a la falta de interpretabilidad y el GDPR. Esto no es verdad. Es verdad que preferimos ocupar modelos más simples porque nos permiten entender -y explicar- de manera más sencilla por qué se están tomando las decisiones. Te recomiendo leer el artículo Why should I trust you? (2016) base de mucho de lo desarrollado para interpretabilidad. 13.1 Interpretabilidad LIME Acrónimo de Local Interpretable Model-Agnostic Explanation. El objetivo es tener explicación que un humano pueda entender sobre cualquier modelo supervisado a través de un modelo local más simple. La explicación se genera para cada predicción realizada. Esta basado en la suposición de que un modelo complejo es lineal en una escala local. En el contexto de LIME: Local: Se refiere a que un modelo simple es lo suficientemente bueno/igual de bueno localmente que uno complejo globalmente. Interpretable: Se refiere a que la explicación debe ser entendida por un ser humano. Model-agnostic: Se refiere a tratar a cualquier modelo de clasificación, complejo o no, como una caja negra a la que metemos observaciones y obtenemos predicciones, no nos interesa cómo genera estas predicciones, lo que nos interesa es generarlas. 13.1.1 Proceso El objetivo de LIME es entender por qué el modelo de machine learning hace cierta predicción, para ello LIME prueba qué pasa con las predicciones si le brindamos al modelo las mismas observaciones pero con variaciones. Genera un nuevo set de datos que consiste de una muestra permutada con sus predicciones originales. Con este nuevo set LIME entrena un modelo interpretable que puede ser: regresión lineal como LASSO regresión logística árboles de decisión Naive Bayes K-NN. etc. Este modelo interpretable es ponderado por la proximidad de las observaciones en la muestra a la instancia de interés (la predicción que queremos explicar). El modelo generado debe ser una buena aproximación al modelo de caja negra localmente, pero no necesariamente en forma global. A esto se le llama local fidelity. Para entrenar el modelo interpretable, seguimos los siguientes pasos: Seleccionamos la observación de la que queremos una explicación. Seleccionamos una vecindad para que LIME ajuste el modelo. Seleccionamos el número de features más importantes con los que queremos realizar la explicación (se recomienda que sea menor a \\(10\\)). LIME genera perturbaciones en una muestra de datos del conjunto de datos Texto: “Agrega” o “quita” palabras del texto original aleatoriamente. La columna de probabilidad corresponde a la probabilidad de que el enunciado sea Spam o no. La columna peso corresponde a la proximidad del enunciado con variación al enunciado original y está calculado como 1 menos la proporción de palabras que fueron eliminadas. Por ejemplo: Si una de 7 palabras fueron removidas, el peso correspondería a: \\(1- 1/7=0.86\\). Al pasar dos enunciados al modelo de interpretabilidad se identifica que para los casos donde está la palabra channel! el enunciado será clasificado como spam que es el feature con mayor peso para la etiqueta spam. Imágenes: “Apaga” y “prende” pixeles de la imagen original. Datos tabulares: De cada feature genera nuevas muestras tomadas de una distribución normal con la \\(\\mu\\) y \\(\\sigma\\) del feature. Las observaciones más “cercanas” tendrán más peso que el resto. LIME ocupa un kernel de suavizamiento exponencial. La implementación de LIME ocupa como kernel \\(0.75\\) veces la raíz cuadrada del número de features que tenga nuestro dataset. ¿Eso es bueno o malo? Hasta el momento no hay una justificación matemática o estadística que justifique el definir ese número. Obtiene las predicciones del modelo de caja negra para las observaciones generadas en el paso \\(3\\). Calcula las distancias entre la predicción original y las obtenidas con los datos modificados. Datos tabulares: Distancia euclidiana por default. Datos numéricos: Se obtiene la media y desviación estándar y se discretiza a sus cuartiles. Datos categóricos: Se calcula la frecuencia de cada valor. Texto: Distancia coseno. Imágenes: Distancia euclidiana. Convierte la distancia a un score de similitud. Datos tabulares: Utiliza el kernel de suavizamiento exponencial. Texto: Distancia coseno. Imágenes: Genera un modelo simple con los datos modificados Selecciona las \\(m\\) mejores características (depende del modelo utilizado para crear el modelo de interpretabilidad) como explicación para la predicción del modelo de caja negra. 13.1.2 Características principales Vecindad: kernel con suavizamiento exponencial. Peso de influencia Similitud: Con respecto a las observaciones originales. Selección de variables: Las variables que explican la predicción generada. Ventajas: Fácil de implementar. Se puede ocupar en datos tabulares, imágenes y texto. Existen paquetes de implementación para R (lime) y Python (lime). Las explicaciones son cortas (pocos features), por lo que son más fáciles de entender por un humano no entrenado en machine learning. La métrica de fidelity nos permite identificar qué tan confiable es el modelo de interpretabilidad en explicar las predicciones del modelo de caja negra en las vecindad del punto de interés. Se pueden ocupar otros features en el modelo de interpretabilidad que los ocupados para entrenar el de caja negra. Desventajas: La selección correcta de vecindad es el problema más grave de LIME. Para minimizar este problema deberemos probar con diferentes vecindades y ver cuál(es) son las de mayor sentido. Variación de las observaciones originales: ¿Qué tal que la distribución no es normal? Al muestrear de una distribución gaussiana podemos ignorar correlaciones entre features. Dependiendo del tamaño de la vecindad tendremos resultados muy diferentes. Este es el mayor problema de LIME. 13.1.3 Implementación con R library(pdp) library(vip) library(randomForest) library(lime) set.seed(123) model_rf &lt;- randomForest( x = dplyr::select(telco_train, -one_of(&quot;customerID&quot;, &quot;Churn&quot;)), y = telco_train$Churn, ntree = 100 ) explainer_caret &lt;- lime::lime(dplyr::select(telco_train, -one_of(&quot;customerID&quot;, &quot;Churn&quot;)), model_rf) Veamos cómo llegaron a ser la predicciones, usando Lime. model_type.randomForest &lt;- function(x,...){ return(&quot;classification&quot;) } predict_model.randomForest &lt;- function(x, newdata, type = &quot;prob&quot;) { predict(x, newdata, type = type) %&gt;% as.data.frame() } set.seed(123) new_data &lt;- telco_train %&gt;% dplyr::select(-one_of(&quot;customerID&quot;, &quot;Churn&quot;)) %&gt;% .[2,] new_data %&gt;% t() ## [,1] ## TotalCharges 0.4733424 ## MonthlyCharges 0.6636772 ## SeniorCitizen -0.4417148 ## Contract_One.year 1.0000000 ## Contract_Two.year 0.0000000 explanation &lt;- lime::explain( x = new_data, explainer = explainer_caret, feature_select = &quot;auto&quot;, # Método de selección de variables n_features = 10, # Número de características para explicar el modelo n_labels = 1 ) explanation ## # A tibble: 5 × 13 ## model_t…¹ case label label…² model…³ model…⁴ model…⁵ feature featu…⁶ featur…⁷ ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 classifi… 1 No 0.99 0.140 0.772 0.990 TotalC… 0.473 1.28e-1 ## 2 classifi… 1 No 0.99 0.140 0.772 0.990 Monthl… 0.664 -1.43e-1 ## 3 classifi… 1 No 0.99 0.140 0.772 0.990 Senior… -0.442 1.45e-4 ## 4 classifi… 1 No 0.99 0.140 0.772 0.990 Contra… 1 2.43e-1 ## 5 classifi… 1 No 0.99 0.140 0.772 0.990 Contra… 0 -1.00e-2 ## # … with 3 more variables: feature_desc &lt;chr&gt;, data &lt;list&gt;, prediction &lt;list&gt;, ## # and abbreviated variable names ¹​model_type, ²​label_prob, ³​model_r2, ## # ⁴​model_intercept, ⁵​model_prediction, ⁶​feature_value, ⁷​feature_weight Para obtener una representación más intuitiva, podemos usar plot_features() proporcionado para obtener una descripción visual de las explicaciones. plot_features(explanation, ncol = 1) Otro ejemplo set.seed(123) new_data2 &lt;- telco_train %&gt;% dplyr::select(-one_of(&quot;customerID&quot;, &quot;Churn&quot;)) %&gt;% .[c(2, 4),] new_data2 %&gt;% t() ## [,1] [,2] ## TotalCharges 0.4733424 -0.8504306 ## MonthlyCharges 0.6636772 0.6487142 ## SeniorCitizen -0.4417148 2.2634452 ## Contract_One.year 1.0000000 0.0000000 ## Contract_Two.year 0.0000000 0.0000000 explanation2 &lt;- lime::explain( x = new_data2, explainer = explainer_caret, feature_select = &quot;auto&quot;, # Método de selección de variables n_features = 10, # Número de características para explicar el modelo n_labels = 1 ) plot_features(explanation2, ncol = 2) 13.2 Interpretabilidad DALEXtra El marco tidymodels no contiene software para explicaciones de modelos. En cambio, los modelos entrenados y evaluados con tidymodels se pueden explicar con paquetes complementarios como lime, vip y DALEX: DALEX: Contiene funciones que usamos cuando queremos usar métodos explicativos independientes del modelo, por lo que se pueden aplicar a cualquier algoritmo. Construyamos explicaciones independientes del modelo de regresión de XGBoost para descubrir por qué hacen las predicciones que hacen. Podemos usar el paquete adicional DALEXtra para implementar DALEX, que brinda soporte para tidymodels. # Se declara el modelo de clasificación xgboost_reg_model &lt;- boost_tree( mode = &quot;regression&quot;, trees = 1000, tree_depth = tune(), min_n = tune(), loss_reduction = tune(), sample_size = tune(), mtry = tune(), learn_rate = tune() ) %&gt;% set_engine(&quot;xgboost&quot;) # Se declara el flujo de trabajo xgboost_workflow &lt;- workflow() %&gt;% add_model(xgboost_reg_model) %&gt;% add_recipe(receta_casas_prep) xgboost_tune_result &lt;- readRDS(&quot;models/xgboost_model_reg.rds&quot;) best_xgboost_model_1se &lt;- select_by_one_std_err( xgboost_tune_result, metric = &quot;rmse&quot;, &quot;rmse&quot;) # Selección del mejor modelo final_xgboost_model &lt;- xgboost_workflow %&gt;% finalize_workflow(best_xgboost_model_1se) %&gt;% fit(data = ames_train) library(DALEXtra) explainer_xgb &lt;- explain_tidymodels( final_xgboost_model, data = ames_train, y = ames_train$Sale_Price , label = &quot;XGBoost&quot;, verbose = TRUE ) ## Preparation of a new explainer is initiated ## -&gt; model label : XGBoost ## -&gt; data : 2197 rows 74 cols ## -&gt; data : tibble converted into a data.frame ## -&gt; target variable : 2197 values ## -&gt; predict function : yhat.workflow will be used ( default ) ## -&gt; predicted values : No value for predict function target column. ( default ) ## -&gt; model_info : package tidymodels , ver. 1.0.0 , task regression ( default ) ## -&gt; predicted values : numerical, min = 25723.63 , mean = 180537.9 , max = 563293.2 ## -&gt; residual function : difference between y and yhat ( default ) ## -&gt; residuals : numerical, min = -150327.6 , mean = 19.60113 , max = 210721.6 ## A new explainer has been created! Las explicaciones del modelo local proporcionan información sobre una predicción para una sola observación. Por ejemplo, consideremos una casa dúplex antigua en el vecindario de North Ames. duplex &lt;- ames_train[537,] duplex %&gt;% glimpse() ## Rows: 1 ## Columns: 74 ## $ MS_SubClass &lt;fct&gt; Duplex_All_Styles_and_Ages ## $ MS_Zoning &lt;fct&gt; Residential_Low_Density ## $ Lot_Frontage &lt;dbl&gt; 60 ## $ Lot_Area &lt;int&gt; 7200 ## $ Street &lt;fct&gt; Pave ## $ Alley &lt;fct&gt; No_Alley_Access ## $ Lot_Shape &lt;fct&gt; Regular ## $ Land_Contour &lt;fct&gt; Lvl ## $ Utilities &lt;fct&gt; AllPub ## $ Lot_Config &lt;fct&gt; Inside ## $ Land_Slope &lt;fct&gt; Gtl ## $ Neighborhood &lt;fct&gt; North_Ames ## $ Condition_1 &lt;fct&gt; Norm ## $ Condition_2 &lt;fct&gt; Norm ## $ Bldg_Type &lt;fct&gt; Duplex ## $ House_Style &lt;fct&gt; One_Story ## $ Overall_Cond &lt;fct&gt; Average ## $ Year_Built &lt;int&gt; 1949 ## $ Year_Remod_Add &lt;int&gt; 1950 ## $ Roof_Style &lt;fct&gt; Gable ## $ Roof_Matl &lt;fct&gt; CompShg ## $ Exterior_1st &lt;fct&gt; BrkFace ## $ Exterior_2nd &lt;fct&gt; Stone ## $ Mas_Vnr_Type &lt;fct&gt; None ## $ Mas_Vnr_Area &lt;dbl&gt; 0 ## $ Exter_Cond &lt;fct&gt; Typical ## $ Foundation &lt;fct&gt; Slab ## $ Bsmt_Cond &lt;fct&gt; No_Basement ## $ Bsmt_Exposure &lt;fct&gt; No_Basement ## $ BsmtFin_Type_1 &lt;fct&gt; No_Basement ## $ BsmtFin_SF_1 &lt;dbl&gt; 5 ## $ BsmtFin_Type_2 &lt;fct&gt; No_Basement ## $ BsmtFin_SF_2 &lt;dbl&gt; 0 ## $ Bsmt_Unf_SF &lt;dbl&gt; 0 ## $ Total_Bsmt_SF &lt;dbl&gt; 0 ## $ Heating &lt;fct&gt; Wall ## $ Heating_QC &lt;fct&gt; Fair ## $ Central_Air &lt;fct&gt; N ## $ Electrical &lt;fct&gt; FuseF ## $ First_Flr_SF &lt;int&gt; 1040 ## $ Second_Flr_SF &lt;int&gt; 0 ## $ Gr_Liv_Area &lt;int&gt; 1040 ## $ Bsmt_Full_Bath &lt;dbl&gt; 0 ## $ Bsmt_Half_Bath &lt;dbl&gt; 0 ## $ Full_Bath &lt;int&gt; 2 ## $ Half_Bath &lt;int&gt; 0 ## $ Bedroom_AbvGr &lt;int&gt; 2 ## $ Kitchen_AbvGr &lt;int&gt; 2 ## $ TotRms_AbvGrd &lt;int&gt; 6 ## $ Functional &lt;fct&gt; Typ ## $ Fireplaces &lt;int&gt; 0 ## $ Garage_Type &lt;fct&gt; Detchd ## $ Garage_Finish &lt;fct&gt; Unf ## $ Garage_Cars &lt;dbl&gt; 2 ## $ Garage_Area &lt;dbl&gt; 420 ## $ Garage_Cond &lt;fct&gt; Typical ## $ Paved_Drive &lt;fct&gt; Paved ## $ Wood_Deck_SF &lt;int&gt; 0 ## $ Open_Porch_SF &lt;int&gt; 0 ## $ Enclosed_Porch &lt;int&gt; 0 ## $ Three_season_porch &lt;int&gt; 0 ## $ Screen_Porch &lt;int&gt; 0 ## $ Pool_Area &lt;int&gt; 0 ## $ Pool_QC &lt;fct&gt; No_Pool ## $ Fence &lt;fct&gt; No_Fence ## $ Misc_Feature &lt;fct&gt; None ## $ Misc_Val &lt;int&gt; 0 ## $ Mo_Sold &lt;int&gt; 6 ## $ Year_Sold &lt;int&gt; 2009 ## $ Sale_Type &lt;fct&gt; WD ## $ Sale_Condition &lt;fct&gt; Normal ## $ Sale_Price &lt;int&gt; 90000 ## $ Longitude &lt;dbl&gt; -93.6089 ## $ Latitude &lt;dbl&gt; 42.03584 Existen múltiples enfoques para comprender por qué un modelo predice un precio determinado para esta casa dúplex. Una se denomina explicación de “desglose” y calcula cómo las contribuciones atribuidas a características individuales cambian la predicción del modelo medio para una observación en particular, como nuestro dúplex. Para el modelo de bosques aleatorios, las variables Total_Bsmt_SF, Gr_Liv_Area y BsmtFin_Type_1 son las que más contribuyen a que el precio baje desde la intercepción. xgb_breakdown &lt;- predict_parts(explainer = explainer_xgb, new_observation = duplex) xgb_breakdown %&gt;% saveRDS(&quot;models/xgb_breakdown.rds&quot;) xgb_breakdown &lt;- readRDS(&#39;models/xgb_breakdown.rds&#39;) ¡¡ TEORÍA !! La idea detrás de Shapley Additive Explanations (Lundberg y Lee 2017), es que las contribuciones promedio de las características se calculan bajo diferentes combinaciones o “coaliciones” de ordenamientos de características. Calculemos las atribuciones SHAP para nuestra dúplex, usando B = 20 ordenaciones aleatorias. set.seed(1801) shap_duplex &lt;- predict_parts( explainer = explainer_xgb, new_observation = duplex, type = &quot;shap&quot;, B = 20 ) shap_duplex %&gt;% saveRDS(&#39;models/shap_duplex.rds&#39;) Los diagramas de caja de la figura siguiente muestran la distribución de las contribuciones en todos los ordenamientos que probamos, y las barras muestran la atribución promedio para cada variable. shap_duplex &lt;- readRDS(&#39;models/shap_duplex.rds&#39;) shap_duplex2 &lt;- shap_duplex %&gt;% as_tibble() %&gt;% dplyr::filter(contribution !=0) %&gt;% dplyr::arrange(desc(abs(contribution))) shap_duplex2 %&gt;% group_by(variable) %&gt;% mutate(mean_val = mean(contribution)) %&gt;% ungroup() %&gt;% mutate(variable = fct_reorder(variable, abs(mean_val))) %&gt;% ggplot(aes(contribution, variable, fill = mean_val &gt; 0)) + geom_col(data = ~distinct(., variable, mean_val), aes(mean_val, variable), alpha = 0.5) + geom_boxplot(width = 0.5) + scale_fill_viridis_d() + theme(legend.position = &quot;none&quot;) + labs(y = NULL) ¿Qué pasa con una observación diferente en nuestro conjunto de datos? Veamos una casa unifamiliar más grande y nueva en el vecindario de Gilbert. big_house &lt;- ames_train[1671,] big_house %&gt;% glimpse() ## Rows: 1 ## Columns: 74 ## $ MS_SubClass &lt;fct&gt; Two_Story_1946_and_Newer ## $ MS_Zoning &lt;fct&gt; Residential_Low_Density ## $ Lot_Frontage &lt;dbl&gt; 0 ## $ Lot_Area &lt;int&gt; 8068 ## $ Street &lt;fct&gt; Pave ## $ Alley &lt;fct&gt; No_Alley_Access ## $ Lot_Shape &lt;fct&gt; Slightly_Irregular ## $ Land_Contour &lt;fct&gt; Lvl ## $ Utilities &lt;fct&gt; AllPub ## $ Lot_Config &lt;fct&gt; Inside ## $ Land_Slope &lt;fct&gt; Gtl ## $ Neighborhood &lt;fct&gt; Gilbert ## $ Condition_1 &lt;fct&gt; Norm ## $ Condition_2 &lt;fct&gt; Norm ## $ Bldg_Type &lt;fct&gt; OneFam ## $ House_Style &lt;fct&gt; Two_Story ## $ Overall_Cond &lt;fct&gt; Average ## $ Year_Built &lt;int&gt; 2002 ## $ Year_Remod_Add &lt;int&gt; 2002 ## $ Roof_Style &lt;fct&gt; Gable ## $ Roof_Matl &lt;fct&gt; CompShg ## $ Exterior_1st &lt;fct&gt; VinylSd ## $ Exterior_2nd &lt;fct&gt; VinylSd ## $ Mas_Vnr_Type &lt;fct&gt; None ## $ Mas_Vnr_Area &lt;dbl&gt; 0 ## $ Exter_Cond &lt;fct&gt; Typical ## $ Foundation &lt;fct&gt; PConc ## $ Bsmt_Cond &lt;fct&gt; Typical ## $ Bsmt_Exposure &lt;fct&gt; No ## $ BsmtFin_Type_1 &lt;fct&gt; Unf ## $ BsmtFin_SF_1 &lt;dbl&gt; 7 ## $ BsmtFin_Type_2 &lt;fct&gt; Unf ## $ BsmtFin_SF_2 &lt;dbl&gt; 0 ## $ Bsmt_Unf_SF &lt;dbl&gt; 1010 ## $ Total_Bsmt_SF &lt;dbl&gt; 1010 ## $ Heating &lt;fct&gt; GasA ## $ Heating_QC &lt;fct&gt; Excellent ## $ Central_Air &lt;fct&gt; Y ## $ Electrical &lt;fct&gt; SBrkr ## $ First_Flr_SF &lt;int&gt; 1010 ## $ Second_Flr_SF &lt;int&gt; 1257 ## $ Gr_Liv_Area &lt;int&gt; 2267 ## $ Bsmt_Full_Bath &lt;dbl&gt; 0 ## $ Bsmt_Half_Bath &lt;dbl&gt; 0 ## $ Full_Bath &lt;int&gt; 2 ## $ Half_Bath &lt;int&gt; 1 ## $ Bedroom_AbvGr &lt;int&gt; 4 ## $ Kitchen_AbvGr &lt;int&gt; 1 ## $ TotRms_AbvGrd &lt;int&gt; 8 ## $ Functional &lt;fct&gt; Typ ## $ Fireplaces &lt;int&gt; 1 ## $ Garage_Type &lt;fct&gt; BuiltIn ## $ Garage_Finish &lt;fct&gt; RFn ## $ Garage_Cars &lt;dbl&gt; 2 ## $ Garage_Area &lt;dbl&gt; 390 ## $ Garage_Cond &lt;fct&gt; Typical ## $ Paved_Drive &lt;fct&gt; Paved ## $ Wood_Deck_SF &lt;int&gt; 120 ## $ Open_Porch_SF &lt;int&gt; 46 ## $ Enclosed_Porch &lt;int&gt; 0 ## $ Three_season_porch &lt;int&gt; 0 ## $ Screen_Porch &lt;int&gt; 0 ## $ Pool_Area &lt;int&gt; 0 ## $ Pool_QC &lt;fct&gt; No_Pool ## $ Fence &lt;fct&gt; No_Fence ## $ Misc_Feature &lt;fct&gt; None ## $ Misc_Val &lt;int&gt; 0 ## $ Mo_Sold &lt;int&gt; 12 ## $ Year_Sold &lt;int&gt; 2009 ## $ Sale_Type &lt;fct&gt; ConLI ## $ Sale_Condition &lt;fct&gt; Normal ## $ Sale_Price &lt;int&gt; 200000 ## $ Longitude &lt;dbl&gt; -93.64331 ## $ Latitude &lt;dbl&gt; 42.05938 Calculamos las atribuciones promedio SHAP de la misma manera. set.seed(1802) shap_house &lt;- predict_parts( explainer = explainer_xgb, new_observation = big_house, type = &quot;shap&quot;, B = 20 ) shap_house %&gt;% saveRDS(&#39;models/shap_house.rds&#39;) shap_house &lt;- readRDS(&#39;models/shap_house.rds&#39;) shap_house2 &lt;- shap_house %&gt;% as_tibble() %&gt;% dplyr::filter(contribution !=0) %&gt;% dplyr::arrange(desc(abs(contribution))) shap_house2 %&gt;% group_by(variable) %&gt;% mutate(mean_val = mean(contribution)) %&gt;% ungroup() %&gt;% mutate(variable = fct_reorder(variable, abs(mean_val))) %&gt;% ggplot(aes(contribution, variable, fill = mean_val &gt; 0)) + geom_col(data = ~distinct(., variable, mean_val), aes(mean_val, variable), alpha = 0.5) + geom_boxplot(width = 0.5) + scale_fill_viridis_d() + theme(legend.position = &quot;none&quot;) + labs(y = NULL) A diferencia del dúplex, el área habitable, año de construcción y área del segundo piso de esta casa contribuyen a que su precio sea más alto. Los paquetes como DALEX y su paquete de soporte DALEXtra y lime se pueden integrar en un análisis de tidymodels para proporcionar este tipo de explicativos de modelos. Las explicaciones del modelo son solo una parte de la comprensión de si su modelo es apropiado y efectivo, junto con las estimaciones del rendimiento del modelo. 13.2.1 Otros métodos Partial Dependence Plot (PDP). Efecto marginal de una o dos variables en la predicción. Accumulated Local Effects (ALE). Cómo las variables influyen en promedio a la predicción. 13.2.2 Consejos Trata cada modelo que desarrollas como una afectación de vida o muerte a un humano directamente. Identifica si estás agregando sesgo, inequidad, injusticia con interpretabilidad. Siempre que tengas un modelo de aprendizaje supervisado genera interpretabilidad en tu proceso de desarrollo. Siempre que tengas un modelo de aprendizaje supervisado genera interpretabilidad para el usuario final. "],["a-b---testing.html", "Capítulo 14 A / B - testing 14.1 Elementos en riesgo 14.2 Costo de retención 14.3 Diseño experimental 14.4 Extracción de muestra 14.5 Estimación muestral", " Capítulo 14 A / B - testing Los modelos de machine learning en la gran mayoría de las ocasiones tienen un propósito comercial. Como científicos de datos, es nuestra responsabilidad ayudar al negocio a tomar mejores decisiones que beneficien a la empresa en donde se ha desarrollado el modelo predictivo. Particularmente, una de las aplicaciones de mayor impacto en los últimos años es la reducción del CHURN El churn rate o tasa de cancelación, es el porcentaje de clientes o suscriptores que dejan de utilizar los servicios que ofrece una empresa. Es bastante conocido en el mundo del marketing que es mucho más caro adquirir nuevos clientes que retener a aquellos con los que ya cuenta la empresa. Es por esta razón que adicional a los esfuerzos de captar nuevos clientes se realizan esfuerzos por retener a los clientes con alta probabilidad de abandonar la empresa. A partir de los modelos de machine learning que han sido estudiados en el curso para detectar la posible inclusión a una categoría (cancelación de clientes) es que se realizará en este capítulo el estudio de técnicas para cuantificar el impacto de los modelos predictivos y de las estrategias de retención. Existen múltiples estrategias que surgen en las áreas de marketing para retener a los clientes. Hay una inversión muy grande de dinero que busca encontrar las mejores ideas que contribuyan a la retención. Sin embargo, poner todas estas ideas y estrategias a trabajar puede ser bastante costoso y riesgoso. ¿Qué pasa si la empresa gasta grandes cantidades de dinero en campañas publicitarias y estas no ayudan a alcanzar la meta? ¿Qué sucede si se le invierte mucho tiempo en refinar la estrategia y esta nunca atrae a los clientes esperados? Este capítulo está destinado a discutir métodos de evaluación de esas ideas, de modo que antes de comprometernos completamente con una campaña e implementarla con todos los clientes, podamos hacer pequeñas pruebas con significancia estadística para determinar cuál de ellas tiene el mejor impacto sobre los objetivos planteados. 14.1 Elementos en riesgo El primer paso es detectar a los posibles canceladores de servicios para la empresa. Esta tarea ha sido ampliamente estudiada en el curso mediante múltiples modelos predictivos, entre los que se encuentran: Regresión bernoulli (logística) Regresión Ridge Regresión Lasso KNN Árbol de decisión Bagging Bosque aleatorio Boosting Stacking Recordemos que anteriormente ya se ha realizado la partición de los datos y se cuenta con varias configuraciones de cada modelo. library(tidyverse) library(tidymodels) library(readr) library(patchwork) telco &lt;- read_csv(&quot;data/Churn.csv&quot;) set.seed(1234) telco_split &lt;- initial_split(telco, prop = .70) telco_train &lt;- training(telco_split) telco_test &lt;- testing(telco_split) knn_tune_class_result &lt;- readRDS(&quot;models/knn_model_cla.rds&quot;) A partir de los resultados de los modelos, se realizó una priorización para determinar cuál de ellos eran los más valiosos en términos de predicción. show_best(knn_tune_class_result, n = 10, metric = &quot;roc_auc&quot;) ## # A tibble: 10 × 8 ## K weight_func .metric .estimator mean n std_err .config ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 75 gaussian roc_auc binary 0.831 10 0.00452 Preprocessor1_Model… ## 2 79 cos roc_auc binary 0.831 10 0.00458 Preprocessor1_Model… ## 3 59 cos roc_auc binary 0.831 10 0.00442 Preprocessor1_Model… ## 4 67 gaussian roc_auc binary 0.831 10 0.00446 Preprocessor1_Model… ## 5 70 cos roc_auc binary 0.831 10 0.00456 Preprocessor1_Model… ## 6 65 cos roc_auc binary 0.831 10 0.00453 Preprocessor1_Model… ## 7 56 cos roc_auc binary 0.831 10 0.00445 Preprocessor1_Model… ## 8 51 cos roc_auc binary 0.831 10 0.00443 Preprocessor1_Model… ## 9 48 cos roc_auc binary 0.831 10 0.00436 Preprocessor1_Model… ## 10 59 gaussian roc_auc binary 0.830 10 0.00440 Preprocessor1_Model… Y mediante una elección de métrica y método, se seleccionó al más valioso para re-entrenarse usando todos los datos disponibles de entrenamiento. knn_classification_best_model &lt;- select_best(knn_tune_class_result, metric = &quot;roc_auc&quot;) knn_classification_best_model ## # A tibble: 1 × 3 ## K weight_func .config ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 75 gaussian Preprocessor1_Model26 knn_classification_final_model &lt;- knn_workflow %&gt;% finalize_workflow(knn_classification_best_model) %&gt;% parsnip::fit(data = telco_train) Mediante este modelo final, se realizaron las predicciones de cancelación de servicios a clientes de telecomunicaciones. Para estos clientes se cuenta con la respuesta correcta debido a que se trata de los datos de testing. class_results &lt;- predict(knn_classification_final_model, telco_test, type = &quot;prob&quot;) %&gt;% bind_cols( Churn = telco_test$Churn, customerID = telco_test$customerID) %&gt;% relocate(customerID, .before = .pred_No) %&gt;% mutate(Churn = factor(Churn, levels = c(&#39;Yes&#39;, &#39;No&#39;), labels = c(&#39;Yes&#39;, &#39;No&#39;))) class_results ## # A tibble: 2,113 × 4 ## customerID .pred_No .pred_Yes Churn ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 5575-GNVDE 0.969 0.0311 No ## 2 9305-CDSKC 0.279 0.721 Yes ## 3 6713-OKOMC 0.723 0.277 No ## 4 7469-LKBCI 1 0 No ## 5 9959-WOFKT 0.877 0.123 No ## 6 4183-MYFRB 0.519 0.481 No ## 7 1680-VDCWW 0.998 0.00238 No ## 8 6322-HRPFA 0.845 0.155 No ## 9 8665-UTDHZ 0.652 0.348 Yes ## 10 5248-YGIJN 0.991 0.00923 No ## # … with 2,103 more rows Hasta este punto, únicamente se han calculado probabilidades y no se han tomado decisiones sobre la determinación de elementos a quienes se realizará alguna intervención de retención. roc_auc_value &lt;- roc_auc( class_results, truth = Churn, estimate = .pred_Yes ) pr_auc_value &lt;- pr_auc( class_results, truth = Churn, estimate = .pred_Yes ) roc_curve_data &lt;- roc_curve( class_results, truth = Churn, estimate = .pred_Yes ) roc_curve_plot &lt;- roc_curve_data %&gt;% ggplot(aes(x = 1 - specificity, y = sensitivity)) + geom_path(size = 1, colour = &#39;lightblue&#39;) + geom_abline() + coord_equal() + ggtitle(paste0(&quot;ROC Curve &quot;, &quot;(&quot;, round(roc_auc_value$.estimate, 2),&quot;)&quot;)) + theme_minimal() pr_curve_data &lt;- pr_curve( class_results, truth = Churn, estimate = .pred_Yes ) pr_curve_plot &lt;- pr_curve_data %&gt;% ggplot(aes(x = recall, y = precision)) + geom_path(size = 1, colour = &#39;lightblue&#39;) + coord_equal() + ggtitle(paste0(&quot;Precision vs Recall &quot;, &quot;(&quot;, round(pr_auc_value$.estimate, 2),&quot;)&quot;)) + theme_minimal() roc_curve_plot + pr_curve_plot 14.2 Costo de retención Supongamos que existe un presupuesto designado a retener a los clientes de una compañía de telecomunicaciones que son áltamente probables de cancelar su servicio en los siguientes 3 meses. El presupuesto asignado es de $50,000.00 dólares y el equipo el marketing aún no se decide qué ofrecer: 1 mes gratis de servicio 1 celular gratis con valor de $100 dólares (para la empresa) ¿Cuántos clientes podrían ser intervenidos con cada uno de los posibles métodos? Considere los siguientes escenarios: Todas las retenciones son mediante meses gratis de servicio. Todas las retenciones son mediante el celular gratis. Costo de promoción de servicio El primer escenario considera que se obsequie un mes gratis de servicio al renovar un año completo. Para cada cliente se tiene en los datos el pago mensual que realizan, por lo que es posible priorizar de alguna manera a los clientes con mayor necesidad de retención promo_1 &lt;- class_results %&gt;% mutate( MonthlyCharges = telco_test$MonthlyCharges, Expected_Loss = .pred_Yes * MonthlyCharges) %&gt;% arrange(desc(Expected_Loss)) %&gt;% mutate(Budget = cumsum(MonthlyCharges)) %&gt;% filter(Budget &lt;= 50000) promo_1 %&gt;% select(-Churn, -.pred_No) ## # A tibble: 565 × 5 ## customerID .pred_Yes MonthlyCharges Expected_Loss Budget ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1400-MMYXY 0.860 106. 91.1 106. ## 2 5419-CONWX 0.854 99.8 85.2 206. ## 3 2754-SDJRD 0.818 100. 81.9 306. ## 4 6210-KBBPI 0.815 99.4 81.1 405. ## 5 9300-AGZNL 0.830 94 78.0 499. ## 6 2656-TABEH 0.764 100. 76.6 600. ## 7 3178-FESZO 0.764 100. 76.5 700. ## 8 5419-JPRRN 0.754 101. 76.5 801. ## 9 0880-TKATG 0.756 101. 76.5 902. ## 10 7181-BQYBV 0.742 102. 76.0 1005. ## # … with 555 more rows Sin acabarse el presupuesto de $50,000 dólares, el número de clientes a quienes es posible ofrecer la promoción es de 587/2113, lo que representa cerca del 28% de los clientes. promo_1 %&gt;% summarise( min_prob = min(.pred_Yes), mean_monthly_charge = mean(MonthlyCharges), sum_monthly_charge = sum(MonthlyCharges), sum_yearly_charge = sum(MonthlyCharges)*11 ) ## # A tibble: 1 × 4 ## min_prob mean_monthly_charge sum_monthly_charge sum_yearly_charge ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.247 88.4 49973. 549704. Al calcular el valor mínimo de probabilidad de cancelar el servicio, se observa que el umbral se encuentra cercano a 0.20, por lo que este es un buen candidato a usar en futuros meses si se usa este método y se cuenta con tal presupuesto. El gasto mensual promedio de los clientes es de $85 dólares. El costo total para la empresa es de $49,959.75 y el beneficio de invertir esta cantidad es de $549,557 dólares en un año. Este beneficio es el máximo a obtener si todos los clientes aceptaran renovar su suscripción. Será necesario realizar un experimento para comparar el beneficio neto cuando se implementa esta táctica de marketing u alguna otra estrategia. Costo de promoción de producto El segundo método es más sencillo de calcular, pues supone un costo constante para cualquier cliente. Al regalar un producto en donde la empresa paga $100 dólares por celular, se logra ofrecer \\(50000/100 = 500\\) productos. promo_2 &lt;- class_results %&gt;% mutate( MonthlyCharges = telco_test$MonthlyCharges, Profit = 100, Expected_Loss = .pred_Yes * MonthlyCharges) %&gt;% arrange(desc(Expected_Loss)) %&gt;% mutate(Budget = cumsum(Profit)) %&gt;% filter(Budget &lt;= 50000) promo_2 %&gt;% select(-Churn, -.pred_No) ## # A tibble: 500 × 6 ## customerID .pred_Yes MonthlyCharges Profit Expected_Loss Budget ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1400-MMYXY 0.860 106. 100 91.1 100 ## 2 5419-CONWX 0.854 99.8 100 85.2 200 ## 3 2754-SDJRD 0.818 100. 100 81.9 300 ## 4 6210-KBBPI 0.815 99.4 100 81.1 400 ## 5 9300-AGZNL 0.830 94 100 78.0 500 ## 6 2656-TABEH 0.764 100. 100 76.6 600 ## 7 3178-FESZO 0.764 100. 100 76.5 700 ## 8 5419-JPRRN 0.754 101. 100 76.5 800 ## 9 0880-TKATG 0.756 101. 100 76.5 900 ## 10 7181-BQYBV 0.742 102. 100 76.0 1000 ## # … with 490 more rows promo_2 %&gt;% summarise( min_prob = min(.pred_Yes), mean_monthly_charge = mean(MonthlyCharges), sum_monthly_charge = sum(MonthlyCharges), sum_yearly_charge = sum(MonthlyCharges)*12 - sum(Profit) ) ## # A tibble: 1 × 4 ## min_prob mean_monthly_charge sum_monthly_charge sum_yearly_charge ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.290 88.5 44259. 481106. A diferencia del primer método, el umbral de probabilidad con esta segunda campaña es cercano a 0.30, lo que implica mayor precisión a costa de menor cobertura. Adicionalmente, el beneficio total de renovación de contrato es menor que el beneficio alcanzado por el primer método en el escenario utópico en que todos los clientes renuevan. Estos cálculos sirven exclusivamente para acotar los escenarios, pues no es de esperarse que realmente todos los clientes renueven sus contratos. En las siguientes secciones se procede a realizar experimentos para evaluar cuál de las campañas tiene una mayor tasa de retención de clientes. set.seed(13582) results_promo_1 &lt;- promo_1 %&gt;% rowwise() %&gt;% mutate( result = rbinom(1,1,0.40), cancell = if_else(result == 1, &quot;Yes&quot;, &quot;No&quot;)) %&gt;% select(-result) results_promo_1 ## # A tibble: 565 × 8 ## # Rowwise: ## customerID .pred_No .pred_Yes Churn MonthlyCharges Expected_…¹ Budget cancell ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1400-MMYXY 0.140 0.860 Yes 106. 91.1 106. No ## 2 5419-CONWX 0.146 0.854 Yes 99.8 85.2 206. No ## 3 2754-SDJRD 0.182 0.818 No 100. 81.9 306. No ## 4 6210-KBBPI 0.185 0.815 Yes 99.4 81.1 405. Yes ## 5 9300-AGZNL 0.170 0.830 Yes 94 78.0 499. No ## 6 2656-TABEH 0.236 0.764 Yes 100. 76.6 600. No ## 7 3178-FESZO 0.236 0.764 Yes 100. 76.5 700. No ## 8 5419-JPRRN 0.246 0.754 Yes 101. 76.5 801. Yes ## 9 0880-TKATG 0.244 0.756 Yes 101. 76.5 902. No ## 10 7181-BQYBV 0.258 0.742 Yes 102. 76.0 1005. No ## # … with 555 more rows, and abbreviated variable name ¹​Expected_Loss Primero, analicemos el siguiente resultado, el cual corresponde a la efectividad del método de retención por descuento de mensualidad gratis. results_promo_1 %&gt;% group_by(cancell) %&gt;% summarise( n = n(), mean_monthly_charge = mean(MonthlyCharges), sum_monthly_charge = sum(MonthlyCharges), sum_yearly_charge = sum(MonthlyCharges)*11 ) %&gt;% mutate(prop = round(n/sum(n), 2)) %&gt;% relocate(prop, .after = n) ## # A tibble: 2 × 6 ## cancell n prop mean_monthly_charge sum_monthly_charge sum_yearly_charge ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 No 357 0.63 88.7 31662. 348280. ## 2 Yes 208 0.37 88.0 18311. 201424. El contrafactual se refiere a una versión alternativa en la que la alteración (en este caso, método de retención) en la serie de sucesos conduce a un resultado distinto del que realmente ocurrió. En la siguiente tabla, aparecen los resultados del contrafactual (etiqueta original de Churn). promo_1 %&gt;% group_by(Churn) %&gt;% summarise( n = n(), mean_monthly_charge = mean(MonthlyCharges), sum_monthly_charge = sum(MonthlyCharges), sum_yearly_charge = sum(MonthlyCharges)*12 -sum(100) ) %&gt;% mutate(prop = round(n/sum(n), 2)) %&gt;% relocate(prop, .after = n) ## # A tibble: 2 × 6 ## Churn n prop mean_monthly_charge sum_monthly_charge sum_yearly_charge ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Yes 320 0.57 87.6 28020. 336141. ## 2 No 245 0.43 89.6 21953 263336 Hay que observar que se obtiene una mejora en la tasa de cancelación (Churn) cuando se aplica el método de retención basado en descuento de mensualidad. \\[\\text{diferencia} = \\$348,280 - \\$263,336 = \\$84,944\\] Esta diferencia es el verdadero beneficio de haber implementado e invertido en ciencia de datos y machine learning. Por supuesto, este resultado deberá ser mayor al esfuerzo de inversión de todo el proyecto. Quizá en la primera iteración se realiza una inversión mayor, no obstante, a lo largo del tiempo deberá ser rentable para el negocio. Por supuesto, este resultado es teórico, pues es imposible conocer el contrafactual de la forma en que fue presentado anteriormente debido a que una vez que se implementa un método, se vuelve imposible conocer el resultado de haber hecho otra cosa. Este mismo problema se enfrenta en una gran cantidad de estudios médicos, sociales, de diseño de imagen corporativa, etc, por lo que las mismas técnicas estadísticas serán presentadas para dar solución a esta estimación. WHAT IF… ¿Qué pasaría si la tasa de churn fuese mayor a la obtenida al no implementar ninguna campaña? ¿Cómo sabemos que estamos maximizando las ganancias a través de la elección de la mejor campaña de retención? 14.3 Diseño experimental Sería formidable lograr conocer cuál es el mejor método de retención antes de aplicar el ejercicio de retención a todos los clientes. Esto definitivamente ayudaría a maximizar ganancias y evitar tener pérdidas con alguna mala decisión. A través de la experimentación es posible estimar los resultados de diferentes escenarios. Para lograrlo, se requiere de la creación de muestras representativas de la población que simulen con cierto error tolerable el resultado de cada campaña de retención. Para realizar esta práctica es importante introducirnos al tema de: muestreo. 14.3.1 ¿Qué es y para qué sirve muestreo? El muestreo es el proceso de seleccionar un conjunto de individuos de una población con el fin de estudiarlos y poder caracterizar el total de la población. Nos ayuda a obtener información fiable de la población a partir de una muestra de la que extrae inferencias estadísticas con un margen de error medido en términos de probabilidades. En otras palabras, en una investigación por muestreo podremos estudiar el comportamiento y las opiniones de toda una población analizando únicamente una parte de esta, teniendo en cuenta que siempre existirá un margen de error a la hora de realizar dichos cálculos. Ventajas Reducción de costos: Los costos de un estudio serán menores si los datos de interés se pueden obtener a partir de una muestra de la población. Por ejemplo, cuando se realizan estudios de prevalencia de un evento de interés, es más económico medir una muestra representativa de 1500 sujetos de una población, que el total de individuos que la componen. Eficiencia: Al trabajar con un número reducido de sujetos de estudio, representativos de la población; el tiempo necesario para conducir el estudio y obtener resultados y conclusiones será notoriamente menor. Una muestra puede ser obtenida de diferentes maneras: No probabilística. Por cuotas Conveniencia Bola de nieve Discrecional Probabilística Aleatorio Simple Sistemático Estratificado Conglomerados Polietápico Etc. El temario de muestreo es largo, por lo que nuestro enfoque se centrará en el muestreo probabilístico aleatorio simple y el estratificado. El muestreo probabilístico se define como aquél en que todos los individuos de la población tienen una probabilidad de entrar a formar parte de la muestra. Los diseños en que interviene el azar producen muestras representativas la mayoría de las veces. 14.3.2 Muestreo estratificado En este tipo de muestreo la población de estudio se divide en subgrupos o estratos, escogiendo posteriormente una muestra al azar de cada estrato. Esta división suele realizarse según una característica que pueda influir sobre los resultados del estudio. Si la estratificación se realiza respecto una característica se denomina muestreo estratificado simple, y si se realiza respecto a dos o más características se denomina muestreo estratificado compuesto. Ejemplo: Si existen 5 millones de hipertensos en una población y hay un \\(35\\%\\) de pacientes que fuman, podemos estratificar de manera que en nuestra muestra queden representados al igual que en el total de la población, la misma proporción de hipertensos fumadores (\\(35\\%\\)) y de no fumadores (\\(65\\%\\)). 14.3.3 Marco de muestreo El primer paso consiste en crear un marco de muestreo, el cual es el listado de los posibles elementos a ser seleccionados en la muestra. Tiene la característica de que todos los elementos deben pertenecer a la población objetivo. Este marco de muestreo podría llegar a tener los siguientes 3 errores: Duplicidad: Existen elementos repetidos Sobre-representatividad: Existen elementos en el marco que no pertenecen a la población objetivo Sub-representatividad: Existen elementos pertenecientes a la población objetivo que no se encuentran representados en el marco Ante estos problemas, es necesario realizar una limpieza de los datos disponibles para obtener un conjunto lo más adecuado para extraer estos problemas. 14.3.4 Tamaño de muestra Es imprescindible en una buena estimación conocer el tamaño de muestra adecuado que permitirá controlar o cuantificar el error de muestreo. Se presenta a continuación la fórmula estadística que permite conocer el tamaño de muestra necesario para lograr con 95% de confianza (\\(\\alpha\\)) un error de muestreo (e). Esta fórmula corresponde se calcula a partir de un diseño de muestreo aleatorio simple, donde la variable de respuesta es dicotómica (sí o no): \\[m=\\frac{p(1-p)Z_{1-\\alpha/2}^2}{e²};\\] \\[n=\\frac{m}{1+\\frac{m}{N}} \\frac{deff}{(1-TNR)}\\] Donde: N = Es el tamaño de población. n = Tamaño de muestra. p = Proporción esperada (50% por default). \\(Z_{1-\\alpha/2}\\) = Percentil \\(\\alpha/2\\) de la distribución Normal estándar. e = Error de muestreo máximo a tolerar. TNR = Tasa de no respuesta deff = Efecto de diseño (aumento en la varianza por usar un diseño distinto al aleatorio simple) Esta fórmula puede demostrarse a partir del siguiente planteamiento: \\[P[|\\theta-\\hat{\\theta}| &lt; error] = Z_{1-\\alpha/2}\\] Para mayor teoría de muestreo, consultar el libro: Model Assisted Survey Sampling ¡¡ RECORDAR !! En la medida en que se desea disminuir el error de muestreo, es necesario incrementar de manera exponencial el tamaño de muestra. Se presenta a continuación la función creada con R para conocer el tamaño de muestra de una población. Es necesario indicar los parámetros e hipótesis bajo los cuales se realizará dicho cálculo. Posteriormente, se presenta una gráfica que permite conocer el tamaño de muestra de acuerdo con el error de muestreo a tolerar (suponiendo fijos otros parámetros). 14.3.4.1 Implementación en R n_muestra &lt;- function(e = 0.04, p = 0.5, alpha = 0.95, N = 100000, deff = 1.5, tnr = 0.10){ m = p*(1-p)*(qnorm(1- (1-alpha)/2)/e)^2 n = ceiling( (m/(1 + m/N)) * deff/(1 - tnr ) ) return(n) } Ejemplo Suponiendo un tamaño poblacional de 4.5 millones de personas, y con 95% de confianza un error máximo a tolerar de 3.5%, el tamaño de muestra necesario para estimar una proporción es: n_muestra(e = 0.035, p = 0.5, alpha = 0.95, N = 4500000, deff = 1, tnr = 0.20) ## [1] 980 En este ejemplo, por practicidad se redondea el tamaño de la muestra a 1,00 unidades. Veamos otro ejemplo: n_muestra(e = 0.04, p = 0.5, alpha = 0.95, N = 900, deff = 1, tnr = 0.20) ## [1] 451 En este segundo caso, se redondea a 450 casos. Veamos ahora una gráfica que permita analizar la relación existente entre el tamaño de muestra y el error de muestreo. Ahora analizaremos la relación existente entre el tamaño de muestra y el tamaño poblacional cuando fijamos el error de muestreo en 3.5%. Se puede apreciar que a mayor tamaño de población es necesario una muestra más grande, sin embargo, no es un comportamiento lineal. Debido a que una de las condiciones de muestreo es que la selección es aleatoria, existe un punto de saturación de información, en donde no se necesita aumentar más el tamaño de muestra (aunque el tamaño de la población siga creciendo) ¡¡ RECORDAR !! Para lograr estimaciones representativas de ALTA calidad sobre la población, es indispensable realizar un buen diseño de muestreo, de lo contrario los resultados pueden estar sesgados. EJERCICIO Realizar el cálculo del tamaño de muestra necesario para los siguientes escenarios: Muestra representativa de alumnos de esta clase Muestra de clientes probables a cancelar su servicio de telecomunicaciones con los datos vistos en clase (Test) 14.3.5 Distribución muestral Una vez que el tamaño de muestra ha sido calculado, es común que se proceda a distribuirla de una manera estratégica que permita recolectar de manera óptima la información necesaria. Existe distintos tipos de distribución (afijación), las cuales dependen de que existan estratos (grupos) de la población sobre los cuales se desea distribuir la muestra. Entre las más comunes, se encuentran: Afijación simple: En este tipo de afijación, la muestra se distribuye de manera igualitaria entre los distintos grupos. La fórmula es la siguiente: \\[n_h=\\frac{n}{L}\\] Donde: \\(n_h=\\) Es el tamaño de muestra en el h-ésimo estrato \\(n=\\) Es el tamaño de muestra total \\(L=\\) Es el número de estratos Afijación proporcional: Posiblemente es el tipo de afijación más usado. Esta estrategia se usa para distribuir la muestra de manera proporcional a una característica. Por regla general, se distribuye de acuerdo con la distribución poblacional (viviendas o personas) en zonas geográficas. La fórmula es la siguiente: \\[n_h = \\frac{P_h}{P} \\cdot n\\] Donde: \\(n_h=\\) Es el tamaño de muestra ajustado en el h-ésimo estrato \\(n=\\) Es el tamaño de muestra global \\(P_h=\\) Es la población en el h-ésimo estrato \\(P=\\) Es la población total 14.3.5.1 Implementación en R Suponga por un momento que contamos con una característica particular de interés en la que podemos clasificar a la población de clientes, puede ser el sexo, la edad, ingresos o alguna otra clasificación de interés (producto de contratación). marco &lt;- telco_test %&gt;% select(customerID, gender, Contract) %&gt;% mutate( cod_gender = as.numeric(as.factor(gender)), cod_contract = as.numeric(as.factor(Contract)), ) %&gt;% unite(estrato, starts_with(&quot;cod_&quot;), sep = &quot;-&quot;) %&gt;% left_join( class_results %&gt;% select(customerID, .pred_Yes) ) %&gt;% filter(.pred_Yes &gt;= 0.20) marco ## # A tibble: 1,046 × 5 ## customerID gender Contract estrato .pred_Yes ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 9305-CDSKC Female Month-to-month 1-1 0.721 ## 2 6713-OKOMC Female Month-to-month 1-1 0.277 ## 3 4183-MYFRB Female Month-to-month 1-1 0.481 ## 4 8665-UTDHZ Male Month-to-month 2-1 0.348 ## 5 6047-YHPVI Male Month-to-month 2-1 0.508 ## 6 4080-IIARD Female Month-to-month 1-1 0.377 ## 7 3714-NTNFO Female Month-to-month 1-1 0.292 ## 8 1658-BYGOY Male Month-to-month 2-1 0.604 ## 9 7410-OIEDU Male Month-to-month 2-1 0.505 ## 10 2273-QCKXA Male Month-to-month 2-1 0.488 ## # … with 1,036 more rows marco %&lt;&gt;% arrange(estrato) %&gt;% mutate(ID_unit = 1:nrow(marco)) pob_estrato &lt;- marco %&gt;% group_by(gender, Contract, estrato) %&gt;% summarise( n = n(), mean_prob = mean(.pred_Yes, na.rm = T), .groups = &quot;drop&quot; ) %&gt;% arrange(desc(mean_prob)) pob_estrato ## # A tibble: 4 × 5 ## gender Contract estrato n mean_prob ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Male Month-to-month 2-1 498 0.481 ## 2 Female Month-to-month 1-1 468 0.481 ## 3 Female One year 1-2 45 0.246 ## 4 Male One year 2-2 35 0.238 Suponiendo que el tamaño de muestra seleccionado es de 450 individuos y se desea hacer una afijación proporcional, el tamaño de muestra en cada estrato queda asignado de la siguiente forma: distribucion &lt;- pob_estrato %&gt;% mutate( prop = n/sum(n), n_h = round(prop * 450) ) %&gt;% arrange(estrato) distribucion ## # A tibble: 4 × 7 ## gender Contract estrato n mean_prob prop n_h ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Female Month-to-month 1-1 468 0.481 0.447 201 ## 2 Female One year 1-2 45 0.246 0.0430 19 ## 3 Male Month-to-month 2-1 498 0.481 0.476 214 ## 4 Male One year 2-2 35 0.238 0.0335 15 De esta forma, el tamaño final de la muestra será de 449 unidades, repartidos en cada estrato como se muestra anteriormente. 14.3.6 Probabilidades y Factores Dependiendo del diseño de muestreo implementado, a cada unidad se le deberá asignar una probabilidad de ser incluida en la muestra. Entre las más comunes se encuentran: Probabilidades Muestreo Aleatorio Simple sin reemplazo: Ocurre cuando NO se realiza reemplazo de la muestra y un elemento es seleccionado una sola vez. \\[\\pi_k=\\frac{n}{N}\\] En el caso del muestreo estratificado, el muestreo se realiza de manera aleatoria simple dentro de cada estrato, por lo que la probabilidad interna a un estrato debe ser la misma. Factor de expansión El factor de expansión se refiere al número de individuos fuera de la muestra que son representados por cada uno de los individuos incluidos dentro de la muestra, Este factor es calculado para cada elemento muestral. Matemáticamente, este cálculo se realiza a través del inverso de probabilidad de selección: \\[F_k=\\frac{1}{\\pi_k}\\] En R realizaremos automáticamente el cálculo de la probabilidad de inclusión y posteriormente invertiremos esta probabilidad para extraer el factor de expansión. 14.4 Extracción de muestra Una vez definido el diseño de muestreo y tamaño de muestra, procedemos a extraer nuestra muestra de acuerdo con el diseño determinado. En esta sección probaremos 2 formas de extraer la muestra final. 14.4.1 Muestreo estratificado Este tipo de muestreo implica la creación previa de estratos y sus respectivas distribución por algunos de los métodos antes estudiados. Es importante mencionar que las funciones usadas para este tipo de muestreo implican que las unidades muestrales se encuentran ordenadas por estrato de modo que todos los elementos pertenecientes a un mismo estrato se encuentran juntos. set.seed(13258) muestra_estratificada_1 &lt;- strata( data = marco, stratanames = &quot;estrato&quot;, size = distribucion$n_h, method = &quot;srswor&quot;, description = TRUE ) ## Stratum 1 ## ## Population total and number of selected units: 468 201 ## Stratum 2 ## ## Population total and number of selected units: 45 19 ## Stratum 3 ## ## Population total and number of selected units: 498 214 ## Stratum 4 ## ## Population total and number of selected units: 35 15 ## Number of strata 4 ## Total number of selected units 449 muestra_estratificada_1 &lt;- marco %&gt;% filter(ID_unit %in% muestra_estratificada_1$ID_unit) %&gt;% left_join(muestra_estratificada_1, by = c(&quot;ID_unit&quot;, &quot;estrato&quot;)) %&gt;% mutate(factor = 1/Prob) glimpse(muestra_estratificada_1) ## Rows: 449 ## Columns: 9 ## $ customerID &lt;chr&gt; &quot;4080-IIARD&quot;, &quot;8108-UXRQN&quot;, &quot;4846-WHAFZ&quot;, &quot;4412-YLTKF&quot;, &quot;40… ## $ gender &lt;chr&gt; &quot;Female&quot;, &quot;Female&quot;, &quot;Female&quot;, &quot;Female&quot;, &quot;Female&quot;, &quot;Female&quot;,… ## $ Contract &lt;chr&gt; &quot;Month-to-month&quot;, &quot;Month-to-month&quot;, &quot;Month-to-month&quot;, &quot;Mont… ## $ estrato &lt;chr&gt; &quot;1-1&quot;, &quot;1-1&quot;, &quot;1-1&quot;, &quot;1-1&quot;, &quot;1-1&quot;, &quot;1-1&quot;, &quot;1-1&quot;, &quot;1-1&quot;, &quot;1-… ## $ .pred_Yes &lt;dbl&gt; 0.3771216, 0.2912334, 0.3978843, 0.4042027, 0.4086263, 0.31… ## $ ID_unit &lt;int&gt; 4, 7, 10, 11, 12, 13, 14, 17, 18, 22, 26, 27, 29, 30, 32, 3… ## $ Prob &lt;dbl&gt; 0.4294872, 0.4294872, 0.4294872, 0.4294872, 0.4294872, 0.42… ## $ Stratum &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… ## $ factor &lt;dbl&gt; 2.328358, 2.328358, 2.328358, 2.328358, 2.328358, 2.328358,… Finalmente… para extraer estos resultados en un archivo manipulable por nosotros u otras personas, hacemos uso de la función write.csv(), la cual nos permitirá almacenar nuestra muestra en formato de excel con extensión .csv: write.csv(muestra_estratificada_1, &quot;data/muestra_estratificada_1.csv&quot;, row.names = F) Para diseñar la muestra de otro método de marketing para incentivar la re-contratación de servicios se realiza otra extracción independiente de datos set.seed(579315) muestra_estratificada_2 &lt;- strata( data = marco %&gt;% filter(!ID_unit %in% muestra_estratificada_1$ID_unit), stratanames = &quot;estrato&quot;, size = distribucion$n_h, method = &quot;srswor&quot;, description = TRUE ) ## Stratum 1 ## ## Population total and number of selected units: 267 201 ## Stratum 2 ## ## Population total and number of selected units: 26 19 ## Stratum 3 ## ## Population total and number of selected units: 284 214 ## Stratum 4 ## ## Population total and number of selected units: 20 15 ## Number of strata 4 ## Total number of selected units 449 muestra_estratificada_2 &lt;- marco %&gt;% filter(ID_unit %in% muestra_estratificada_2$ID_unit) %&gt;% left_join(muestra_estratificada_2, by = c(&quot;ID_unit&quot;, &quot;estrato&quot;)) %&gt;% left_join(muestra_estratificada_1 %&gt;% distinct(estrato, factor), by = &quot;estrato&quot;) %&gt;% mutate(Prob = 1/factor) glimpse(muestra_estratificada_2) ## Rows: 449 ## Columns: 9 ## $ customerID &lt;chr&gt; &quot;9305-CDSKC&quot;, &quot;6713-OKOMC&quot;, &quot;4183-MYFRB&quot;, &quot;4080-IIARD&quot;, &quot;59… ## $ gender &lt;chr&gt; &quot;Female&quot;, &quot;Female&quot;, &quot;Female&quot;, &quot;Female&quot;, &quot;Female&quot;, &quot;Female&quot;,… ## $ Contract &lt;chr&gt; &quot;Month-to-month&quot;, &quot;Month-to-month&quot;, &quot;Month-to-month&quot;, &quot;Mont… ## $ estrato &lt;chr&gt; &quot;1-1&quot;, &quot;1-1&quot;, &quot;1-1&quot;, &quot;1-1&quot;, &quot;1-1&quot;, &quot;1-1&quot;, &quot;1-1&quot;, &quot;1-1&quot;, &quot;1-… ## $ .pred_Yes &lt;dbl&gt; 0.7214355, 0.2766141, 0.4807612, 0.3771216, 0.8756702, 0.29… ## $ ID_unit &lt;int&gt; 1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 21, 23, 25,… ## $ Prob &lt;dbl&gt; 0.4294872, 0.4294872, 0.4294872, 0.4294872, 0.4294872, 0.42… ## $ Stratum &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… ## $ factor &lt;dbl&gt; 2.328358, 2.328358, 2.328358, 2.328358, 2.328358, 2.328358,… write.csv(muestra_estratificada_2, &quot;data/muestra_estratificada_2.csv&quot;, row.names = F) Estos dos conjuntos muestrales son compartidos con el área de marketing para que a cada uno de ellos les ofrezcan de manera independiente una promoción de retención. Posteriormente, podremos realizar pruebas de hipótesis para muestras independientes o implementar alguna otra metodología estadística. 14.5 Estimación muestral Es posible que al realizar la captura de forma manual, existan diversos errores, entre los cuales destacan: Errores de dedo Faltas de ortografía Ilegibilidad de respuestas Inconsistencias en las respuestas Es posible disminuir estos errores cuando el levantamiento se realiza a través de un sistema en donde el capturista elija la opción de respuesta y no se preste a errores. 14.5.1 Implementación de diseño muestral Habiendo recibido los resultados, es importante crear el diseño de muestreo en R que permite iniciar el análisis de resultados de acuerdo con la ponderación que cada unidad de muestreo obtuvo en el diseño muestral. resultados_promocion_1 &lt;- read_csv(&quot;data/muestra_estratificada_1.csv&quot;) %&gt;% mutate(., Churn = sample(c(&quot;Yes&quot;, &quot;No&quot;), size = nrow(.), replace = T, prob = c(0.35, 0.65)), promotion = &quot;descount&quot;) resultados_promocion_2 &lt;- read_csv(&quot;data/muestra_estratificada_2.csv&quot;) %&gt;% mutate(., Churn = sample(c(&quot;Yes&quot;, &quot;No&quot;), size = nrow(.), replace = T, prob = c(0.15, 0.85)), promotion = &quot;cell phone&quot;) resultados_promocion &lt;- bind_rows(resultados_promocion_1, resultados_promocion_2) Lo primero, es indicar el diseño muestral usado. En R, existe una librería de nombre srvyr que facilita los análisis de resultados muestrales: library(srvyr) design &lt;- resultados_promocion %&gt;% as_survey_design( ids = customerID, weights = factor, strata = estrato, pps = &quot;brewer&quot;, variance = &quot;HT&quot;) %&gt;% as_survey_rep( type = &quot;bootstrap&quot;, replicates = 1000 ) design ## Call: Called via srvyr ## Survey bootstrap with 1000 replicates. ## Data variables: customerID (chr), gender (chr), Contract (chr), estrato (chr), ## .pred_Yes (dbl), ID_unit (dbl), Prob (dbl), Stratum (dbl), factor (dbl), ## Churn (chr), promotion (chr) 14.5.2 Estimación de resultados El diseño de muestreo ha sido creado. Por lo que a partir de este momento comenzaremos a hacer estimaciones estadísticas. Es importante mencionar que la técnica de estimación de resultados corresponde al estimador Horvitz-Thompson, el cual se calcula de la siguiente forma: \\[\\hat{Y}=\\sum_{i=1}^{n}{\\frac{X_i}{\\pi_i}}=\\sum_{i=1}^{n}{X_i \\cdot F_i}\\] Donde: \\(\\hat{Y}=\\) Es la estimación de la variable de interés \\(X_i=\\) Es la variable de interés observada en la muestra \\(\\pi_i=\\) Es la probabilidad de selección del i-ésimo individuo en la muestra \\(F_i=\\) Es el factor de expansión del i-ésimo individuo. Fue calculado como \\(\\frac{1}{\\pi_i}\\) En el caso de proporciones o estimadores de razón, el cálculo se realiza a través de: \\[\\hat{P} = \\frac{\\hat{Y}}{\\hat{N}}; \\quad \\quad \\hat{R} = \\frac{\\hat{Y}}{\\hat{X}}\\] Ahora sí, procedemos a realizar estas estimaciones en R para conocer los resultados de interés. La variable Churn contiene la pregunta de interés: ¿El cliente canceló su servicio? El resultado se muestra a continuación: design %&gt;% group_by(Churn) %&gt;% summarise( prop = survey_prop( na.rm = TRUE, vartype = c(&quot;ci&quot;, &quot;cv&quot;), level = 0.95, proportion = TRUE ) ) ## # A tibble: 2 × 5 ## Churn prop prop_low prop_upp prop_cv ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 No 0.716 0.685 0.745 0.0214 ## 2 Yes 0.284 0.255 0.315 0.0541 Podemos observar que en los resultados obtenemos la proporción puntual y su intervalo de confianza al 95%. Obtenemos también la varianza estimada de nuestra estimación y el coeficiente de variación. Esta última métrica nos permite saber la precisión y confiabilidad de la estimación. En la literatura suelde decirse que de acuerdo al nivel obtenido en este valor, podemos determinar la calidad de estimación: CV &lt; 0.05 \\(---&gt;\\) Excelente CV &lt; 0.10 \\(---&gt;\\) Muy bueno CV &lt; 0.15 \\(---&gt;\\) Bueno CV &lt; 0.25 \\(---&gt;\\) Regular CV &gt; 0.25 \\(---&gt;\\) Malo La clasificación usada anteriormente es una guía. Deberá entenderse únicamente como una sugerencia y no como una regla absoluta. Este criterio suele variar de investigador a investigador. 14.5.2.1 Estimación cruzada Es muy común querer analizar los resultados de manera cruzada con alguna variable de interés. A continuación, se presentará el ejemplo de cómo estimar resultados de la tasa de cancelación de acuerdo con el método promocional ofertado. estimation_1 &lt;- design %&gt;% group_by(promotion, Churn) %&gt;% summarise( prop = survey_prop( na.rm = TRUE, vartype = c(&quot;ci&quot;, &quot;cv&quot;), level = 0.95, proportion = TRUE ) ) %&gt;% ungroup() %&gt;% mutate_at(vars(starts_with(&quot;prop_&quot;)), round, digits = 3) estimation_1 ## # A tibble: 4 × 6 ## promotion Churn prop prop_low prop_upp prop_cv ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 cell phone No 0.833 0.795 0.866 0.021 ## 2 cell phone Yes 0.167 0.134 0.205 0.107 ## 3 descount No 0.599 0.553 0.643 0.038 ## 4 descount Yes 0.401 0.357 0.447 0.057 estimation_1 %&gt;% ggplot(aes(Churn, y = prop, ymin = prop_low, ymax = prop_upp, fill = Churn)) + geom_bar(stat = &quot;identity&quot;, position = &quot;dodge&quot;) + geom_errorbar(width = 0.2) + geom_text(aes(label = percent(prop)), y = 0.05, position = position_dodge(width = 0.9), vjust = 1.5, size = 3, colour=&quot;black&quot;) + scale_fill_manual(aesthetics = &quot;fill&quot;, values = c(&quot;No&quot; = &quot;lightblue&quot;, &quot;Yes&quot; = &quot;red&quot;)) + facet_wrap(~promotion) + ylab(&quot;Percentage&quot;) + ggtitle(&quot;Churn rate by groups&quot;) En caso de que se deseen analizar los resultados mediante una combinación de variables, solo es necesario agregarlas a la agrupación. A continuación, se muestra un ejemplo de agregación por resultado y sexo: estimation_2 &lt;- design %&gt;% group_by(promotion, gender, Churn) %&gt;% summarise( total = survey_total( na.rm = T, vartype = NULL ), prop = survey_prop( na.rm = TRUE, vartype = c(&quot;ci&quot;, &quot;cv&quot;), level = 0.95, proportion = TRUE ) ) %&gt;% ungroup() %&gt;% mutate(total = round(total)) %&gt;% mutate_at(vars(starts_with(&quot;prop_&quot;)), round, digits = 3) estimation_2 ## # A tibble: 8 × 8 ## promotion gender Churn total prop prop_low prop_upp prop_cv ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 cell phone Female No 742 0.830 0.79 0.864 0.022 ## 2 cell phone Female Yes 151 0.170 0.136 0.21 0.11 ## 3 cell phone Male No 130 0.848 0.566 0.96 0.052 ## 4 cell phone Male Yes 23 0.152 0.04 0.434 0.291 ## 5 descount Female No 315 0.614 0.546 0.678 0.055 ## 6 descount Female Yes 198 0.386 0.322 0.454 0.087 ## 7 descount Male No 312 0.585 0.518 0.649 0.057 ## 8 descount Male Yes 221 0.415 0.351 0.482 0.08 estimation_2 %&gt;% ggplot(aes(Churn, y = prop, ymin = prop_low, ymax = prop_upp, fill = Churn)) + geom_bar(stat = &quot;identity&quot;, position = &quot;dodge&quot;) + geom_errorbar(width = 0.2) + geom_text(aes(label = percent(prop)), y = 0.15, position = position_dodge(width = 0.9), vjust = 1.5, size = 3, colour=&quot;black&quot;) + scale_fill_manual(aesthetics = &quot;fill&quot;, values = c(&quot;No&quot; = &quot;lightblue&quot;, &quot;Yes&quot; = &quot;red&quot;)) + facet_wrap(~ promotion + gender, ncol = 2) + ylab(&quot;Percentage&quot;) + ggtitle(&quot;Churn rate by groups and gender&quot;) Aparentemente con el método por descuento la polarización en la tasa de cancelación es más grande para hombres que para mujeres, no obstante, debieran realizarse pruebas de hipótesis de tomar una decisión en cuanto a la proposición mencionada. "],["análisis-de-componentes-principales.html", "Capítulo 15 Análisis de Componentes Principales 15.1 Construcción matemática 15.2 Implementación en R 15.3 Reducción de dimensión 15.4 Representación gráfica", " Capítulo 15 Análisis de Componentes Principales El análisis PCA (por sus siglas en inglés) es una técnica de reducción de dimensión útil tanto para el proceso de análisis exploratorio, el inferencial y predictivo. Es una técnica ampliamente usada en muchos estudios, pues permite sintetizar la información relevante y desechar aquello que no aporta tanto. Es particularmente útil en el caso de conjuntos de datos “amplios” en donde las variables están correlacionadas entre sí y donde se tienen muchas variables para cada observación. En los conjuntos de datos donde hay muchas variables presentes, no es fácil trazar los datos en su formato original, lo que dificulta tener una idea de las tendencias presentes en ellos. PCA permite ver la estructura general de los datos, identificando qué observaciones son similares entre sí y cuáles son diferentes. Esto puede permitirnos identificar grupos de muestras que son similares y determinar qué variables hacen a un grupo diferente de otro. 15.1 Construcción matemática Sea \\(X\\) una matriz de \\(n\\) renglones y \\(p\\) columnas, se denota por \\(X_i\\) a la i-ésima columna que representa una característica del conjunto en su totalidad… Se desean crear nuevas variables llamadas Componentes Principales, las cuales son creadas como combinación lineal (suma ponderada) de las variables originales, por lo que cada una de las variables nuevas contiene parcialmente información de todas las variables originales. \\[Z_1 = a_{11}X_1 +a_{12}X_2 + ... + a_{1p}X_p\\] \\[Z_2 = a_{21}X_1 +a_{22}X_2 + ... + a_{2p}X_p\\] \\[...\\] \\[Z_p = a_{p1}X_1 +a_{p2}X_2 + ... + a_{pp}X_p\\] Donde: \\(Z_i\\) es la iésima componente nueva creada como combinación de las características originales \\(X_1, X_2, ... X_p\\) son las columnas (variables originales) \\(a_{ij}\\) es el peso o aportación de cada columna j a la nueva componente i. Se desea que la primer componente principal capture la mayor varianza posible de todo el conjunto de datos. \\[\\forall i \\in 2,...,p \\quad Var(Z_1)&gt;Var(Z_i)\\] La segunda componente principal deberá SER INDEPENDIENTE de la primera y deberá abarcar la mayor varianza posible del restante. Esta condición se debe cumplir para toda componente i, de tal forma que las nuevas componentes creadas son independientes entre sí y acumulan la mayor proporción de varianza en las primeras de ellas, dejando la mínima proporción de varianza a las últimas componentes. \\[Z_1 \\perp\\!\\!\\!\\perp Z_2 \\quad \\&amp; \\quad Var(Z_1)&gt;Var(Z_2)&gt;Var(Z_i)\\] El punto anterior permite desechar unas cuantas componentes (las últimas) sin perder mucha varianza. ¡¡ RECORDAR !! A través de CPA se logra retener la mayor cantidad de varianza útil pero usando menos componentes que el número de variables originales. Para que este proceso sea efectivo, debe existir ALTA correlación entre las variables originales. Cuando muchas variables se correlacionan entre sí, todas contribuirán fuertemente al mismo componente principal. Cada componente principal suma un cierto porcentaje de la variación total en el conjunto de datos. Cuando sus variables iniciales estén fuertemente correlacionadas entre sí y podrá aproximar la mayor parte de la complejidad de su conjunto de datos con solo unos pocos componentes principales. Agregar componentes adicionales hace que la estimación del conjunto de datos total sea más precisa, pero también más difícil de manejar. 15.2 Implementación en R Para ejemplificar el uso de CPA, usaremos los datos de CONAPO para replicar el índice de marginación social, el cual pretende dar una medida de pobreza por regiones, las cuales pueden ser entidades, municipios, localidades, agebs o incluso manzanas*. Existen MUUUCHAS librerías que facilitan el análisis de componentes principales. En este blog se puede encontrar la diferencia en su implementación. Todas ofrecen resultados útiles y confiables. library(sf) library(magrittr) library(tidymodels) indice_marg &lt;- st_read(&#39;data/IMEF_2010.dbf&#39;, quiet = TRUE) glimpse(indice_marg) ## Rows: 32 ## Columns: 16 ## $ CVE_ENT &lt;chr&gt; &quot;01&quot;, &quot;02&quot;, &quot;03&quot;, &quot;04&quot;, &quot;05&quot;, &quot;06&quot;, &quot;07&quot;, &quot;08&quot;, &quot;09&quot;, &quot;10&quot;, &quot;1… ## $ AÑO &lt;int&gt; 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 20… ## $ POB_TOT &lt;int&gt; 1184996, 3155070, 637026, 822441, 2748391, 650555, 4796580, 34… ## $ ANALF &lt;dbl&gt; 3.274040, 2.600783, 3.234464, 8.370643, 2.645050, 5.157943, 17… ## $ SPRIM &lt;dbl&gt; 14.754823, 12.987567, 14.273833, 22.541207, 12.168029, 18.4761… ## $ OVSDE &lt;dbl&gt; 1.0649743, 0.4322072, 0.9436751, 6.4196750, 1.0916308, 0.68577… ## $ OVSEE &lt;dbl&gt; 0.62347891, 0.94517891, 2.84464884, 2.59080046, 0.53707721, 0.… ## $ OVSAE &lt;dbl&gt; 0.9854257, 3.5616214, 7.0865085, 9.7378176, 1.3908497, 1.17060… ## $ VHAC &lt;dbl&gt; 30.33066, 29.05839, 31.73806, 45.96720, 30.26891, 31.32052, 53… ## $ OVPT &lt;dbl&gt; 1.761813, 3.398537, 5.814081, 4.500699, 1.423701, 4.691477, 15… ## $ PL_5000 &lt;dbl&gt; 25.1626166, 10.3491523, 15.6188287, 30.8755279, 12.1486353, 14… ## $ PO2SM &lt;dbl&gt; 33.64880, 21.86970, 23.29986, 45.51076, 30.04270, 32.04402, 69… ## $ IM &lt;dbl&gt; -0.91086057, -1.14014880, -0.68128749, 0.43357139, -1.14000448… ## $ GM &lt;chr&gt; &quot;Bajo&quot;, &quot;Muy bajo&quot;, &quot;Bajo&quot;, &quot;Alto&quot;, &quot;Muy bajo&quot;, &quot;Bajo&quot;, &quot;Muy a… ## $ LUGAR &lt;int&gt; 28, 30, 23, 10, 29, 26, 2, 21, 32, 15, 14, 1, 6, 27, 22, 8, 19… ## $ NOM_ENT &lt;chr&gt; &quot;Aguascalientes&quot;, &quot;Baja California&quot;, &quot;Baja California Sur&quot;, &quot;C… pca_recipe &lt;- recipe(IM ~ ., data = indice_marg) %&gt;% update_role(NOM_ENT, GM, new_role = &quot;id&quot;) %&gt;% step_normalize(ANALF, SPRIM, OVSDE, OVSEE, OVSAE, VHAC, OVPT, PL_5000, PO2SM) %&gt;% step_pca(ANALF, SPRIM, OVSDE, OVSEE, OVSAE, VHAC, OVPT, PL_5000, PO2SM, num_comp=9, res=&quot;res&quot;) %&gt;% step_rm(LUGAR, AÑO, POB_TOT) %&gt;% prep() juice(pca_recipe) ## # A tibble: 32 × 13 ## CVE_ENT GM NOM_ENT IM PC1 PC2 PC3 PC4 PC5 PC6 ## &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 01 Bajo Aguasc… -0.911 -2.34 -0.227 0.372 0.492 0.264 0.0764 ## 2 02 Muy b… Baja C… -1.14 -2.93 0.595 -0.0597 -0.492 0.291 -0.0508 ## 3 03 Bajo Baja C… -0.681 -1.75 1.37 -0.683 -0.400 -0.304 0.160 ## 4 04 Alto Campec… 0.434 1.12 -0.819 -0.151 -0.271 -0.929 0.178 ## 5 05 Muy b… Coahui… -1.14 -2.93 -0.144 0.157 -0.133 0.0419 -0.0786 ## 6 06 Bajo Colima -0.779 -2.00 0.0316 0.552 -0.136 0.320 -0.729 ## 7 07 Muy a… Chiapas 2.32 5.96 0.132 1.36 -0.0122 -0.673 -0.471 ## 8 08 Bajo Chihua… -0.520 -1.34 1.05 -1.27 0.633 -0.646 -0.387 ## 9 09 Muy b… Distri… -1.48 -3.81 0.110 0.159 -0.453 0.205 -0.355 ## 10 10 Medio Durango 0.0525 0.135 0.675 -1.50 0.929 -0.448 0.146 ## # … with 22 more rows, and 3 more variables: PC7 &lt;dbl&gt;, PC8 &lt;dbl&gt;, PC9 &lt;dbl&gt; Veamos los pasos de esta receta: Primero, debemos decirle a la receta qué datos se usan para predecir la variable de respuesta. Se actualiza el rol de las variables nombre de entidad y grado de marginación con la función NOM_ENT, ya que es una variable que queremos mantener por conveniencia como identificador de filas, pero no son un predictor ni variable de respuesta. Necesitamos centrar y escalar los predictores numéricos, porque estamos a punto de implementar PCA. Finalmente, usamos step_pca() para realizar el análisis de componentes principales. La función prep() es la que realiza toda la preparación de la receta. Una vez que hayamos hecho eso, podremos explorar los resultados del PCA. Comencemos por ver cómo resultó el PCA. Podemos ordenar los resultados mediante la función tidy(), incluido el paso de PCA, que es el segundo paso. Luego hagamos una visualización para ver cómo se ven los componentes. A continuación se muestran la desviación estándar, porcentaje de varianza y porcentaje de varianza acumulada que aporta cada componente principal. summary(pca_recipe$steps[[2]]$res) ## Importance of components: ## PC1 PC2 PC3 PC4 PC5 PC6 PC7 ## Standard deviation 2.572 0.82085 0.7920 0.64640 0.52101 0.44069 0.31797 ## Proportion of Variance 0.735 0.07487 0.0697 0.04643 0.03016 0.02158 0.01123 ## Cumulative Proportion 0.735 0.80990 0.8796 0.92603 0.95619 0.97777 0.98900 ## PC8 PC9 ## Standard deviation 0.25660 0.18201 ## Proportion of Variance 0.00732 0.00368 ## Cumulative Proportion 0.99632 1.00000 Podemos observar que en la primera componente principal, las \\(9\\) variables que utilizó el Consejo Nacional de Población para obtener el Índice de Marginación 2010 aportan de manera positiva en el primer componente principal. library(tidytext) tidied_pca &lt;- tidy(pca_recipe, 2) tidied_pca %&gt;% filter(component %in% paste0(&quot;PC&quot;, 1:4)) %&gt;% group_by(component) %&gt;% top_n(9, abs(value)) %&gt;% ungroup() %&gt;% mutate(terms = reorder_within(terms, abs(value), component)) %&gt;% ggplot(aes(abs(value), terms, fill = value &gt; 0)) + geom_col() + facet_wrap(~component, scales = &quot;free_y&quot;) + scale_y_reordered() + labs( x = &quot;Absolute value of contribution&quot;, y = NULL, fill = &quot;Positive?&quot; )+ theme_minimal() Notamos que las \\(9\\) variables aportan entre el \\(25\\%\\) y el \\(35\\%\\) a la primera componente principal. 15.3 Reducción de dimensión Existe en la literatura basta información sobre el número de componentes a retener en un análisis de PCA. El siguiente gráfico lleva por nombre gráfico de codo y muestra el porcentaje de varianza explicado por cada componente principal. library(factoextra) library(FactoMineR) res.pca &lt;- indice_marg %&gt;% select(ANALF, SPRIM, OVSDE, OVSEE, OVSAE, VHAC, OVPT, PL_5000, PO2SM) %&gt;% as.data.frame() %&gt;% set_rownames(indice_marg$NOM_ENT) %&gt;% FactoMineR::PCA(graph=FALSE) fviz_eig(res.pca, addlabels=TRUE, ylim=c(0, 100)) El gráfico anterior muestra que hay una diferencia muy grande entre la varianza retenida por la 1er componente principal y el resto de las variables. Dependiendo del objetivo del análisis, podrá elegirse el numero adecuado de componentes a retener, no obstante, la literatura sugiere retener 1 o 2 componentes principales. Es posible realizar el proceso de componentes principales y elegir una de las dos opciones siguientes: Especificar el número de componentes a retener Indicar el porcentaje de varianza a alcanzar La segunda opción elegirá tantas componentes como sean necesarias hasta alcanzar el hiperparámetro mínimo indicado. A continuación se ejemplifica: Caso 1: pca_recipe &lt;- recipe(IM ~ ., data = indice_marg) %&gt;% update_role(NOM_ENT, GM, new_role = &quot;id&quot;) %&gt;% step_normalize(ANALF, SPRIM, OVSDE, OVSEE, OVSAE, VHAC, OVPT, PL_5000, PO2SM) %&gt;% step_pca(ANALF, SPRIM, OVSDE, OVSEE, OVSAE, VHAC, OVPT, PL_5000, PO2SM,num_comp=2) %&gt;% step_rm(LUGAR, AÑO, POB_TOT) %&gt;% prep() juice(pca_recipe) ## # A tibble: 32 × 6 ## CVE_ENT GM NOM_ENT IM PC1 PC2 ## &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 01 Bajo Aguascalientes -0.911 -2.34 -0.227 ## 2 02 Muy bajo Baja California -1.14 -2.93 0.595 ## 3 03 Bajo Baja California Sur -0.681 -1.75 1.37 ## 4 04 Alto Campeche 0.434 1.12 -0.819 ## 5 05 Muy bajo Coahuila de Zaragoza -1.14 -2.93 -0.144 ## 6 06 Bajo Colima -0.779 -2.00 0.0316 ## 7 07 Muy alto Chiapas 2.32 5.96 0.132 ## 8 08 Bajo Chihuahua -0.520 -1.34 1.05 ## 9 09 Muy bajo Distrito Federal -1.48 -3.81 0.110 ## 10 10 Medio Durango 0.0525 0.135 0.675 ## # … with 22 more rows Caso 2: pca_recipe &lt;- recipe(IM ~ ., data = indice_marg) %&gt;% update_role(NOM_ENT, GM, new_role = &quot;id&quot;) %&gt;% step_normalize(ANALF, SPRIM, OVSDE, OVSEE, OVSAE, VHAC, OVPT, PL_5000, PO2SM) %&gt;% step_pca(ANALF, SPRIM, OVSDE, OVSEE, OVSAE, VHAC, OVPT, PL_5000, PO2SM,threshold=0.90) %&gt;% step_rm(LUGAR, AÑO, POB_TOT) %&gt;% prep() juice(pca_recipe) ## # A tibble: 32 × 8 ## CVE_ENT GM NOM_ENT IM PC1 PC2 PC3 PC4 ## &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 01 Bajo Aguascalientes -0.911 -2.34 -0.227 0.372 0.492 ## 2 02 Muy bajo Baja California -1.14 -2.93 0.595 -0.0597 -0.492 ## 3 03 Bajo Baja California Sur -0.681 -1.75 1.37 -0.683 -0.400 ## 4 04 Alto Campeche 0.434 1.12 -0.819 -0.151 -0.271 ## 5 05 Muy bajo Coahuila de Zaragoza -1.14 -2.93 -0.144 0.157 -0.133 ## 6 06 Bajo Colima -0.779 -2.00 0.0316 0.552 -0.136 ## 7 07 Muy alto Chiapas 2.32 5.96 0.132 1.36 -0.0122 ## 8 08 Bajo Chihuahua -0.520 -1.34 1.05 -1.27 0.633 ## 9 09 Muy bajo Distrito Federal -1.48 -3.81 0.110 0.159 -0.453 ## 10 10 Medio Durango 0.0525 0.135 0.675 -1.50 0.929 ## # … with 22 more rows Así es como usaremos el análisis de componentes principales para mejorar la estructura de variables que sirven de input para cualquiera de los modelos posteriores. Continuaremos con un paso más de pre-procesamiento antes de comenzar a aprender nuevos modelos. 15.4 Representación gráfica A partir de estas gráficas, se logran realizar simplificaciones o variaciones de gráficas para estudiar posibles agrupaciones, como se muestra en el siguiente gráfico. library(ggrepel) juice(pca_recipe) %&gt;% mutate(GM = factor(GM, levels = c(&quot;Muy alto&quot;, &quot;Alto&quot;, &quot;Medio&quot;, &quot;Bajo&quot;, &quot;Muy bajo&quot;)), ordered = T) %&gt;% ggplot(aes(PC1, PC2, label = NOM_ENT)) + geom_point(aes(color = GM), alpha = 0.7, size = 2) + geom_text_repel() + ggtitle(&quot;Grado de marginación de entidades&quot;) Finalmente, podemos observar como (de izquierda a derecha) los estados con grado de marginación Muy bajo, Bajo, Medio, Alto y Muy Alto respectivamente. juice(pca_recipe) %&gt;% ggplot(aes(x = IM, y = PC1)) + geom_smooth(method = &quot;lm&quot;) + geom_point(size = 2) + ggtitle(&quot;Comparación: Índice Marginación Vs PCA CP1&quot;) "],["clustering-no-jerárquico.html", "Capítulo 16 Clustering No Jerárquico 16.1 Cálculo de distancia 16.2 K - means 16.3 Partitioning Around Medoids (PAM) 16.4 DBSCAN 16.5 Comparación de algoritmos", " Capítulo 16 Clustering No Jerárquico 16.1 Cálculo de distancia Otro parámetro que podemos ajustar para el modelo es la distancia usada, existen diferentes formas de medir qué tan “cerca” están dos puntos entre sí, y las diferencias entre estos métodos pueden volverse significativas en dimensiones superiores. La más utilizada es la distancia euclidiana, el tipo estándar de distancia. \\[d(X,Y) = \\sqrt{\\sum_{i=1}^{n} (x_i-y_i)^2}\\] Otra métrica es la llamada distancia de Manhattan, que mide la distancia tomada en cada dirección cardinal, en lugar de a lo largo de la diagonal. \\[d(X,Y) = \\sum_{i=1}^{n} |x_i - y_i|\\] De manera más general, las anteriores son casos particulares de la distancia de Minkowski, cuya fórmula es: \\[d(X,Y) = (\\sum_{i=1}^{n} |x_i-y_i|^p)^{\\frac{1}{p}}\\] La distancia de coseno es ampliamente en análisis de texto, sistemas de recomendación. \\[d(X,Y)= 1 - \\frac{\\sum_{i=1}^{n}{X_iY_i}}{\\sqrt{\\sum_{i=1}^{n}{X_i^2}}\\sqrt{\\sum_{i=1}^{n}{Y_i^2}}}\\] La distancia de Jaccard es ampliamente usada para medir similitud cuando se trata de variables categóricas. Es usado en análisis de texto y sistemas de recomendación. \\[d(X, Y) = \\frac{X \\cap Y}{X \\cup Y}\\] La distancia de Gower´s mide la similitud entre variables de forma distinta dependiendo del tipo de dato (numérica, nominal, ordinal). \\[D_{Gower}(X_1, X_2) = 1 - \\frac{1}{p} \\sum_{j=1}^{p}{s_j(X_1, X_2)} ; \\quad \\quad s_j(X_1, X_2)=1-\\frac{|y_{1j}-y_{2j}|}{R_j} \\] Para mayor detalle sobre el funcionamiento de la métrica, revisar el siguiente link Un link interesante Otro link interesante 16.1.1 Distancias homogéneas Las distancias basadas en la correlación son ampliamente usadas en múltiples análisis. La función get_dist() puede ser usada para calcular la distancia basada en correlación. Esta medida puede calcularse mediante pearson, spearman o kendall. library(factoextra) USArrests_scaled &lt;- scale(USArrests) dist.cor &lt;- get_dist(USArrests_scaled, method = &quot;pearson&quot;) round(as.matrix(dist.cor)[1:7, 1:7], 1) ## Alabama Alaska Arizona Arkansas California Colorado Connecticut ## Alabama 0.0 0.7 1.4 0.1 1.9 1.7 1.7 ## Alaska 0.7 0.0 0.8 0.4 0.8 0.5 1.9 ## Arizona 1.4 0.8 0.0 1.2 0.3 0.6 0.8 ## Arkansas 0.1 0.4 1.2 0.0 1.6 1.4 1.9 ## California 1.9 0.8 0.3 1.6 0.0 0.1 0.7 ## Colorado 1.7 0.5 0.6 1.4 0.1 0.0 1.0 ## Connecticut 1.7 1.9 0.8 1.9 0.7 1.0 0.0 16.1.2 Distancias mixtas La naturaleza los datos es muy distinta. Existen datos numéricos, nominales y ordinales que requieren de un procesamiento distinto. Es de particular interés analizar la distancia entre observaciones cuando se trata de variables con diferente naturaleza: numérico - numérico numérico - nominal numérico - ordinal nominal - nominal nominal - ordinal Existe una función en R que detecta la naturaleza de cada variable y calcula la asociación entre individuos. library(cluster) library(dplyr) data(flower) glimpse(flower) ## Rows: 18 ## Columns: 8 ## $ V1 &lt;fct&gt; 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0 ## $ V2 &lt;fct&gt; 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0 ## $ V3 &lt;fct&gt; 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1 ## $ V4 &lt;fct&gt; 4, 2, 3, 4, 5, 4, 4, 2, 3, 5, 5, 1, 1, 4, 3, 4, 2, 2 ## $ V5 &lt;ord&gt; 3, 1, 3, 2, 2, 3, 3, 2, 1, 2, 3, 2, 2, 2, 2, 2, 2, 1 ## $ V6 &lt;ord&gt; 15, 3, 1, 16, 2, 12, 13, 7, 4, 14, 8, 9, 6, 11, 10, 18, 17, 5 ## $ V7 &lt;dbl&gt; 25, 150, 150, 125, 20, 50, 40, 100, 25, 100, 45, 90, 20, 80, 40, 20… ## $ V8 &lt;dbl&gt; 15, 50, 50, 50, 15, 40, 20, 15, 15, 60, 10, 25, 10, 30, 20, 60, 60,… Como puede observarse, cada una de las variables anteriores tiene una naturaleza distinta. La función daisy() calcula la matriz de disimilaridades de acuerdo con la metodología correspondiente a cada par de variables. dd &lt;- daisy(flower) round(as.matrix(dd)[1:10, 1:10], 2) ## 1 2 3 4 5 6 7 8 9 10 ## 1 0.00 0.89 0.53 0.35 0.41 0.23 0.29 0.42 0.58 0.61 ## 2 0.89 0.00 0.51 0.55 0.62 0.66 0.60 0.46 0.43 0.45 ## 3 0.53 0.51 0.00 0.57 0.37 0.30 0.49 0.60 0.45 0.47 ## 4 0.35 0.55 0.57 0.00 0.64 0.42 0.34 0.30 0.81 0.56 ## 5 0.41 0.62 0.37 0.64 0.00 0.34 0.42 0.47 0.33 0.38 ## 6 0.23 0.66 0.30 0.42 0.34 0.00 0.19 0.57 0.51 0.41 ## 7 0.29 0.60 0.49 0.34 0.42 0.19 0.00 0.41 0.59 0.59 ## 8 0.42 0.46 0.60 0.30 0.47 0.57 0.41 0.00 0.64 0.66 ## 9 0.58 0.43 0.45 0.81 0.33 0.51 0.59 0.64 0.00 0.43 ## 10 0.61 0.45 0.47 0.56 0.38 0.41 0.59 0.66 0.43 0.00 16.1.3 Visualización de distancias En cualquier análisis, es de gran valor contar con un gráfico que permita conocer de manera práctica y simple el resumen de distancias. Un mapa de calor es una solución bastante útil, el cual representará de en una escala de color a los elementos cerca y lejos. fviz_dist(dist.cor) El nivel del color es proporcional al valor de disimilaridad entre observaciones. Cuando la distancia es cero, el color es rojo puro y cuando la distancia es amplia, el color es azul puro. Los elementos que pertenecen a un mismo cluster se muestran en orden consecutivo. 16.2 K - means La agrupación en grupos con K-means es uno de los algoritmos de aprendizaje de máquina no supervisados más simples y populares. K-medias es un método de agrupamiento, que tiene como objetivo la partición de un conjunto de n observaciones en k grupos en el que cada observación pertenece al grupo cuyo valor medio es más cercano. Un cluster se refiere a una colección de puntos de datos agregados a a un grupo debido a ciertas similitudes. 16.2.1 Ajuste de modelo: ¿Cómo funciona el algortimo? Paso 1: Seleccionar el número de clusters K El primer paso en k-means es elegir el número de conglomerados, K. Como estamos en un problema de análisis no supervisado, no hay K correcto, existen métodos para seleccionar algún K pero no hay respuesta correcta. Paso 2: Seleccionar K puntos aleatorios de los datos como centroides. A continuación, seleccionamos aleatoriamente el centroide para cada grupo. Supongamos que queremos tener 2 grupos, por lo que K es igual a 2, seleccionamos aleatoriamente los centroides: Paso 3: Asignamos todos los puntos al centroide del cluster más cercano. Una vez que hemos inicializado los centroides, asignamos cada punto al centroide del cluster más cercano: Paso 4: Volvemos a calcular los centroides de los clusters recién formados. Ahora, una vez que hayamos asignado todos los puntos a cualquiera de los grupos, el siguiente paso es calcular los centroides de los grupos recién formados: Paso 5: Repetir los pasos 3 y 4. Criterios de paro: Existen tres criterios de paro para detener el algoritmo: Los centroides de los grupos recién formados no cambian: Podemos detener el algoritmo si los centroides no cambian. Incluso después de múltiples iteraciones, si obtenemos los mismos centroides para todos los clusters, podemos decir que el algoritmo no está aprendiendo ningún patrón nuevo y es una señal para detener el entrenamiento. Los puntos permanecen en el mismo grupo: Otra señal clara de que debemos detener el proceso de entrenamiento si los puntos permanecen en el mismo cluster incluso después de entrenar el algoritmo para múltiples iteraciones. Se alcanza el número máximo de iteraciones: Finalmente, podemos detener el entrenamiento si se alcanza el número máximo de iteraciones. Supongamos que hemos establecido el número de iteraciones en 100. El proceso se repetirá durante 100 iteraciones antes de detenerse. 16.2.2 Calidad de ajuste 16.2.2.1 Inercia La idea detrás de la agrupación de k-medias consiste en definir agrupaciones de modo que se minimice la variación total dentro de la agrupación (conocida como within cluster variation o inertia). Existen distintos algoritmos de k-medias, el algoritmo estándar es el algoritmo de Hartigan-Wong, que define within cluster variation como la suma de las distancias euclidianas entre los elementos y el centroide correspondiente al cuadrado: \\[W(C_k)=\\sum_{x_i \\in C_k}(x_i-\\mu_k)²\\] donde \\(x_i\\) es una observación que pertenece al cluster \\(C_k\\) y \\(\\mu_k\\) es la media del cluster \\(C_k\\) Cada observación \\(x_i\\) se asigna a un grupo de modo que la suma de cuadrados de la distancia de la observación a sus centroide del grupo asignado \\(\\mu_k\\) es mínima. Definimos la total within cluster variation total de la siguiente manera: \\[total \\quad within = \\sum_{k=1}^{K}W(C_k) = \\sum_{k=1}^{K}\\sum_{x_i \\in C_k}(x_i-\\mu_k)²\\] 16.2.3 ¿Cómo seleccionamos K? Una de las dudas más comunes que se tienen al trabajar con K-Means es seleccionar el número correcto de clusters. El número máximo posible de conglomerados será igual al número de observaciones en el conjunto de datos. Pero entonces, ¿cómo podemos decidir el número óptimo de agrupaciones? Una cosa que podemos hacer es trazar un gráfico, también conocido como gráfica de codo, donde el eje x representará el número de conglomerados y el eje y será una métrica de evaluación, en este caso usaremos inertia. Comenzaremos con un valor de K pequeño, digamos 2. Entrenaremos el modelo usando 2 grupos, calculamos la inercia para ese modelo y, finalmente, agregamos el punto en el gráfico mencionado. Digamos que tenemos un valor de inercia de alrededor de 1000: Ahora, aumentaremos el número de conglomerados, entrenaremos el modelo nuevamente y agregaremos el valor de inercia en la gráfica con distintos números de K: Cuando cambiamos el valor de K de 2 a 4, el valor de inercia se redujo de forma muy pronunciada. Esta disminución en el valor de inercia se reduce y eventualmente se vuelve constante a medida que aumentamos más el número de grupos. Entonces, el valor de K donde esta disminución en el valor de inercia se vuelve constante se puede elegir como el valor de grupo correcto para nuestros datos. Aquí, podemos elegir cualquier número de conglomerados entre 6 y 10. Podemos tener 7, 8 o incluso 9 conglomerados. También debe tener en cuenta el costo de cálculo al decidir la cantidad de clusters. Si aumentamos el número de clusters, el costo de cálculo también aumentará. Entonces, si no tiene recursos computacionales altos, deberíamos un número menor de clusters. 16.2.4 Implementación en R Usaremos los datos USArrests, que contiene estadísticas, en arrestos por cada 100,000 residentes por asalto, asesinato y violación en cada uno de los 50 estados de EE. UU. En 1973. También se da el porcentaje de la población que vive en áreas urbanas. data(&quot;USArrests&quot;) head(USArrests) ## Murder Assault UrbanPop Rape ## Alabama 13.2 236 58 21.2 ## Alaska 10.0 263 48 44.5 ## Arizona 8.1 294 80 31.0 ## Arkansas 8.8 190 50 19.5 ## California 9.0 276 91 40.6 ## Colorado 7.9 204 78 38.7 df &lt;-scale(USArrests, center = T, scale = T) df &lt;- na.omit(df) head(df, n = 5) ## Murder Assault UrbanPop Rape ## Alabama 1.24256408 0.7828393 -0.5209066 -0.003416473 ## Alaska 0.50786248 1.1068225 -1.2117642 2.484202941 ## Arizona 0.07163341 1.4788032 0.9989801 1.042878388 ## Arkansas 0.23234938 0.2308680 -1.0735927 -0.184916602 ## California 0.27826823 1.2628144 1.7589234 2.067820292 Usaremos la función kmeans(), los siguientes parámetros son los más usados: X: matriz numérica de datos, o un objeto que puede ser forzado a tal matriz (como un vector numérico o un marco de datos con todas las columnas numéricas). centers: ya sea el número de conglomerados(K), o un conjunto de centros de conglomerados iniciales (distintos). Si es un número, se elige un conjunto aleatorio de observaciones (distintas) en x como centros iniciales. iter.max: el número máximo de iteraciones permitido. nstart: si centers es un número, ¿cuántos conjuntos aleatorios deben elegirse? algorithm: Algoritmo a usar En el siguiente ejemplo se agruparán los datos en seis grupos (centers = 6). Como se había mencionado, la función kmeans también tiene una opción nstart que intenta múltiples configuraciones iniciales y regresa la mejor, agregar nstart = 25 generará 25 configuraciones iniciales. k6 &lt;- kmeans(df, centers = 6, nstart = 25) La salida de kmeans es una lista con distinta información. La más importante: cluster: Un vector de números enteros (de 1:K) que indica el grupo al que se asigna cada punto. centers: una matriz de centros. totss: La suma total de cuadrados. withinss: Vector de suma de cuadrados dentro del grupo, un componente por grupo. tot.withinss: Suma total de cuadrados dentro del conglomerado, es decir, sum(withinss) betweenss: La suma de cuadrados entre grupos, es decir, \\(totss-tot.withinss\\). size: el número de observaciones en cada grupo. También podemos ver nuestros resultados usando la función fviz_cluster(). Esto proporciona una ilustración de los grupos. Si hay más de dos dimensiones (variables), fviz_cluster() realizará un análisis de componentes principales (PCA) y trazará los puntos de datos de acuerdo con los dos primeros componentes principales que explican la mayor parte de la varianza. library(factoextra) fviz_cluster(k6, data = df, ellipse.type = &quot;t&quot;, repel = TRUE) Debido a que el número de conglomerados (K) debe establecerse antes de iniciar el algoritmo, a menudo es recomendado utilizar varios valores diferentes de K y examinar las diferencias en los resultados. Podemos ejecutar el mismo proceso para 3, 4 y 5 clusters, y los resultados se muestran en la siguiente figura: library(patchwork) library(gridExtra) k2 &lt;- kmeans(df, centers = 2, nstart = 25) k3 &lt;- kmeans(df, centers = 3, nstart = 25) k4 &lt;- kmeans(df, centers = 4, nstart = 25) k5 &lt;- kmeans(df, centers = 5, nstart = 25) p2 &lt;- fviz_cluster(k2, geom = &quot;point&quot;, ellipse.type = &quot;t&quot;, data = df) + ggtitle(&quot;K = 2&quot;) p3 &lt;- fviz_cluster(k3, geom = &quot;point&quot;, ellipse.type = &quot;t&quot;, data = df) + ggtitle(&quot;K = 3&quot;) p4 &lt;- fviz_cluster(k4, geom = &quot;point&quot;, ellipse.type = &quot;t&quot;, data = df) + ggtitle(&quot;K = 4&quot;) p5 &lt;- fviz_cluster(k5, geom = &quot;point&quot;, ellipse.type = &quot;t&quot;, data = df) + ggtitle(&quot;K = 5&quot;) grid.arrange(p2, p3, p4, p5, nrow = 2) Recordemos que podemos usar la gráfica de codo para obtener el número óptimo de K, usaremos la función fviz_nbclust() para esto. set.seed(123) wss_plot &lt;- fviz_nbclust(df, kmeans, method = &quot;wss&quot;) wss_plot set.seed(123) final &lt;- kmeans(df, 2, nstart = 25) kmeans_plot &lt;- fviz_cluster( final, data = df, ellipse.type = &quot;t&quot;, repel = TRUE) + ggtitle(&quot;K-Means Plot&quot;) + theme_minimal() + theme(legend.position = &quot;bottom&quot;) kmeans_plot 16.2.5 Warnings Dado que este algoritmo está basado en promedios, debe de ser considerada su sensibilidad a valores atípicos, esto es, si un valor esta lejos del resto, el centroide de un cluster puede cambiar drásticamente y eso significa que también puede incluir dentro del mismo grupo a puntos diferentes de los que de otra manera no serían incluidos en ese conglomerado. 16.3 Partitioning Around Medoids (PAM) El algoritmo k-medoides es un enfoque de agrupamiento para particionar un conjunto de datos en k grupos o clusters. En k-medoides, cada grupo está representado por uno de los puntos de datos pertenecientes a un grupo. Estos puntos son nombrados medoides. El término medoide se refiere a un objeto dentro de un grupo para el cual la disimilitud promedio entre él y todos los demás miembros del cluster son mínimos. Corresponde a el punto más céntrico del grupo. Este algoritmo es una alternativa sólida de k-medias. Debido a que este algoritmo es menos sensible al ruido y los valores atípicos, en comparación con k-medias, pues usa medoides como centros de conglomerados en lugar de medias. El uso de medias implica que la agrupación de k-medias es muy sensible a los valores atípicos, lo cual puede afectar gravemente la asignación de observaciones a los conglomerados. El método de agrupamiento de k-medoides más común es el algoritmo PAM (Partitioning Around Medoids, Kaufman &amp; Rousseeuw, 1990). 16.3.1 Algoritmo PAM El algoritmo PAM se basa en la búsqueda de k objetos representativos o medoides entre las observaciones del conjunto de datos. Después de encontrar un conjunto de k medoides, los grupos se construyen asignando cada observación al medoide más cercano. Posteriormente, cada medoide m y cada punto de datos no medoide seleccionado se intercambian y se calcula la función objetivo. La función objetivo corresponde a la suma de las disimilitudes de todos los objetos a su medoide más cercano. El objetivo es encontrar k objetos representativos que minimicen la suma de disimilitudes de las observaciones con su objeto representativo más cercano. Como se mencionó anteriormente, el algoritmo PAM funciona con una matriz de disimilitud y para calcular esta matriz, el algoritmo puede utilizar dos métricas: La distancia euclidiana, que es la raíz de la suma de cuadrados de las diferencias; Y la distancia de Manhattan, que es la suma de distancias absolutas. Nota: En la práctica, se debería obtener resultados similares la mayor parte del tiempo, utilizando cualquiera de estas distancias mencionadas. Si lo datos contienen valores atípicos, distancia de Manhattan debería dar resultados más sólidos, mientras que la distancia euclidiana se vería influenciada por valores inusuales. 16.3.2 Implementación en R Para estimar el número óptimo de clusters, usaremos el método de silueta promedio. La idea es calcular el algoritmo PAM utilizando diferentes valores de los conglomerados k. Después, la silueta promedio de los conglomerados se dibuja de acuerdo con el número de conglomerados. La silueta media mide la calidad de un agrupamiento. Una silueta media alta indica una buena agrupación. El número óptimo de conglomerados k es el que maximiza la silueta promedio sobre un rango de valores posibles para k La función fviz_nbclust() del paquete factoextra proporciona una solución conveniente para estimar el número óptimo de conglomerados con diferentes métodos. library(cluster) # Elbow method Elbow &lt;- fviz_nbclust(df, pam, method = &quot;wss&quot;) + geom_vline(xintercept = 4, linetype = 2)+ labs(subtitle = &quot;Elbow method&quot;) Elbow ¿Qué se hace en estos casos? Es importante conocer las diferencias entre cada grupo y entender si este resultado tiene sentido desde un enfoque de negocio. Una vez entendiendo la razón por la cual 9 grupos es viable desde múltiples criterios, se deberá tomar la decisión del número adecuado de grupos. Otro criterio importante, es entender los índices y su interpretación. Conocer el significado de cada índice será importante para conocer la validez de su uso y consideración. A partir de la gráfica se observa que la cantidad sugerida de grupos es 2 o 9, por lo que en la siguiente sección se clasificarán las observaciones en 2 grupos. La función pam() del paquete Cluster se puede utilizar para calcular PAM. k_mediods &lt;- pam(df, 2) print(k_mediods) ## Medoids: ## ID Murder Assault UrbanPop Rape ## New Mexico 31 0.8292944 1.3708088 0.3081225 1.1603196 ## Nebraska 27 -0.8008247 -0.8250772 -0.2445636 -0.5052109 ## Clustering vector: ## Alabama Alaska Arizona Arkansas California ## 1 1 1 2 1 ## Colorado Connecticut Delaware Florida Georgia ## 1 2 2 1 1 ## Hawaii Idaho Illinois Indiana Iowa ## 2 2 1 2 2 ## Kansas Kentucky Louisiana Maine Maryland ## 2 2 1 2 1 ## Massachusetts Michigan Minnesota Mississippi Missouri ## 2 1 2 1 1 ## Montana Nebraska Nevada New Hampshire New Jersey ## 2 2 1 2 2 ## New Mexico New York North Carolina North Dakota Ohio ## 1 1 1 2 2 ## Oklahoma Oregon Pennsylvania Rhode Island South Carolina ## 2 2 2 2 1 ## South Dakota Tennessee Texas Utah Vermont ## 2 1 1 2 2 ## Virginia Washington West Virginia Wisconsin Wyoming ## 2 2 2 2 2 ## Objective function: ## build swap ## 1.441358 1.368969 ## ## Available components: ## [1] &quot;medoids&quot; &quot;id.med&quot; &quot;clustering&quot; &quot;objective&quot; &quot;isolation&quot; ## [6] &quot;clusinfo&quot; &quot;silinfo&quot; &quot;diss&quot; &quot;call&quot; &quot;data&quot; La salida impresa muestra: Los medoides del grupo: una matriz, cuyas filas son los medoides y las columnas son variables El vector de agrupación: un vector de números enteros (de \\(1:k\\)) que indica la agrupación a que se asigna a cada punto Para visualizar los resultados de la partición, usaremos la función fviz_cluster() del paquete factoextra. Esta función dibuja un diagrama de dispersión de puntos de datos coloreados por números de grupo. Si los datos contienen más de 2 variables, se utiliza el algoritmo de análisis de componentes principales (PCA) para reducir la dimensionalidad de los datos. En este caso, los dos primeros componentes se utilizan para trazar los datos. pam_plot &lt;- fviz_cluster( k_mediods, palette = c(&quot;#00AFBB&quot;, &quot;#FC4E07&quot;), ellipse.type = &quot;t&quot;, repel = TRUE, ggtheme = theme_minimal()) + ggtitle(&#39;K-Medoids Plot&#39;) + theme(legend.position = &quot;bottom&quot;) pam_plot Ejercicio: Calcular los centroides de cada grupo realizando 2 segmentaciones Calcular los centroides de cada grupo realizando 9 segmentaciones Comparar los resultados anteriores y comentar 16.4 DBSCAN DBSCAN (agrupación espacial basada en densidad y aplicación con ruido), es un algoritmo de agrupamiento basado en densidad, que puede utilizarse para identificar agrupaciones de cualquier forma en un conjunto de datos que contenga ruido y valores atípicos. La idea básica detrás del enfoque de agrupamiento basado en densidad se deriva de un método de agrupamiento intuitivo. Por ejemplo, mirando la siguiente figura, uno puede identificar fácilmente cuatro grupos junto con varios puntos de ruido, debido a las diferencias en la densidad de puntos. Los clusters son regiones densas en el espacio de datos, separadas por regiones de menor densidad de puntos. El algoritmo DBSCAN se basa en esta noción intuitiva de “clusters” y “ruido”. La idea clave es que para cada punto de un grupo, la vecindad de un determinado radio debe contener al menos un número mínimo de puntos. Los métodos de particionamiento vistos anteriormente son adecuados para encontrar grupos de forma esférica o grupos convexos. En otras palabras, ellos funcionan bien solo para grupos compactos y bien separados. Además, también son severamente afectados por la presencia de ruido y valores atípicos en los datos. Desafortunadamente, los datos de la vida real pueden contener: Grupos de forma arbitraria como los como se muestra en la siguiente figura (grupos ovalados, lineales y en forma de “S”). Muchos valores atípicos y ruido. El gráfico anterior contiene 5 grupos y valores atípicos, que incluyen: 2 clusters ovalados 2 clusters lineales 1 cluster compacto Dados los datos “multishapes” del paquete factoextra, el algoritmo de k-medias tiene dificultades para identificar estos grupos con formas arbitrarias. Para ilustrar esta situación, el siguiente código calcula k-medias en el conjunto de datos mencionado. La función fviz_cluster() del paquete factoextra se utiliza para visualizar los clusters. data(&quot;multishapes&quot;) df &lt;- multishapes[, 1:2] set.seed(123) kmeans &lt;- kmeans(df, 5, nstart = 25) fviz_cluster( kmeans, df, geom = &quot;point&quot;, ellipse= FALSE, show.clust.cent = FALSE, palette = &quot;jco&quot;, ggtheme = theme_minimal() ) Sabemos que hay 5 grupos de en los datos, pero se puede ver que el método de k-medias identifica incorrectamente estos 5 grupos. 16.4.1 Algoritmo El objetivo es identificar regiones densas, que se pueden medir por la cantidad de objetos cerca de un punto dado. Se requieren dos parámetros importantes para DBSCAN: epsilon (“eps”): Define el radio de vecindad alrededor un punto x. puntos mínimos (“MinPts”): Es el número mínimo de vecinos dentro del radio “eps”. Cualquier punto x en el conjunto de datos, con un recuento de vecinos mayor o igual que MinPts, es marcado como un punto central. Decimos que x es un punto fronterizo, si el número de sus vecinos es menos que MinPts, pero pertenece a la vecindad de algún punto central z. Finalmente, si un punto no es ni un núcleo ni un punto fronterizo, entonces se denomina punto de ruido o parte aislada. La siguiente figura muestra los diferentes tipos de puntos (puntos centrales, fronterizos y atípicos) usando MinPts = 6. Aquí x es un punto central porque los vecinos \\(s_{\\epsilon}(x) = 6\\), y es un punto fronterizo ya que \\(s_{\\epsilon}(y) &lt; \\text{ MinPts}\\), pero pertenece a la vecindad del punto central x. Finalmente, z es un punto de ruido. Comenzamos definiendo \\(3\\) términos, necesarios para comprender el algoritmo DBSCAN: Densidad directa alcanzable: Un punto \\(A\\) es directamente de densidad alcanzable desde otro punto \\(B\\) si: \\(A\\) está en la vecindad de \\(B\\) y \\(B\\) es un punto central. Densidad alcanzable: Un punto \\(A\\) es la densidad alcanzable desde \\(B\\) si hay un conjunto de los puntos centrales que van de \\(B\\) a \\(A\\). Densidad conectada: Dos puntos \\(A\\) y \\(B\\) están densamente conectados si hay un punto central \\(C\\), de modo que tanto \\(A\\) como \\(B\\) tienen densidad alcanzable desde \\(C\\). Un cluster basado en densidad se define como un grupo de puntos conectados por densidad. El algoritmo de agrupamiento basado en densidad (DBSCAN) funciona de la siguiente manera: Para cada punto \\(x_i\\), calcular la distancia entre \\(x_i\\) y los otros puntos. Hallar todos los puntos vecinos dentro de la distancia eps del punto de partida (\\(x_i\\)). Cada punto con número de vecinos mayor o igual a MinPts, se marca como punto central o visitado. Para cada punto central, si aún no está asignado a un cluster, crear un nuevo cluster. Encuentrar recursivamente todos sus puntos densamente conectados y asignarlos a el mismo grupo que el punto central. Iterar a través de los puntos no visitados restantes en el conjunto de datos. Los puntos que no pertenecen a ningún cluster se tratan como valores atípicos o ruido. 16.4.2 Estimación de parámetros MinPts: Cuanto mayor sea el conjunto de datos, mayor será el valor de minPts. Deben elegirse al menos 3. \\(\\epsilon\\): El valor de \\(\\epsilon\\) se puede elegir mediante un gráfico de distancia \\(k\\), trazando la distancia al vecino más cercano \\(k = minPts\\). Los buenos valores de \\(\\epsilon\\) son donde el gráfico muestra una fuerte curva. 16.4.2.1 Estimación el valor óptimo de \\(\\epsilon\\) El método consiste en calcular las \\(k\\) distancias vecinas más cercanas en una matriz de puntos. La idea es calcular, el promedio de las distancias de cada punto a su \\(k\\) vecino más cercano. El valor de k será especificado por el usuario y corresponde a MinPts. A continuación, estas k-distancias se trazan en orden ascendente. El objetivo es determinar la “rodilla”, que corresponde al parámetro óptimo de eps. Una “rodilla” corresponde a un umbral donde se produce un cambio brusco a lo largo de la curva. La función kNNdistplot() de el paquete dbscan se puede usar para dibujar la distancia-k. library(dbscan) dbscan::kNNdistplot(df, k = 5) abline(h = 0.15, lty = 2) Se puede ver que el valor óptimo de \\(\\epsilon\\) está alrededor de una distancia de \\(0.15\\). 16.4.3 Implementación en R Utilizaremos el paquete fpc para calcular DBSCAN. También es posible utilizar el paquete dbscan, que proporciona una re-implementación más rápida del algoritmo en comparación con el paquete fpc. library(&quot;fpc&quot;) set.seed(123) db &lt;- fpc::dbscan(df, eps = 0.15, MinPts = 5) fviz_cluster( db, data = df, stand = FALSE, ellipse = FALSE, show.clust.cent = FALSE, geom = &quot;point&quot;, palette = &quot;jco&quot;, ggtheme = theme_minimal() ) Nota: La función fviz_cluster() usa diferentes símbolos de puntos para los puntos centrales (es decir, puntos semilla) y puntos fronterizos. Los puntos negros corresponden a valores atípicos. Puede verse que DBSCAN funciona mejor para estos conjuntos de datos y puede identificar el conjunto correcto de clusters en comparación con los algoritmos de k-medias. Los resultados del algoritmo se pueden ver de la siguiente manera print(db) ## dbscan Pts=1100 MinPts=5 eps=0.15 ## 0 1 2 3 4 5 ## border 31 24 1 5 7 1 ## seed 0 386 404 99 92 50 ## total 31 410 405 104 99 51 En la tabla anterior, los nombres de las columnas son el número de grupo. El grupo \\(0\\) corresponde a valores atípicos (puntos negros en el gráfico DBSCAN). 16.4.4 Ventajas de DBSCAN A diferencia de K-medias, DBSCAN no requiere que el usuario especifique el número de clusters que se generarán. DBSCAN puede encontrar cualquier forma de clusters. No es necesario que el grupo sea circular. DBSCAN puede identificar valores atípicos. 16.4.5 Aplicación DBSCAN Aplicaremos ahora el algoritmo DBSCAN a los datos USArrests. Veamos primero el valor óptimo de \\(\\epsilon\\) para estos datos. data(&quot;USArrests&quot;) df &lt;- scale(USArrests) dbscan::kNNdistplot(df, k = 3) abline(h = 1.175, lty = 1.5) Se puede ver que el valor óptimo de \\(\\epsilon\\) está alrededor de una distancia de \\(1.175\\). set.seed(114234) db &lt;- fpc::dbscan(df, eps = 1.175, MinPts = 2) print(db) ## dbscan Pts=50 MinPts=2 eps=1.175 ## 0 1 2 ## border 6 0 0 ## seed 0 7 37 ## total 6 7 37 dbscan_plot &lt;- fviz_cluster( db, data = df, stand = FALSE, axes = c(1,2), repel = TRUE, show.clust.cent = FALSE, geom = &quot;point&quot;, palette = &quot;jco&quot;, ellipse.type = &quot;t&quot;, ggtheme = theme_minimal()) + ggtitle(&#39;DBSCAN Plot&#39;) + theme(legend.position = &quot;bottom&quot;) dbscan_plot 16.5 Comparación de algoritmos Un buen análisis de clustering no sucede sin antes comparar los resultados producidos por los distintos algoritmos. A continuación, se presenta la comparación de las gráficas. Esta comparación visual sirve de apoyo para conocer las diferencias entre los distintos métodos, sin embargo, esto no sustituye al análisis numérico de wss, silhouette o algún otro. kmeans_plot + pam_plot + dbscan_plot Ejercicio Comparar el grado de marginación original de CONAPO con los distintos algoritmos de clustering revisados hasta el momento. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
