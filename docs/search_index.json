[["index.html", "Ciencia de Datos y Machine Learning BIENVENIDA Objetivo Alcances del curso Instructores Temario Duración y evaluación del curso Recursos y dinámica de clase", " Ciencia de Datos y Machine Learning BIENVENIDA Objetivo Capacitar y brindar acompañamiento al equipo de analítica de ASERTA en temas relacionados con Ciencia de Datos para la correcta implementación de proyectos y toma de decisiones basadas en la evidencia de datos internos y externos a la empresa para lograr un beneficio operativo y económico. Alcances del curso El participante conocerá los conceptos teóricos alrededor de esta ciencia y sabrá implementar correctamente un análisis exploratorio estadístico y gráfico que le permita conocer a mayor profundidad los datos a usar. Conocerá y sabrá implementar los modelos predictivos de Machine Learning más usados y de mayor impacto en la industria de seguros y fianzas. Finalmente, sabrá tomar decisiones sobre el correcto uso e implementación de los modelos para aumentar el beneficio comercial dentro de la institución. Instructores ACT. ARTURO BRINGAS LinkedIn: arturo-bringas Email: act.arturo.b@ciencias.unam.mx Actuario egresado de la Facultad de Ciencias y Maestría en Ciencia de Datos por el ITAM. Se especializa en modelos predictivos y de clasificación de machine learning aplicado a seguros, marketing, deportes y movilidad internacional. Es jefe de departamento en Investigación Aplicada y Opinión de la UNAM, donde realiza estudios estadísticos de impacto social. Ha sido consultor Senior Data Scientist para empresas y organizaciones como GNP, El Universal, UNAM, Sinnia, Geekend, la Organización de las Naciones Unidas Contra la Droga y el Delito (UNODC), entre otros. Actualmente es profesor de Ciencia de datos y Machine Learning en AMAT y se desempeña como consultor independiente en diferentes proyectos contribuyendo a empresas en temas de análisis estadístico y ciencia de datos con machine learning. ACT. KARINA LIZETTE GAMBOA LinkedIn: KaLizzyGam Email: lizzygamboa@ciencias.unam.mx Actuaria egresada de la Facultad de Ciencias, por la UNAM y candidata a Maestra en Ciencia de Datos por el ITAM. Experiencia en áreas de analítica predictiva e inteligencia del negocio. Lead y Senior Data Scientist en consultoría en diferentes sectores como tecnología, asegurador, financiero y bancario. Es experta en entendimiento de negocio para la correcta implementación de algoritmos de inteligencia y explotación de datos. Actualmente se desarrolla como Arquitecta de Soluciones Analíticas en Merama, startup mexicana clasificada como uno de los nuevos unicornios de Latinoamérica. Senior Data Science en CLOSTER y como profesora del diplomado de Metodología de la Investigación Social por la UNAM así como instructora de cursos de Ciencia de Datos en AMAT. Empresas anteriores: GNP, Actinver Banco y Casa de Bolsa, PlayCity Casinos, RakenDataGroup Consulting, entre otros. Temario Módulo 1: Introducción a R (22 hrs) Objetivo: A través de este módulo se adquirirán los conocimientos necesarios para la operación del software estadístico y la manipulación ágil de datos. Al finalizar, el participante desarrollará análisis exploratorios y reportes automatizados. Estructuras de almacenamiento de datos Almacenamiento Vectores Matrices Listas DataFrames Funciones y estructuras de control Librerías y funciones Condicionamiento Ciclos Manipulación de estructuras de datos Importación de tablas Consultas y transformación de estructuras Iteraciones Manipulación de texto y datos temporales Análisis exploratorio y visualización de datos Guía de visualización Análisis Exploratorio de Datos (EDA) Análisis Gráfico Exploratorio de Datos (GEDA) Reportes con markdown Consultoría y aplicaciones con datos institucionales Módulo 2: Introducción a Ciencia de Datos (18 hrs) Objetivo: Este módulo presenta los conceptos teóricos clave para conocer los términos, objetivo y alcances de proyectos con enfoque en ciencia de datos. Se presenta el flujo de trabajo y organización que deberá seguir un equipo para obtener el mayor beneficio posible. Adicionalmente, se propone presentar el software git y github para implementar correctamente el trabajo en equipo que garantice la reproducibilidad y seguridad del desarrollo realizado. Introducción a ciencia de datos ¿Qué es la ciencia de datos? Objetivo de ciencia de datos Requisitos y aplicaciones Tipos de algoritmos Ciclo de vida de un proyecto Taller de scoping Perfiles de un equipo de ciencia de datos Concepto de Ciencia de Datos Machine learning Análisis supervisado Análisis no supervisado Sesgo y varianza Pre-procesamiento e ingeniería de datos Partición de datos Colaboración y reproducibilidad Git &amp; Github Ambiente de desarrollo Consultoría y aplicaciones con datos institucionales Módulo 3: Machine Learning: Supervisado (38 hrs) Objetivo: Este módulo está diseñado para adquirir los conocimientos técnicos para conocer e implementar los distintos modelos de aprendizaje supervisado que son aplicados en ciencia de datos a la industria de los seguros y fianzas. Modelos de aprendizaje Supervisado Regresión Lineal Regresión logística Regularización Ridge &amp; Lasso Elasticnet KNN Árbol de decisión Bagging Random Forest Boosting Stacking Toma de decisiones enfocadas a negocio Comparación de modelos Balance entre sesgo y cobertura Cuantificación de sesgo e inequidad Cuantificación de ganancia comercial Diseño experimental Consultoría y aplicaciones con datos institucionales Módulo 4: Machine Learning: No Supervisado Objetivo: Este módulo permite al participante conocer técnicas de clustering para clasificar clientes de acuerdo con la utilidad y riesgo para la empresa. Adicionalmente, se presentan aplicaciones de clustering enfocadas a la estratificación de acuerdo con el riesgo geográfico. Técnicas de reducción de dimensión Análisis de componentes principales Creación de índices Clustering Liga simple, compleja y promedio Dendogramas &amp; heatmaps Kmeans &amp; Kmedoids DBSCAN Consultoría y aplicaciones con datos institucionales Requisitos Computadora con al menos 4Gb Ram. Instalación de R con versión &gt;= 4.1.0 Instalación de Rstudio con versión &gt;= 1.4.17 Conocimientos generales de probabilidad, estadística y álgebra lineal Duración y evaluación del curso El programa tiene una duración de 90 hrs. Las clases serán impartidas los días lunes a viernes, de 7:30 am a 9:30 am Serán asignados ejercicios que el participante deberá resolver entre una semana y otra. Al final del curso se solicitará un proyecto final, el cual deberá ser entregado para ser acreedor a la constancia de participación. “La gota abre la piedra, no por su fuerza sino por su constancia” - Ovidio Recursos y dinámica de clase En esta clase estaremos usando: R (descargar) RStudio (descargar) Zoom Clases Pulgar arriba: Voy bien, estoy entendiendo! Pulgar abajo: Eso no quedó muy claro Mano arriba: Quiero participar/preguntar o Ya estoy listo para iniciar Google Drive Notas de clase Finalmente, se dejarán ejercicios que serán clave para el éxito del aprendizaje de los capítulos, por lo que se trabajará en equipo para lograr adquirir el mayor aprendizaje. "],["introducción-a-ciencia-de-datos.html", "Capítulo 1 Introducción a Ciencia de Datos 1.1 ¿Qué es Ciencia de Datos? 1.2 Objetivos 1.3 Requisitos 1.4 Aplicaciones 1.5 Tipos de algoritmos 1.6 Ciclo de un proyecto 1.7 Taller de Scoping 1.8 Perfiles de un equipo 1.9 Flujo de trabajo en ML", " Capítulo 1 Introducción a Ciencia de Datos 1.1 ¿Qué es Ciencia de Datos? Definiendo conceptos: Estadística Disciplina que recolecta, organiza, analiza e interpreta datos. Lo hace a través de una población muestral generando estadística descriptiva y estadística inferencial. La estadística descriptiva, como su nombre lo indica, se encarga de describir datos y obtener conclusiones. Se utilizan números (media, mediana, moda, mínimo, máximo, etc) para analizar datos y llegar a conclusiones de acuerdo a ellos. La estadística inferencial argumenta o infiere sus resultados a partir de las muestras de una población. Se intenta conseguir información al utilizar un procedimiento ordenado en el manejo de los datos de la muestra. La estadística predictiva busca estimar valores y escenarios futuros más probables de ocurrir a partir de referencias históricas previas. Se suelen ocupar como apoyo características y factores áltamente asociados al fenómeno que se desea predecir. Business Intelligence: BI aprovecha el software y los servicios para transformar los datos en conocimientos prácticos que informan las decisiones empresariales estratégicas y tácticas de una organización. Las herramientas de BI acceden y analizan conjuntos de datos y presentan hallazgos analíticos en informes, resúmenes, tableros, gráficos, cuadros, -indicadores- o KPI’s y mapas para proporcionar a los usuarios inteligencia detallada sobre el estado del negocio. BI esta enfocado en analizar la historia pasada para tomar decisiones hacia el futuro. ¿Qué características tiene un KPI? Específicos Continuos y periódicos Objetivos Cuantificables Medibles Realistas Concisos Coherentes Relevantes Machine Learning: Machine learning –aprendizaje de máquina– es una rama de la inteligencia artificial que permite que las máquinas aprendan de los patrones existentes en los datos. Se usan métodos computacionales para aprender de datos con el fin de producir reglas para mejorar el desempeño en alguna tarea o toma de decisión. (Está enfocado en la programación de máquinas para aprender de los patrones existentes en datos principalmente estructurados y anticiparse al futuro) Deep Learning: El aprendizaje profundo es un subcampo del aprendizaje automático que se ocupa de los algoritmos inspirados en la estructura y función del cerebro llamados redes neuronales artificiales. En Deep Learning, un modelo de computadora aprende a realizar tareas de clasificación directamente a partir de imágenes, texto o sonido. Los modelos de aprendizaje profundo pueden lograr una precisión de vanguardia, a veces superando el rendimiento a nivel humano. Los modelos se entrenan mediante el uso de un gran conjunto de datos etiquetados y arquitecturas de redes neuronales que contienen muchas capas. (Está enfocado en la programación de máquinas para el reconocimiento de imágenes y audio (datos no estructurados)) Big data se refiere a los grandes y diversos conjuntos de información que crecen a un ritmo cada vez mayor. Abarca el volumen de información, la velocidad a la que se crea y recopila, y la variedad o alcance de los puntos de datos que se cubren. Los macrodatos a menudo provienen de la minería de datos y llegan en múltiples formatos. Es común que se confunda los conceptos de Big Data y Big Compute, como se mencionó, Big Data se refiere al procesamiento de conjuntos de datos que son más voluminosos y complejos que los tradicionales y Big Compute a herramientas y enfoques que utilizan una gran cantidad de recursos de CPU y memoria de forma coordinada para resolver problemas que usan algoritmos muy complejos. Curiosidad: Servidores en líquido para ser enfriados Curiosidad 2: Centro de datos en el océano Entonces, ¿qué NO es ciencia de datos? No es una tecnología No es una herramienta No es desarrollo de software No es Business Intelligence* No es Big Data* No es Inteligencia Artificial* No es (solo) machine learning No es (solo) deep learning No es (solo) visualización No es (solo) hacer modelos 1.2 Objetivos Los científicos de datos analizan qué preguntas necesitan respuesta y dónde encontrar los datos relacionados. Tienen conocimiento de negocio y habilidades analíticas, así como la capacidad de extraer, limpiar y presentar datos. Las empresas utilizan científicos de datos para obtener, administrar y analizar grandes cantidades de datos no estructurados. Luego, los resultados se sintetizan y comunican a las partes interesadas clave para impulsar la toma de decisiones estratégicas en la organización. Fuente: Blog post de Drew Conway Más sobre Conway: Forbes 2016 1.3 Requisitos Background científico: Conocimientos generales de probabilidad, estadística, álgebra lineal, cálculo, geometría analítica, programación, conocimientos computacionales… etc Datos relevantes y suficientes: Es indispensable saber si los datos con los que se trabajará son relevantes y suficientes, debemos evaluar qué preguntas podemos responder con los datos con los que contamos. Suficiencia: Los datos con los que trabajamos tienen que ser representativos de la población en general, necesitamos que las características representadas en la información sean suficientes para aproximar a la población objetivo. Relevancia: De igual manera los datos tienen que tener relevancia para la tarea que queremos resolver, por ejemplo, es probable que información sobre gusto en alimentos sea irrelevante para predecir número de hijos. Etiquetas: Se necesita la intervención humana para etiquetar, clasificar e introducir los datos en el algoritmo. Software: Existen distintos lenguajes de programación para realizar ciencia de datos 1.4 Aplicaciones Dependiendo de la industria en la que se quiera aplicar Machine Learning, podemos pensar en distintos enfoques, en la siguiente imagen se muestran algunos ejemplos: Podemos pensar en una infinidad de aplicaciones comerciales basadas en el análisis de datos. Con la intención de estructurar las posibles aplicaciones, se ofrece a continuación una categorización que, aunque no es suficiente para englobar todos los posibles casos de uso, sí es sorprendente la cantidad de aplicaciones que abarca. 1. Aplicaciones centradas en los clientes Incrementar beneficio al mejorar recomendaciones de productos Up-selling Cross-selling Reducir tasas de cancelación y mejorar tasas de retención Personalizar experiencia de usuario Mejorar el marketing dirigido Análisis de sentimientos Personalización de productos o servicios 2. Optimización de problemas Optimización de precios Ubicación de nuevas sucursales Maximización de ganancias mediante producción de materias primas Construcción de portafolios de inversión 3. Predicción de demanda Número futuro de clientes Número esperado de viajes en avión / camión / bicis Número de contagios por un virus (demanda médica / medicamentos / etc) Predicción de uso de recursos (luz / agua / gas) 4. Análisis de detección de fraudes Detección de robo de identidad Detección de transacciones ilícitas Detección de servicios fraudulentos Detección de zonas geográficas con actividades ilícitas 1.5 Tipos de algoritmos Los algoritmos de Machine Learning se dividen en tres categorías, siendo las dos primeras las más comunes: La diferencia entre el análisis supervisado y el no supervisado es la etiqueta, es decir, en el análisis supervisado tenemos una etiqueta “correcta” y el objetivo de los algoritmos es predecir esta etiqueta. 1.5.1 Aprendizaje supervisado En el aprendizaje supervisado, la idea principal es aprender bajo supervisión, donde la señal de supervisión se nombra como valor objetivo o etiqueta. Estos algoritmos cuentan con un aprendizaje previo basado en un sistema de etiquetas asociadas a unos datos que les permiten tomar decisiones o hacer predicciones. Conocemos la respuesta correcta de antemano. Esta respuesta correcta fue “etiquetada” por un humano (la mayoría de las veces, en algunas circunstancias puede ser generada por otro algoritmo). Debido a que conocemos la respuesta correcta, existen muchas métricas de desempeño del modelo para verificar que nuestro algoritmo está haciendo las cosas “bien”. Algunos ejemplos son: - Un detector de spam que etiqueta un e-mail como spam o no. - Predecir precios de casas - Clasificación de imagenes - Predecir el clima - ¿Quiénes son los clientes descontentos? Tipos de aprendizaje supervisado (Regresión vs clasificación) Existen dos tipos principales de aprendizaje supervisado, esto depende del tipo de la variable respuesta: Clasificación En el aprendizaje supervisado, los algoritmos de clasificación se usan cuando el resultado es una etiqueta discreta. Esto quiere decir que se utilizan cuando la respuesta se fundamenta en conjunto finito de resultados. Regresión El análisis de regresión es un subcampo del aprendizaje automático supervisado cuyo objetivo es establecer un método para la relación entre un cierto número de características y una variable objetivo continua. 1.5.2 Aprendizaje no supervisado En el aprendizaje no supervisado, carecemos de etiquetas. Por lo tanto, necesitamos encontrar nuestro camino sin ninguna supervisión ni guía. Esto simplemente significa que necesitamos descubrir ¿qué es qué? por nosotros mismos. Aquí no tenemos la respuesta correcta de antemano ¿cómo podemos saber que el algoritmo está bien o mal? Estadísticamente podemos verificar que el algoritmo está bien Siempre tenemos que verificar con el cliente si los resultados que estamos obteniendo tienen sentido de negocio. Por ejemplo, número de grupos y características Algunos ejemplos son: - Encontrar segmentos de clientes. - Reducir la complejidad de un problema - Selección de variables - Encontrar grupos - Reducción de dimensionalidad 1.5.3 Aprendizaje por refuerzo Su objetivo es que un algoritmo aprenda a partir de la propia experiencia. Esto es, que sea capaz de tomar la mejor decisión ante diferentes situaciones de acuerdo a un proceso de prueba y error en el que se recompensan las decisiones correctas. Algunos ejemplos son: - Optimización de campañas de marketing - Reconocimiento facial - Diagnósticos médicos - Clasificar secuencias de ADN Ejemplo: Mario Bros 1.6 Ciclo de un proyecto Identificación del problema Debemos conocer si el problema es significativo, si el problema se puede resolver con ciencia de datos, y si habrá un compromiso real del lado de cliente/usuario/partner para implementar la solución con todas sus implicaciones: recursos físicos y humanos. Scoping El objetivo es definir el alcance del proyecto y por lo tanto definir claramente los objetivos. Conocer las acciones que se llevarán a cabo para cada objetivo. Estas definirán las soluciones analíticas a hacer. Queremos saber si los datos con los que contamos son relevantes y suficientes. Hacer visible los posibles conflictos éticos que se pueden tener en esta fase. Debemos definir el cómo evaluaremos que el análisis de esos datos será balanceada entre eficiencia, efectividad y equidad. Adquisición de datos Adquisición, almacenamiento, entendimiento y preparación de los datos para después poder hacer analítica sober ellos. Asegurar que en la transferencia estamos cumpliendo con el manejo adecuado de datos sensibles y privados. EDA El objetivo en esta fase es conocer los datos con los que contamos y contexto de negocio explicado a través de los mismos. Identificamos datos faltantes, sugerimos cómo imputarlos. Altamente apoyado de visualización y procesos de adquisición y limpieza de datos. Formulación analítica Esta fase incluye empezar a formular nuestro problema como uno de ciencia de datos, el conocimiento adquirido en la fase de exploración nos permite conocer a mayor detalle del problema y por lo tanto de la solución adecuada. Modelado Proceso iterativo para desarrollar diferentes “experimentos”. Mismo algoritmo/método diferentes hiperparámetros (grid search). Diferentes algortimos. Selección de un muy pequeño conjunto de modelos tomando en cuenta un balance entre interpretabilidad, complejidad, desempeño, fairness. Correcta interpretación de los resultados de desempeño de cada modelo. Validación Es muy importante poner a prueba el/los modelo/modelos seleccionados en la fase anterior. Esta prueba es en campo con datos reales, le llamamos prueba piloto. Debemos medir el impacto causal que nuestro modelo tuvo en un ambiente real. Acciones a realizar Finalmente esta etapa corresponde a compartir con los tomadores de decisiones/stakeholders/creadores de política pública los resultados obtenidos y la recomendación de acciones a llevar a cabo -menú de opciones-. Las implicaciones éticas de esta fase consisten en hacer conciente el impacto social de nuestro trabajo. 1.7 Taller de Scoping El scoping es uno de los pasos más importante en los proyectos de ciencia de datos, es ideal realizarlo con ayuda del cliente, tiene como objetivo definir el alcance del proyecto, definir los objetivos, conocer las acciones que se llevaran acabo, conocer si los datos son relevantes y suficientes, proponer soluciones analíticas, entre otros puntos que se tocaran a continuación. 1.7.1 Data Maturity Framework Antes de iniciar con el scoping, queremos conocer si los interesados están listos para realizar un proyecto de ciencia de datos. Para ello, una opción es usar el Data Maturity Framework desarrollado en la Universidad de Chicago. El Data Maturity Framework nos sirve para ver dónde se encuentra la organización en el marco de madurez de datos y cómo mejorar su organización, tecnología y preparación de datos. Tiene tres áreas de contenido: Definición del problema Disponibilidad de datos y tecnología Preparación organizacional Esta dividido en tres partes: Un cuestionario y una encuesta para evaluar la preparación de la organización. Matriz de preparación de datos y tecnología Matriz de preparación organizacional 1.7.2 Scoping Para realizar el scoping podemos apoyarnos del siguiente documento. Ya que sabemos que la organización esta preparada para realizar un proyecto de ciencia de datos, podemos iniciar el scoping. El proceso a seguir es el siguiente: Definir objetivo(s) Considerado el paso más importante del proceso, los stakeholders iniciaran con un planteamiento del problema de manera muy general, nuestra responsabilidad será ir aterrizando ideas y definir el problema de manera más concreta, esta parte del scoping puede ocurrir en distintas iteraciones. Necesitamos hacer que el objetivo sea concreto, medible y optimizable. Cuando se van refinando objetivos, es común que se vaya priorizando por lo que tendremos tradeoffs que irán ligados a las acciones y al contexto del negocio. ¿Qué acciones o intervenciones existen que serán mejoradas a través de este proyecto? Debemos definir acciones concretas, si esto no ocurre es muy probable que la solución no sea implementada por lo que el proyecto no tendrá uso y no se estará haciendo ciencia de datos. La implementación del proyecto debería ayudar a tener mejor información para llevar acabo estas acciones, es decir, el proyecto mejorará la toma de decisiones basadas en la evidencia de los datos. Hacer una lista con las acciones ayuda a que el proyecto sea accionable, es posible que estas acciones no existan aún en la organización, por lo que el proyecto puede ayudar a generar nuevas acciones. Es muy común que la acción definida por el stakeholder sea de muy alto nivel, en ese caso podemos tomar 2 caminos en el scoping: Proponer en el scoping que el proyecto informe a esa acción general. Generar a partir de esa acción general acciones más pequeñas. ¿Qué datos tenemos y cuáles necesitamos? Primero observemos que no se había hablado de los datos hasta este punto, lo anterior porque debemos primero pensar en el problema, entenderlo y luego ver con qué datos contamos para resolverlo. Si hacemos esto primero seguramente acabaremos desarrollando productos de datos “muertos” y no accionables. En este paso se le dará uso al Data Maturity Framework, queremos conocer cómo se guardan los datos, con qué frecuencia, en qué formato, en qué estructura, qué granularidad tiene, desde cuándo tenemos historia de estos datos, si existe un sesgo en su recolección, con qué frecuencia recolectan nueva información, sobrescribe la ya existente? Uno de los objetivos consiste en identificar si la granularidad, frecuencia y horizonte de tiempo en los datos corresponde a la granularidad, frecuencia y horizonte de tiempo de las acciones. ¿Cuál es el análisis que necesitamos hacer? En esta sección del scoping queremos definir qué tipo de análisis necesitamos hacer con los datos con los que contamos para cumplir con los objetivos definidos y generar las acciones identificadas. El análisis puede incluir métodos y herramientas de diferentes disciplinas: ciencias computacionales, ciencia de datos, machine learning, estadística, ciencias sociales. Existen distintos tipos de análisis, los 4 más comunes son: Descripción: Centrado en entender eventos y comportamientos del pasado. Aunque puede confundirse con business intelligence, debido a que ya definimos objetivos y acciones vamos a desarrollar un producto de datos. Para este tipo de análisis podemos ocupar métodos de aprendizaje no supervisado: clustering. Detección: Más concentrado en los eventos que están sucediendo. Detección de anomalías. Predicción: Concentrado en el futuro, prediciendo futuros eventos o comportamientos. Cambio en comportamiento: Concentrado en entender las causas de cambios en comportamientos de personas eventos, organizaciones, vecindarios, etc. En esta fase tenemos que responder las siguientes preguntas: ¿Qué tipo de análisis necesitaremos? Puede ser más de uno. ¿Cómo vamos a validar el análisis? ¿Qué validaciones se pueden hacer con los datos existentes? ¿Cómo podemos diseñar una prueba en campo para validar el análisis antes de que pongamos el producto en producción. Identificar qué acciones se cubren con cada análisis, debemos tener todas las acciones cubiertas. Ejemplos Los siguientes ejemplos forman parte del trabajo de DSSG, en cada uno de estos planteamientos intentaremos responder las siguientes preguntas: ¿Cuál es el objetivo? ¿Cómo se mide el objetivo? ¿Qué se optimiza? ¿Se puede optimizar? ¿Cuáles son los tradeoffs? ¿Que implicaciones éticas identificas? Envenenamiento por plomo: Hace unos años, comenzamos a trabajar con el Departamento de Salud Pública de Chicago para prevenir el envenenamiento por plomo. El objetivo inicial era aumentar la eficacia de sus inspecciones de peligro de plomo. Una forma de lograr ese objetivo sería concentrarse en los hogares que tienen peligros de plomo. Aunque fue útil, este enfoque no lograría su objetivo real, que era evitar que los niños se intoxicaran con plomo. Encontrar un hogar con peligros de plomo y repararlo solo es beneficioso si existe una alta probabilidad de que un niño presente (actualmente o en el futuro) se exponga al plomo. La siguiente iteración del objetivo fue maximizar la cantidad de inspecciones que detectan peligros de plomo en hogares donde hay un niño en riesgo (antes de que el niño se exponga al plomo). Finalmente, llegamos al objetivo final: identificar qué niños corren un alto riesgo de intoxicación por plomo en el futuro y luego dirigir las intervenciones a los hogares de esos niños.. High School Graduation: Uno de los mayores desafíos que enfrentan las escuelas hoy en día es ayudar a sus estudiantes a graduarse (a tiempo). Las tasas de graduación en los EE. UU. Son ~65%. Todos están interesados en identificar a los estudiantes que corren el riesgo de no graduarse a tiempo. Al hablar inicialmente con la mayoría de los distritos escolares, comienzan con un objetivo muy limitado de predecir qué niños es poco probable que se gradúen a tiempo. El primer paso es volver al objetivo de aumentar las tasas de graduación y preguntar si hay un subconjunto específico de estudiantes en riesgo que quieran identificar. ¿Qué pasaría si pudiéramos identificar a los estudiantes que tienen solo un 5% de probabilidades de estar en riesgo frente a los estudiantes que tienen un 95% de probabilidades de no graduarse a tiempo sin apoyo adicional? Si el objetivo es simplemente aumentar las tasas de graduación, es (probablemente) más fácil intervenir e influir en el primer grupo, mientras que el segundo grupo puede ser más desafiante debido a los recursos que necesita. ¿El objetivo es maximizar la probabilidad promedio/media/mediana de graduarse para una clase/escuela o es el objetivo enfocarse en los niños con mayor riesgo y maximizar la probabilidad de graduación del 10% inferior de los estudiantes? ¿O el objetivo es crear más equidad y disminuir la diferencia en la probabilidad de graduación a tiempo entre el cuartil superior y el cuartil inferior? Todos estos son objetivos razonables, pero las escuelas deben comprender, evaluar y decidir qué objetivos les interesan. Esta conversación a menudo los hace pensar más en definir analíticamente cuáles son sus objetivos organizacionales, así como las compensaciones.. Inspecciones: Hemos trabajado en varios proyectos que involucraron inspecciones, como con la EPA (Agencia de Protección Ambiental) y el Departamento de Conservación Ambiental del Estado de Nueva York para ayudarlos a priorizar qué instalaciones inspeccionar para detectar infracciones de eliminación de desechos, con la ciudad de Cincinnati para ayudar a identificar las propiedades en riesgo de violaciones del código para prevenir el deterioro -el proceso a través del cual una ciudad que funcionaba anteriormente, o parte de ella, cae en deterioro y decrepitud-, y con el Grupo del Banco Mundial para ayudarlos a priorizar qué denuncias de fraude y colusión investigar. En la mayoría de los problemas de inspección/investigación, hay muchas más entidades (viviendas, edificios, instalaciones, negocios, contratos) para inspeccionar que los recursos disponibles necesarios para realizar esas inspecciones. El objetivo con el que comienzan la mayoría de estas organizaciones es dirigir sus inspecciones a las entidades que tienen más probabilidades de violar las regulaciones existentes. Ese es un buen comienzo, pero la mayoría de estas organizaciones nunca pueden inspeccionar todas las instalaciones/hogares que pueden no cumplir con las normas, por lo que el objetivo que realmente buscan es la disuasión: reducir la cantidad total de instalaciones que estarán en violación. Un proceso de inspección ideal resultaría entonces en la reducción del número real de violaciones (encontradas o no), lo cual puede no ser lo mismo que un proceso de inspección que tiene como objetivo ser eficiente y aumentar la tasa de aciertos (% de inspección que resulta en violaciones). Programación de la recolección de residuos: Recientemente comenzamos a trabajar con Sanergy, una empresa social con sede en Kenia. Implementan inodoros portátiles en asentamientos urbanos informales y uno de sus mayores costos es contratar personas para vaciar los inodoros. Hoy en día, todos los inodoros se vacían todos los días, aunque existe una variación en cuánto se usan y cuánto se llenan. Para que puedan crecer y mantener bajos los costos, necesitan un enfoque más adaptable que pueda optimizar el cronograma de vaciado de los inodoros. El objetivo en este caso es asegurarse de no vaciar demasiado el inodoro cuando no está lleno, pero tampoco dejar que permanezca lleno porque entonces no se puede usar. Esto se traduce en una formulación que presiona para vaciar el inodoro lo más cerca posible de estar lleno al 100% sin llegar al 100%. 1.8 Perfiles de un equipo La tecnología crece día con día y resulta imposible abarcar todos los conocimientos y especialidades, por lo que resulta indispensable tener un claro entendimiento de los diferentes roles o perfiles de profesionistas que lograrán colaborar en el alcance de objetivos de negocio basados en el análisis y explotación de datos. A continuación se presenta un esquema global sobre el crecimiento de un equipo con enfoque de ingeniería de inteligencia de decisiones basada en análisis de datos: 1) Ingeniero de datos Es fundamental tener la capacidad de obtener datos antes de que tenga sentido hablar sobre el análisis de datos. Si estás trabajando con conjuntos de datos pequeños, la ingeniería de datos consistirá básicamente en ingresar algunos números en una hoja de cálculo. Cuando se opera a una escala más grande, la ingeniería de datos se convierte en una disciplina sofisticada por derecho propio. Alguien del equipo deberá asumir la responsabilidad de lidiar con los complicados aspectos de ingeniería para poder entregar los datos con los que el resto del equipo pueda trabajar. 2) Decisor Antes de contratar a un científico de datos con un master o PhD, hay que asegurarse de que quien toma las decisiones entiende el arte y la ciencia de la toma de decisiones basada en datos. \\[\\text{Las capacidades de toma de decisiones deben estar establecidas, antes de que un equipo pueda obtener valor de los datos}\\] Este individuo es responsable de identificar las decisiones que valen la pena tomar con los datos, plasmarlas en un modelo (desde el diseño de métricas, hasta las inyecciones sobre supuestos estadísticos) y determinar el nivel requerido de rigor analítico, basado en el impacto potencial en el negocio. Busca un pensador profundo que no diga continuamente: “Oh, vaya, eso ni siquiera se me había ocurrido mientras estaba pensando en esta decisión”. Sino más bien que ya lo haya pensado. Y eso otro. Y esto también. Y aquello… 3) Analista Luego, la siguiente contratación es… todos los que ya están trabajando contigo. Todos están calificados para mirar los datos y sentirse inspirados, lo único que puede estar faltando es un poco de familiaridad con el software que sea adecuado para hacer el análisis. Aprender a usar herramientas como R y Python es sólo una mejora sobre MS Paint para la visualización de datos; estas son simplemente herramientas más versátiles para visualizar una variedad más amplia de conjuntos de datos que sólo matrices de píxeles rojo-verde-azul. Si toda la fuerza laboral está empoderada para hacerlo, se tendrá un mucho mejor pulso de negocio que si nadie está mirando ningún dato en lo absoluto. Lo importante a recordar, es que no se debe llegar a conclusiones más allá de los datos. Eso requiere entrenamiento especializado. Al igual que con la foto de arriba, esto es todo lo que puedes decir al respecto: “Esto es lo que hay en mi conjunto de datos”. Por favor, no lo uses para concluir que el Monstruo de Lago Ness es real. 4) Analista experto ¡Entra la versión ultra rápida! Esta persona puede ver más datos más rápido. El juego aquí es velocidad, exploración, descubrimiento… ¡diversión! (Otro término para la analítica es la minería de datos). Este no es el rol relacionado con el rigor y las conclusiones cuidadosas. En su lugar, esta es la persona que ayuda a tu equipo a ver la mayor cantidad de datos posible para que el responsable de la toma de decisiones pueda tener una idea de lo que vale la pena obtener con más detalle. \\[\\text{El trabajo aquí es la velocidad, encontrando potenciales “insights” lo más rápido posible.}\\] Esto puede ser contrario a la intuición, pero no es conveniente asignar esta función a los mejores ingenieros que escriben un magnífico y sólido código de software. El trabajo aquí es velocidad, encontrando potenciales “insights” o revelaciones lo más rápido posible. \\[\\text{Aquellos que se obsesionan con la calidad del código pueden encontrar difícil ser útiles en este rol.}\\] 5) Estadístico Ahora que tenemos a todas estas personas contentas explorando los datos, es mejor tener a alguien cerca para controlar tanta exaltación. Podría ser una buena idea tener a alguien cerca que pueda evitar que el equipo saque conclusiones infundadas. \\[\\text{La inspiración es barata, pero el rigor es caro.}\\] Los Estadísticos ayudan a quienes toman las decisiones a llegar a conclusiones seguras más allá de los datos. Por ejemplo, si el sistema de Machine Learning funcionó para un conjunto de datos, todo lo que se puede concluir es que funcionó en ese conjunto de datos. ¿Funcionará cuando se está ejecutando en producción con otros datos? ¿Debería ser lanzarlo? Necesita algunas habilidades adicionales para lidiar con esas preguntas. Habilidades estadísticas. Si queremos tomar decisiones serias cuando no tenemos datos perfectos, vayamos más despacio y tomemos un enfoque cuidadoso. Los estadísticos ayudan a quienes toman las decisiones a llegar a conclusiones más seguras, más allá de los datos analizados. 6) Ingeniero de Machine Learning El mejor atributo de un ingeniero de Machine Learning / Inteligencia Artificial aplicada es diseñar, crear, evaluar y producir modelos para resolver problemas de la vida real.. Lo que estás buscando es experiencia en transformar código para hacer que los algoritmos existentes acepten y revuelvan tus conjuntos de datos. Además de dedos rápidos para codificar, se busca una personalidad que tenga tolerancia frente al fracaso. Ejecuta los datos a través de un grupo de algoritmos lo más rápido posible y ve sí parece estar funcionando. Una gran parte del trabajo es ir tanteando a ciegas, y se necesita un tipo especial de personalidad para disfrutar eso. \\[\\text{Los perfeccionistas tienden a tener problemas como ingenieros de ML.}\\] Como el problema de negocio no está en un libro de texto, no puede saberse de antemano qué funcionará, por lo que no puede esperarse obtener un resultado perfecto la primera vez. ¡Eso bien!, solo hay que intentar muchos enfoques lo más rápido posible e intenta encontrar una solución. Hablando de “ejecutar los datos a través de modelos o algoritmos” ¿qué datos? Los “inputs” o entradas que tus analistas identificaron como potencialmente interesantes, por supuesto. Es por eso, que tiene sentido contratar primero a los analistas. Es importante que el ingeniero de Machine Learning tenga un profundo respeto por la parte del proceso donde el rigor es vital: la evaluación. ¿Funcionó realmente la solución con nuevos datos? Por suerte, tomaste una buena decisión con tu anterior contratación, por lo que todo lo que tienes que hacer ahora es pasarle la batuta al estadístico. Los ingenieros de ML aplicada más experimentados tienen una idea muy clara de cuánto tiempo tarda aplicar varios enfoques. 7) Científico de Datos Un científico de datos es alguien que es un experto completo en los tres roles anteriores. No todos usan esta definición: verás las solicitudes de empleo por ahí con personas que se autodenominan “científicos de datos” cuando sólo dominan realmente uno de los tres, así que vale la pena comprobarlo. \\[\\text{Los científicos de datos son totalmente expertos en las tres funciones anteriores.}\\] Este rol está en la posición # 7 porque contratar a los verdaderos tres-en-uno es una opción costosa. Si puedes contratar uno dentro de tu presupuesto, sería una gran idea, pero si tiene un presupuesto ajustado, considera la posibilidad de entrenar y hacer crecer a tus actuales especialistas-de-una-sola-función. 8) Gerente de Análisis / Líder de Ciencia de Datos El Gerente de Analítica es la gallina de los huevos de oro: son un híbrido entre el científico de datos y el que toma las decisiones. Su presencia en el equipo actúa como un multiplicador de fuerzas, lo que garantiza que tu equipo de ciencia de datos no quede en fuera de juego en lugar de agregar valor a tu negocio. Esta persona se queda despierta por la noche pensando preguntas como: “¿Cómo diseñamos las preguntas correctas? ¿Cómo tomamos decisiones? ¿Cómo podemos asignar mejor a nuestros expertos? ¿Qué vale la pena hacer? ¿Las habilidades y los datos coincidirán con los requerimientos? ¿Cómo aseguramos buenos datos de entrada? 9) Experto Cualitativo / Científico Social A veces, quien toma las decisiones es un brillante líder, gerente, motivador, influyente o navegante de la política organizacional, pero no tiene experiencia en el arte y la ciencia de la toma de decisiones. La toma de decisiones es mucho más que un talento. Si su decisor no ha perfeccionado su oficio, podrían hacer más daño que bien. En lugar de despedir a un tomador de decisiones no calificado, puedes complementarlo con un experto cualitativo. No despidas a un tomador de decisiones no calificado, compleméntalo. Puedes contratarle un “upgrade” en forma de ayudante. El experto cualitativo está para complementar sus habilidades. Esta persona generalmente tiene formación en ciencias sociales y datos: Los economistas de comportamiento, neuroeconomistas y psicólogos reciben la capacitación más especializada, pero la gente autodidacta también puede ser buena en esto. El trabajo consiste en ayudar al responsable de la toma de decisiones a aclarar ideas, examinar todos los ángulos y convertir las intuiciones ambiguas en instrucciones bien pensadas en un lenguaje que facilite la ejecución al resto del equipo. No nos damos cuenta de lo valiosos que son los científicos sociales. Por lo general, están mejor equipados que los científicos de datos para traducir en métricas concretas, las intuiciones e intenciones de quienes toman las decisiones. El experto cualitativo no es el que toma las decisiones. En su lugar, se aseguran de que el responsable de la toma de decisiones haya captado completamente la información disponible para poderla tomar. También son un asesor de confianza, un compañero de intercambio de ideas y una caja de resonancia para quien toma las decisiones. Tenerlos a bordo es una excelente manera de garantizar que el proyecto comience en la dirección correcta. 10) Investigador Se trata de profesionistas de larga trayectoria tanto académica como práctica. Son profesionistas con PhD. Su gran experiencia y conocimientos los hacen ser capaces de construir nuevas herramientas hechas a la medida que no existen en el mercado. Este perfil es de gran beneficio cuando ya se cuenta con las posiciones anteriores. Si el investigador es el primer empleado, es probable que no se tenga el entorno adecuado para hacer un buen uso y aprovechamiento de sus conocimientos y habilidades. \\[\\text{&quot;Antes de construir ese bolígrafo espacial para viajar a la Luna, }\\] \\[\\text{comprueba primero si un lápiz puede hacer el trabajo.&quot;}\\] Es conveniente esperar hasta que el equipo se haya desarrollado lo suficiente como para haber averiguado para qué específicamente necesitan un investigador. Extra) Personal Adicional Además de los roles que vimos, estas son algunas de las personas que podrían para participar en un proyecto de inteligencia de decisiones: Experto en áreas específicas de negocio Ética Ingeniero de software Ingeniero de Confiabilidad Diseñador de Experiencia de Usuario Visualizador interactivo / diseñador gráfico. Especialista en recolección de datos Gerente de producto de datos Gerente de proyecto / programas Muchos proyectos no pueden prescindir de ellos. La única razón por la que no figuran en el top 10 es que la inteligencia de decisiones no es su negocio principal. En su lugar, son genios en su propia área y han aprendido lo suficiente sobre datos y la toma de decisiones para ser muy útiles en el proyecto. Piensa en ellos como si tuvieran su propia carrera universitaria, pero con suficiente amor por la inteligencia de decisiones que eligieron estudiarla como una especialización. Fuente: Medium Google Data Career Path Google es una compañía bastante especializada en procesos de desarrollo de tecnología con grandes aplicaciones comerciales. Cada compañía puede armar su propio equipo de acuerdo con la estrategia que más se apegue a sus objetivos, no obstante, siempre vale la pena conocer los perfiles que Google define y sugiere. Fuente: Data Science On Google Cloud Platform Esta definición de roles es particularmente técnica, por lo que es importante añadir los perfiles de expertos de negocio. 1.9 Flujo de trabajo en ML Al conocer el camino antes de atravesarlo habrá mucho más probabilidad de que sea alcanzado de manera exitosa. Como un preciado extra, además podrá lograrse en una cantidad significativamente menor de tiempo a lo que llevaría avanzar sin un esquema de trabajo. En esta sección se muestra el flujo de trabajo general que todo científico de datos debe implementar para llevar a cabo un exitoso proyecto basado en análisis de datos. Existen múltiples propuestas sobre el flujo de trabajo a implementar, sin embargo, la mayoría son muy similares. La diferencia estas propuestas son aspectos triviales que no se ven reflejados de una manera importante en la estructura del flujo de trabajo. El diagrama anterior muestra los elementos que pueden encontrarse en todo proyecto de analítica. 1. Business Understanding: La ciencia y tecnología aplicada a un problema es peligrosa si no se cuenta con el conocimiento de la naturaleza del problema y los factores que los afectan. El entendimiento del negocio es vital para lograr resolver el problema y solo mediante el profundo entendimiento es que se logra encontrar nuevas aportaciones para mejorar la calidad de la solución. 2. Data Mining: Es indispensable contar con una fuente de datos que provea a todo el equipo la información necesaria para el desarrollo del proyecto. Recolectar datos de diversas fuentes y centralizarlos para el consumo de todos aquellos autorizados a consumirlos, es la primer tarea técnica a satisfacer. En esta etapa se deberán separar los datos que serán usados en las siguientes etapas para: Creación del modelo Pruebas de calidad Validación de resultados 3. Data Cleaning: Los datos suelen no estar listos para ser usados. Suelen existir datos faltantes o equivocados que en caso de usarse directamente provocarán resultados de baja calidad. Es indispensable limpiar estos datos para que los modelos tengan resultados con la mejor calidad posible. 4. Data Exploration: La exploración es necesaria para conocer el contenido, estructura, distribución y relación que existe entre los datos. Los modelos suelen tener distintos supuestos del comportamiento de la información para poder ser implementados. En la medida en que se conocen estos atributos, será posible proponer alternativas y soluciones adicionales que aporten valor. 5. Feature Engineering: La ingeniería de variables es el proceso en el que se manufacturan nuevas variables que aportan valor adicional al que ofrecen las variables originales. Esta etapa es posible realizarse con éxito cuando se conoce el negocio y se ha realizado una exploración de datos profunda. 6. Predictive Modeling Una vez que se ha creado el conjunto de datos que sirve de insumo para el análisis, se procede a la creación del modelo. En esta etapa se ponen a prueba múltiples modelos hasta elegir el que aporta la mejor solución de acuerdo con el objetivo de negocio. 7. Data Visualization Todos los modelos tienen un tiempo de vida finita. Con el paso del tiempo la calidad va disminuyendo debido a cambios en el comportamiento de la población o fenómenos involucrados, por lo que los resultados y la calidad del modelo deben ser monitoreados constantemente para conocer el momento en que dejan de ser útiles y debe darse mantenimiento al proyecto. En R, existen librerías que ayudan a llevar a cabo todo el proceso de principio a fin. En las siguientes sesiones se llevará a cabo la explicación de todo el proceso y su implementación. "],["introducción-a-r.html", "Capítulo 2 Introducción a R 2.1 ¿Cómo obtener R? 2.2 ¿Qué es RStudio? 2.3 R como lenguaje orientado a objetos 2.4 Estructuras de almacenamiento 2.5 Funciones básicas de R 2.6 Estructuras de control 2.7 Guía de estilo", " Capítulo 2 Introducción a R R (R Core Team) es un entorno y lenguaje de programación que permite el análisis estadístico de información y reportes gráficos. Es ampliamente usado en investigación por la comunidad estadística en campos como la biomedicina, minería de datos, finanzas, seguros, entre otros. Ha ganado mucha popularidad en los últimos años al ser un software libre que está en constante crecimiento por las aportaciones de otros usuarios y que permite la interacción con software estadísticos como STATA, SAS, SPSS, etc. R permite la incorporación de librerías y paqueterías con funcionalidades específicas, por lo que es un lenguaje de programación muy completo y fácil de usar. 2.1 ¿Cómo obtener R? R puede ser fácilmente descargado de forma gratuita desde el sitio oficial http://www.r-project.org/. R está disponible para las plataformas Windows, Mac y Linux. 2.2 ¿Qué es RStudio? RStudio es un Entorno de Desarrollo Integrado (IDE, por sus siglas en inglés) para R. Este permite y facilita el desarrollo y ejecución de sintaxis para código en R, incluye una consola y proporciona herramientas para la gestión del espacio de trabajo. RStudio está disponible para Windows, Mac y Linux o para navegadores conectados a RStudio Server o RStudio Server Pro. Algunas de las principales características de Rstudio que lo hacen una gran herramienta para trabajar en R, son: Auto completado de código Sangría inteligente Resaltado de sintaxis Facilidad para definir funciones Soporte integrado Documentación integrada Administración de directorios y proyectos Visor de datos Depurador interactivo para corregir errores Conexión con Rmarkwon y Sweave La siguiente imagen muestra la forma en la que está estructurado RStudio. El orden de las ventanas puede ser elegido por el usuario, así como las características de tipo de letra, tamaño y color de fondo, entre otras características. Figure 2.1: Páneles de trabajo de Rstudio 2.3 R como lenguaje orientado a objetos R es un lenguaje de programación orientado a objetos (POO). Un objeto es “cualquier cosa con significado para el problema que se trata de resolver”. Los objetos tienen características fundamentales que permiten identificarlos, conocerlos y entender su comportamiento. De acuerdo con (Schildt 2009), estas características son: Identidad: Esta es la propiedad que da nombre a cada uno de los objetos y que permite declararlos, distinguirlos de manera única, usarlos y llamarlos para la representación de su contenido. Comportamiento: Esta es la propiedad que determina las operaciones que puede realizar el objeto, es decir, permite conocer las capacidades y alcances de la funcionalidad de cada objeto. El comportamiento permite conocer la interacción que puede existir con otros objetos y los resultados que generarán. Estructura: El estado se refiere a un conjunto de características o atributos específicos del objeto dados en un momento determinado, y que pueden cambiar en un instante de tiempo. En la programación orientada a objetos, un programa recolecta muchos objetos para ser tratado como un conjunto dinámico de objetos interactuando entre sí. Los objetos están definidos por: Atributos: Son las propiedades o características de los datos contenidos en un objeto. Los valores asociados a un objeto en un momento determinado del tiempo determinan su estado. Métodos: Acceden a los atributos de los objetos y determinan el comportamiento de los datos contenidos. 2.4 Estructuras de almacenamiento En R existen varios tipos de objectos que permiten que el usuario pueda almacenar la información para realizar procedimientos estadísticos y gráficos. Los principales objetos en R son vectores, matrices, arreglos, marcos de datos y listas. A continuación se presentan las características de estos objetos y la forma para crearlos. 2.4.1 Operadores de asignación En R se pueden hacer asignación de varias formas, a continuación se presentan los operadores disponibles para tal fin. &lt;- este es el operador de asignación a izquierda, es el más usado y recomendado. -&gt; este es el operador de asignación a derecha, no es frecuente su uso. = el símbolo igual sirve para hacer asignaciones pero NO se recomienda usarlo. &lt;&lt;- este es un operador de asignación global y sólo debe ser usado por usuarios avanzados. Ejemplo Almacene los valores 5.3, 4.6 y 25 en los objetos a, b y age respectivamente, use diferentes símbolos de asignación. Para hacer lo solicitado se podría usar el siguiente código. a &lt;- 5.3 # Recomendado 4.6 -&gt; b # No es usual age = 25 # No recomendado Aunque una asignación se puede hacer de tres formas diferentes, se recomienda sólo usar el símbolo &lt;-. 2.4.2 Variables Las variables sirven para almacenar un valor que luego vamos a utilizar en algún procedimiento. Para hacer la asignación de un valor a alguna variable se utiliza el operador &lt;- entre el valor y el nombre de la variable. A continuación un ejemplo sencillo. x &lt;- 5 (2 * x) + 3 ## [1] 13 En el siguiente ejemplo se crea la variable país y se almacena el nombre Colombia, luego se averigua el número de caracteres de la variable país. pais &lt;- &quot;México&quot; nchar(pais) ## [1] 6 También existen variables lógicas y estas toman los valores verdadero (TRUE) o falso (FALSE) dependiendo del resultado lógico puesto a prueba. Ejemplo: y &lt;- 10 y == (5 + 3 + 2) ## [1] TRUE y != 5 + 5 ## [1] FALSE 2.4.3 Vectores Los vectores vectores son arreglos ordenados en los cuales se puede almacenar información de tipo numérico (variable cuantitativa), alfanumérico (variable cualitativa) o lógico (TRUE o FALSE), pero no mezclas de éstos. La función de R para crear un vector es c() y que significa concatenar; dentro de los paréntesis de esta función se ubica la información a almacenar. Una vez construido el vector se acostumbra a etiquetarlo con un nombre corto y representativo de la información que almacena, la asignación se hace por medio del operador &lt;- entre el nombre y el vector. A continuación se presenta un ejemplo de cómo crear tres vectores que contienen las respuestas de cinco personas a tres preguntas que se les realizaron. edad &lt;- c(15, 19, 13, NA, 20) deporte &lt;- c(TRUE, TRUE, NA, FALSE, TRUE) sexo &lt;- c(&quot;Hombre&quot;, &quot;Mujer&quot;, &quot;Hombre&quot;, &quot;Hombre&quot;, &quot;Mujer&quot;) El vector edad es un vector cuantitativo y contiene las edades de las 5 personas. En la cuarta posición del vector se colocó el símbolo NA que significa Not Available debido a que no se registró la edad para esa persona. Al hacer una asignación se acostumbra a dejar un espacio antes y después del operador &lt;- de asignación. El segundo vector es llamado deporte y es un vector lógico que almacena las respuestas a la pregunta de si la persona practica deporte, nuevamente aquí hay un NA para la tercera persona. El último vector sexo contiene la información del sexo de cada persona, como esta variable es cualitativa es necesario usar las comillas ” ” para encerrar las respuestas. ¡¡ RECORDAR !! Cuando se usa NA para representar una información Not Available no se deben usar comillas. Es posible usar comillas ‘sencillas’ o comillas “dobles” para ingresar valores de una variable cualitativa. Si se desea ver lo que está almacenado en cada uno de estos vectores, se debe escribir en la consola de R el nombre de uno de los objetos y luego se presiona la tecla enter o intro, al realizar esto lo que se obtiene se muestra a continuación. edad ## [1] 15 19 13 NA 20 deporte ## [1] TRUE TRUE NA FALSE TRUE sexo ## [1] &quot;Hombre&quot; &quot;Mujer&quot; &quot;Hombre&quot; &quot;Hombre&quot; &quot;Mujer&quot; 2.4.3.1 ¿Cómo extraer elementos de un vector? Para extraer un elemento almacenado dentro un vector se usan los corchetes [] y dentro de ellos la posición o posiciones que interesan. Ejemplo Si queremos extraer la edad de la tercera persona escribimos el nombre del vector y luego \\[3\\] para indicar la tercera posición de edad, a continuación el código. edad[3] ## [1] 13 Si queremos conocer el sexo de la segunda y quinta persona, escribimos el nombre del vector y luego, dentro de los corchetes, escribimos otro vector con las posiciones 2 y 5 que nos interesan así: \\(c(2, 5)\\), a continuación el código. sexo[c(2, 5)] ## [1] &quot;Mujer&quot; &quot;Mujer&quot; Si nos interesan las respuestas de la práctica de deporte, excepto la de la persona 3, usamos \\[-3\\] luego del nombre del vector para obtener todo, excepto la tercera posición. deporte[-3] ## [1] TRUE TRUE FALSE TRUE ¡¡ RECORDAR !! Si desea extraer varios posiciones de un vector NUNCA escriba esto: mi_vector[2, 5, 7]. Tiene que crear un vector con las posiciones y luego colocarlo dentro de los corchetes así: \\[mi\\_vector[c(2, 5, 7)]\\] 2.4.4 Matrices Las matrices son arreglos rectangulares de filas y columnas con información numérica, alfanumérica o lógica. Para construir una matriz se usa la función matrix( ). Por ejemplo, para crear una matriz de 4 filas y 5 columnas (de dimensión 4×5) con los primeros 20 números positivos se escribe el código siguiente en la consola. mimatriz &lt;- matrix(data = 1:20, nrow = 4, ncol = 5, byrow = FALSE) El argumento data de la función sirve para indicar los datos que se van a almacenar en la matriz, los argumentos nrow y ncol sirven para definir la dimensión de la matriz y por último el argumento byrow sirve para indicar si la información contenida en data se debe ingresar por filas o no. Para observar lo que quedó almacenado en el objeto mimatriz se escribe en la consola el nombre del objeto seguido de la tecla enter o intro. mimatriz ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 5 9 13 17 ## [2,] 2 6 10 14 18 ## [3,] 3 7 11 15 19 ## [4,] 4 8 12 16 20 2.4.4.1 ¿Cómo extraer elementos de una matriz? Al igual que en el caso de los vectores, para extraer elementos almacenados dentro de una matriz se usan los corchetes [ , ] y dentro, separado por una coma, el número de fila(s) y el número de columna(s) que nos interesan. Ejemplo Si queremos extraer el valor almacenado en la fila 3 y columna 4 usamos el siguiente código. mimatriz[3, 4] ## [1] 15 Si queremos recuperar toda la fila 2 usamos el siguiente código. mimatriz[2, ] # No se escribe nada luego de la coma ## [1] 2 6 10 14 18 Si queremos recuperar toda la columna 5 usamos el siguiente código. mimatriz[, 5] # No se escribe nada antes de la coma ## [1] 17 18 19 20 Si queremos recuperar la matriz original sin las columnas 2 y 4 usamos el siguiente código. mimatriz[, -c(2, 4)] # Las columnas como vector ## [,1] [,2] [,3] ## [1,] 1 9 17 ## [2,] 2 10 18 ## [3,] 3 11 19 ## [4,] 4 12 20 Si queremos recuperar la matriz original sin la fila 1 ni columna 3 usamos el siguiente código. mimatriz[-1, -3] # Signo de menos para eliminar ## [,1] [,2] [,3] [,4] ## [1,] 2 6 14 18 ## [2,] 3 7 15 19 ## [3,] 4 8 16 20 2.4.5 Arreglos Un arreglo es una matriz de varias dimensiones con información numérica, alfanumérica o lógica. Para construir una arreglo se usa la función array( ). Por ejemplo, para crear un arreglo de 3 × 4 × 2 con las primeras 24 letras minúsculas del alfabeto se escribe el siguiente código. miarray &lt;- array(data = letters[1:24], dim=c(3, 4, 2)) El argumento data de la función sirve para indicar los datos que se van a almacenar en el arreglo y el argumento dim sirve para indicar las dimensiones del arreglo. Para observar lo que quedó almacenado en el objeto miarray se escribe en la consola lo siguiente. miarray ## , , 1 ## ## [,1] [,2] [,3] [,4] ## [1,] &quot;a&quot; &quot;d&quot; &quot;g&quot; &quot;j&quot; ## [2,] &quot;b&quot; &quot;e&quot; &quot;h&quot; &quot;k&quot; ## [3,] &quot;c&quot; &quot;f&quot; &quot;i&quot; &quot;l&quot; ## ## , , 2 ## ## [,1] [,2] [,3] [,4] ## [1,] &quot;m&quot; &quot;p&quot; &quot;s&quot; &quot;v&quot; ## [2,] &quot;n&quot; &quot;q&quot; &quot;t&quot; &quot;w&quot; ## [3,] &quot;o&quot; &quot;r&quot; &quot;u&quot; &quot;x&quot; 2.4.5.1 ¿Cómo extraer elementos de un arreglo? Para recuperar elementos almacenados en un arreglo se usan también corchetes, y dentro de los corchetes, las coordenadas del objeto de interés. Ejemplo Si queremos extraer la letra almacenada en la fila 1 y columna 3 de la segunda capa de miarray usamos el siguiente código. miarray[1, 3, 2] # El orden es importante ## [1] &quot;s&quot; Si queremos extraer la segunda capa completa usamos el siguiente código. miarray[,, 2] # No se coloca nada en las primeras posiciones ## [,1] [,2] [,3] [,4] ## [1,] &quot;m&quot; &quot;p&quot; &quot;s&quot; &quot;v&quot; ## [2,] &quot;n&quot; &quot;q&quot; &quot;t&quot; &quot;w&quot; ## [3,] &quot;o&quot; &quot;r&quot; &quot;u&quot; &quot;x&quot; Si queremos extraer la tercera columna de todas las capas usamos el siguiente código. miarray[, 3,] # No se coloca nada en las primeras posiciones ## [,1] [,2] ## [1,] &quot;g&quot; &quot;s&quot; ## [2,] &quot;h&quot; &quot;t&quot; ## [3,] &quot;i&quot; &quot;u&quot; 2.4.6 Data Frames El marco de datos marco de datos o data frame es uno de los objetos más utilizados porque permite agrupar vectores con información de diferente tipo (numérica, alfanumérica o lógica) en un mismo objeto, la única restricción es que los vectores deben tener la misma longitud. Para crear un marco de datos se usa la función data.frame( ), como ejemplo vamos a crear un marco de datos con los vectores edad, deporte y sexo definidos anteriormente. mi_data_frame &lt;- data.frame(edad, deporte, sexo) Una vez creado el objeto mi_data_frame podemos ver el objeto escribiendo su nombre en la consola, a continuación se muestra lo que se obtiene. mi_data_frame ## edad deporte sexo ## 1 15 TRUE Hombre ## 2 19 TRUE Mujer ## 3 13 NA Hombre ## 4 NA FALSE Hombre ## 5 20 TRUE Mujer De la salida anterior vemos que el marco de datos tiene 3 variables (columnas) cuyos nombres coinciden con los nombres de los vectores creados anteriormente, los números consecutivos al lado izquierdo son sólo de referencia y permiten identificar la información para cada persona en el conjunto de datos. Ejercicios: Use funciones o procedimientos (varias líneas) de R para responder cada una de las siguientes preguntas. Construya un vector con 5 nombres de personas. Construya un vector con las edades de las 5 personas anteriores. Construya un marco de datos o data frame con las respuestas de 5 personas a las preguntas: ¿Cuál es su nombre? Sexo de la persona ¿Cuál es su edad en años? ¿En qué alcaldía vive? ¿En qué alcaldía trabaja? 2.4.7 Listas Las listas son otro tipo de objeto muy usado para almacenar objetos de diferente tipo. La instrucción para crear una lista es list( ). A continuación vamos a crear una lista que contiene tres objetos: un vector con 5 números aleatorios llamado mivector, una matriz de dimensión 6×2 con los primeros doce números enteros positivos llamada matriz2 y el tercer objeto será el marco de datos mi_data_frame creado en el apartado anterior. Las instrucciones para crear la lista requerida se muestran a continuación. set.seed(12345) mivector &lt;- runif(n=5) matriz2 &lt;- matrix(data=1:12, ncol=6) milista &lt;- list(E1=mivector, E2=matriz2, E3=mi_data_frame) La función set.seed de la línea número 1 sirve para fijar la semilla de tal manera que los números aleatorios generados en la segunda línea con la función runif sean siempre los mismos. En la última línea del código anterior se construye la lista, dentro de la función list se colocan los tres objetos mivector, matriz2 y mi_data_frame. Es posible colocarle un nombre especial a cada uno de los elementos de la lista, en este ejemplo se colocaron los nombres E1, E2 y E3 para cada uno de los tres elementos. Para observar lo que quedó almacenado en la lista se escribe milista en la consola y el resultado se muestra a continuación. milista ## $E1 ## [1] 0.7209039 0.8757732 0.7609823 0.8861246 0.4564810 ## ## $E2 ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 1 3 5 7 9 11 ## [2,] 2 4 6 8 10 12 ## ## $E3 ## edad deporte sexo ## 1 15 TRUE Hombre ## 2 19 TRUE Mujer ## 3 13 NA Hombre ## 4 NA FALSE Hombre ## 5 20 TRUE Mujer 2.4.7.1 ¿Cómo extraer elementos de una lista? Para recuperar los elementos almacenadas en una lista se usa el operador $, corchetes dobles [[]] o corchetes sencillos []. A continuación unos ejemplos para entender cómo extraer elementos de una lista. Ejemplos Si queremos la matriz almacenada con el nombre de E2 dentro del objeto milista se puede usar el siguiente código. milista$E2 ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 1 3 5 7 9 11 ## [2,] 2 4 6 8 10 12 Es posible indicar la posición del objeto en lugar del nombre, para eso se usan los corchetes dobles. milista[[2]] ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 1 3 5 7 9 11 ## [2,] 2 4 6 8 10 12 El resultado obtenido con milista$E2 y milista[[2]] es exactamente el mismo. Vamos ahora a solicitar la posición 2 pero usando corchetes sencillos. milista[2] ## $E2 ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 1 3 5 7 9 11 ## [2,] 2 4 6 8 10 12 La apariencia de este último resultado es similar, no igual, al encontrado al usar $ y [[]]. Para ver la diferencia vamos a pedir la clase a la que pertenecen los tres últimos objetos usando la función class. A continuación el código usado. class(milista$E2) ## [1] &quot;matrix&quot; &quot;array&quot; class(milista[[2]]) ## [1] &quot;matrix&quot; &quot;array&quot; class(milista[2]) ## [1] &quot;list&quot; De lo anterior se observa claramente que cuando usamos $ o [[]] el resultado es el objeto almacenado, una matriz. Cuando usamos [] el resultado es una lista cuyo contenido es el objeto almacenado. 2.4.8 Ejercicios Use funciones o procedimientos (varias líneas) de R para responder cada una de las siguientes preguntas. Construya un vector con la primeras 20 letras MAYÚSCULAS usando la función LETTERS. Construya una matriz de 10×10 con los primeros 100 números positivos pares. Construya una matriz identidad de dimensión 3×3. Recuerde que una matriz identidad tiene sólo unos en la diagonal principal y los demás elementos son cero. Construya una lista con los anteriores tres objetos creados. Construya un marco de datos o data frame con las respuestas de 5 personas de su trabajo a las preguntas: ¿Cuál es su nombre? ¿Cuál es su antigüedad en la empresa? ¿Cuál es su puesto? ¿Tiene usted algún producto contratado con la empresa? (Sí / No) ¿Cuál? ¿Cuál es el error al ejecutar el siguiente código? ¿A qué se debe? edad &lt;- c(15, 19, 13, NA, 20) deporte &lt;- c(TRUE, TRUE, NA, FALSE, TRUE) sexo &lt;- c(NA, &#39;Hombre&#39;, &#39;Hombre&#39;, NA, &#39;Mujer&#39;) matrix(edad, deporte, sexo) 2.5 Funciones básicas de R En este capítulo se presentará lo que es una función y se mostrarán varias funciones básicas que son útiles para realizar diversas tareas. 2.5.1 ¿Qué es una función de R? En la figura de abajo se muestra una ilustración de lo que es una función o máquina general. Hay unas entradas (inputs) que luego son procesadas dentro de la caja para generar unas salidas (outputs). Un ejemplo de una función o máquina muy común en nuestras casas es la licuadora. Si a una licuadora le ingresamos leche, fresas, azúcar y hielo, el resultado será un delicioso jugo de fresa. Las funciones en R se caracterizan por un nombre corto y que dé una idea de lo que hace la función. Los elementos que pueden ingresar (inputs) a la función se llaman parámetros o argumentos y se ubican dentro de paréntesis, el cuerpo de la función se ubica dentro de llaves y es ahí donde se procesan los inputs para convertirlos en outputs, a continuación se muestra la estructura general de una función. nombre_de_funcion(parametro1, parametro2, ...) { tareas internas tareas internas tareas internas salida } Cuando usamos una función sólo debemos escribir bien el nombre e ingresar correctamente los parámetros de la función, el cuerpo de la función ni lo vemos ni lo debemos modificar. A continuación se presenta un ejemplo de cómo usar la función mean para calcular un promedio. notas &lt;- c(4.0, 1.3, 3.8, 2.0) # Notas de un estudiante mean(notas) ## [1] 2.775 2.5.2 Operaciones básicas En R se pueden hacer diversas operaciones usando operadores binarios. Este tipo de operadores se denomina binarios porque actúan entre dos objetos, a continuación el listado. + operador binario para sumar. - operador binario para restar. * operador binario para multiplicar. / operador binario para dividir. ^ operador binario para potencia. %/% operador binario para obtener el cociente en una división (número entero). %% operador binario para obtener el residuo en una división. A continuación se presentan ejemplos de cómo usar las anteriores funciones. 6 + 4 # Para sumar dos números ## [1] 10 a &lt;- c(1, 3, 2) b &lt;- c(2, 0, 1) # a y b de la misma dimensión a + b # Para sumar los vectores a y b miembro a miembro ## [1] 3 3 3 a - b # Para restar dos vectores a y b miembro a miembro ## [1] -1 3 1 a * b # Para multiplicar ## [1] 2 0 2 a / b # Para dividir ## [1] 0.5 Inf 2.0 a ^ b # Para potencia ## [1] 1 1 2 7 %/% 3 # Para saber las veces que cabe 3 en 7 ## [1] 2 7 %% 3 # Para saber el residuo al dividir 7 entre 3 ## [1] 1 2.5.3 Pruebas lógicas En R se puede verificar si un objeto cumple una condición dada, a continuación el listado de las pruebas usuales. &lt; para saber si un número es menor que otro. &gt; para saber si un número es mayor que otro. == para saber si un número es igual que otro. &lt;= para saber si un número es menor o igual que otro. &gt;= para saber si un número es mayor o igual que otro. A continuación se presentan ejemplos de cómo usar las anteriores funciones. 5 &lt; 12 # ¿Será 5 menor que 12? ## [1] TRUE # Comparando objetos x &lt;- 5 y &lt;- 20 / 4 x == y # ¿Será x igual a y? ## [1] TRUE # Usando vectores a &lt;- c(1, 3, 2) b &lt;- c(2, 0, 1) a &gt; b # Comparación término a término ## [1] FALSE TRUE TRUE a == b # Comparación de igualdad término a término ## [1] FALSE FALSE FALSE 2.5.4 Operadores lógicos En R están disponibles los operadores lógicos negación, conjunción y disyunción. A continuación el listado de los operadores entre los elementos x e y. !x # Negación de x x &amp; y # Conjunción entre x e y x &amp;&amp; y x | y # Disyunción entre x e y x || y xor(x, y) A continuación se presentan ejemplos de cómo usar el símbolo de negación !. ans &lt;- c(TRUE, FALSE, TRUE) !ans # Negando las respuestas almacenadas en ans ## [1] FALSE TRUE FALSE x &lt;- c(5, 1.5, 2, 3, 2) !(x &lt; 2.5) # Negando los resultados de una prueba ## [1] TRUE FALSE FALSE TRUE FALSE A continuación se presentan ejemplos de cómo aplicar la conjunción &amp; y &amp;&amp;. x &lt;- c(5, 1.5, 2) # Se construyen dos vectores para la prueba y &lt;- c(4, 6, 3) x &lt; 4 # ¿Serán los elementos de x menores que 4? ## [1] FALSE TRUE TRUE y &gt; 5 # ¿Serán los elementos de y mayores que 5? ## [1] FALSE TRUE FALSE x &lt; 4 &amp; y &gt; 5 # Conjunción entre las pruebas anteriores. ## [1] FALSE TRUE FALSE x &lt; 4 &amp;&amp; y &gt; 5 # Conjunción vectorial ## [1] FALSE Note las diferencias entre los dos últimos ejemplos, cuando se usa &amp; se hace una prueba término a término y el resultado es un vector, cuando se usa &amp;&amp; se aplica la conjunción al vector de resultados obtenido con &amp;. 2.5.5 Funciones sobre vectores En R podemos destacar las siguientes funciones básicas sobre vectores numéricos. min: para obtener el mínimo de un vector. max: para obtener el máximo de un vector. length: para determinar la longitud de un vector. range: para obtener el rango de valores de un vector, entrega el mínimo y máximo. sum: entrega la suma de todos los elementos del vector. prod: multiplica todos los elementos del vector. which.min: nos entrega la posición en donde está el valor mínimo del vector. which.max: nos da la posición del valor máximo del vector. rev: invierte un vector. Ejemplo Construir en vector llamado myvec con los siguientes elementos: 5, 3, 2, 1, 2, 0, NA, 0, 9, 6. Luego aplicar todas las funciones anteriores para verificar el funcionamiento de las mismas. myvec &lt;- c(5, 3, 2, 1, 2, 0, NA, 0, 9, 6) myvec ## [1] 5 3 2 1 2 0 NA 0 9 6 min(myvec) # Oops, no aparece el mínimo que es Cero. ## [1] NA min(myvec, na.rm=TRUE) # Usamos na.rm = TRUE para remover el NA ## [1] 0 max(myvec, na.rm=T) # Para obtener el valor máximo ## [1] 9 range(myvec, na.rm=T) # Genera min y max simultáneamente ## [1] 0 9 sum(myvec, na.rm=T) # La suma de los valores internos ## [1] 28 prod(myvec, na.rm=T) # El productor de los valores internos ## [1] 0 which.min(myvec) # Posición del valor mínimo 0 en el vector ## [1] 6 which.max(myvec) # Posición del valor máximo 9 en el vector ## [1] 9 De las dos últimas líneas podemos destacar lo siguiente: NO es necesario usar na.rm = TRUE para remover el NA dentro de las funciones which.min ni which.max. El valor mínimo 0 aparece en las posicione 2.5.6 Función rep En R podemos crear repeticiones usando la función rep, la estructura de esta función es: rep(x, times=1, length.out=NA, each=1) Los argumentos de esta función son: x: vector con los elementos a repetir. times: número de veces que el vector x se debe repetir. length.out: longitud deseada para el vector resultante. each: número de veces que cada elemento de x se debe repetir. Ejemplo Construya las siguientes repeticiones usando la función rep, no lo haga ingresando número por número. 1 2 3 4 1 2 3 4 1 1 2 2 3 3 4 4 1 1 2 3 3 4 1 1 2 2 3 3 4 4 La clave para construir una repetición es, descubrir la semilla o elemento que se repite. Las instrucciones para obtener las repeticiones anteriores se muestra a continuación. rep(x=1:4, times=2) ## [1] 1 2 3 4 1 2 3 4 rep(x=1:4, times=c(2,2,2,2)) ## [1] 1 1 2 2 3 3 4 4 rep(x=1:4, times=c(2,1,2,1)) ## [1] 1 1 2 3 3 4 rep(x=1:4, each=2) ## [1] 1 1 2 2 3 3 4 4 2.5.7 Función seq En R podemos crear secuencias de números de una forma sencilla usando la función seq, la estructura de esta función es: seq(from=1, to=1, by, length.out) Los argumentos de esta función son: from: valor de inicio de la secuencia. to: valor de fin de la secuencia, no siempre se alcanza. by: incremento de la secuencia. length.out: longitud deseado de la secuencia. Ejemplo Construya las siguientes tres secuencias usando la función seq. Once valores igualmente espaciados desde 0 hasta 1. Una secuencia de dos en dos comenzando en 1. Una secuencia desde 1 con un salto de \\(\\pi\\) y sin pasar del número 9. El código necesario para obtener las secuencias se muestra a continuación. seq(from=0, to=1, length.out = 11) ## [1] 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 seq(from=1, to=9, by=2) # concuerda con final ## [1] 1 3 5 7 9 seq(from=1, to=9, by=pi) # se mantiene por debajo del final ## [1] 1.000000 4.141593 7.283185 En R existe el operador binario : que sirve para construir secuencias de uno en uno fácilmente. Revise los siguientes ejemplos para entender el funcionamiento del operador :. 2:8 ## [1] 2 3 4 5 6 7 8 3:-5 ## [1] 3 2 1 0 -1 -2 -3 -4 -5 pi:6 # secuencia real ## [1] 3.141593 4.141593 5.141593 6:pi # secuencia entera ## [1] 6 5 4 2.5.8 EJERCICIOS Use funciones o procedimientos (varias líneas) de R para responder (al menos) a de las siguientes preguntas. ¿Qué cantidad de dinero sobra al repartir $10,000 entre 3 personas? ¿Es el número 4,560 divisible por 3? Construya un vector con los números enteros del 2 al 87. ¿Cuáles de esos números son divisibles por 7? Construya dos vectores, el primero con los números enteros desde 7 hasta 3, el segundo vector con los primeros cinco números positivos divisibles por 5. Sea A la condición de ser par en el primer vector. Sea B la condición de ser mayor que 10 en el segundo vector. ¿En cuál de las 5 posiciones se cumple A y B simultáneamente? Construya un vector con los siguientes elementos: 1, -4, 5, 9, -4. Escriba un procedimiento para extraer las posiciones donde está el valor mínimo en el vector. Calcular \\(8!\\) Evaluar la siguiente suma \\(\\sum_{i=3}^{i=7}e^i\\) Evaluar el siguiente producto \\(\\prod_{i=1}^{i=10}\\log\\sqrt{i}\\) Construya un vector cualquiera e inviértalo, es decir, que el primer elemento quede de último, el segundo de penúltimo y así sucesivamente. Compare su resultado con el de la función rev. Crear el vector: \\(1, 2, 3, \\ldots, 19, 20\\). Crear el vector: \\(20, 19, \\ldots , 2, 1\\). Crear el vector: \\(1, -2, 3, -4, 5, -6, \\ldots, 19, -20\\). Crear el vector: \\(0.1^3, 0.2^1, 0.1^6, 0.2^4, . . . , 0.1^{36}, 0.2^{34}\\). Calcular lo siguiente: \\(\\sum_{i=10}^{100}(i^3+4i^2)\\) y \\(\\sum_{i=1}^{25}\\left( \\frac{2^i}{i} + \\frac{3^i}{i^2} \\right)\\). En R hay unas bases de datos incluidas, una de ellas es la base de datos llamada mtcars. Para conocer las variables que están en mtcars usted puede escribir en la consola ?mtcars o también help(mtcars). De la base mtcars obtenga bases de datos que cumplan las siguientes condiciones. Autos que tengan un rendimiento menor a 18 millas por galón de combustible. Autos que tengan 4 cilindros. Autos que pesen más de 2500 libras y tengan transmisión manual. 2.6 Estructuras de control En R se disponen de varias instrucciones de control para facilitar los procedimientos que un usuario debe realizar. A continuación se explican esas instrucciones de control. 2.6.1 Instrucción if Esta instrucción sirve para realizar un conjunto de operaciones si se cumple cierta condición. A continuación se muestra la estructura básica de uso. if (condicion) { operación 1 operación 2 ... operación final } Ejemplo Una secretaria recibe la información del salario básico semanal de un empleado y las horas trabajadas durante la semana por ese empleado. El salario básico es la remuneración por 40 horas de labor por semana, las horas extra son pagadas a 150 pesos. Escriba el procedimiento en R que debe usar la secretaria para calcular el salario semanal de un empleado que trabajó 45 horas y tiene salario básico de 5 mil pesos. El código para calcular el salario final del empleado es el siguiente: sal &lt;- 5000 # Salario básico por semana hlab &lt;- 45 # Horas laboradas por semana if(hlab &gt; 40) { hext &lt;- hlab - 40 salext &lt;- hext * 150 sal &lt;- sal + salext } sal # Salario semanal ## [1] 5750 2.6.2 Instrucción if else Esta instrucción sirve para realizar un conjunto de operaciones cuando NO se cumple cierta condición evaluada por un if. A continuación se muestra la estructura básica de uso. if (condicion) { operación 1 operación 2 ... operación final } else { operación 1 operación 2 ... operación final } 2.6.3 Instrucción ifelse Se recomienda usar la instrucción ifelse cuando hay una sola instrucción para el caso if y para el caso else. A continuación se muestra la estructura básica de uso. ifelse(condición, operación SI cumple, operación NO cumple) Ejemplo Suponga que usted recibe un vector de números enteros, escriba un procedimiento que diga si cada elemento del vector es par o impar. x &lt;- c(5, 3, 2, 8, -4, 1) ifelse(x %% 2 == 0, &#39;Es par&#39;, &#39;Es impar&#39;) ## [1] &quot;Es impar&quot; &quot;Es impar&quot; &quot;Es par&quot; &quot;Es par&quot; &quot;Es par&quot; &quot;Es impar&quot; 2.6.4 Instrucción for La instrucción for es muy útil para repetir un procedimiento cierta cantidad de veces. A continuación se muestra la estructura básica de uso. for (i in secuencia) { operación 1 operación 2 ... operación final } Ejemplo Escriba un procedimiento para crear 10 muestras de tamaño 100 de una distribución uniforme entre uno y tres. Para cada una de las muestra, se debe contar el número de elementos de la muestra que fueron mayores o iguales a 2.5. nrep &lt;- 10 # Número de repeticiones n &lt;- 100 # Tamaño de la muestra conteo &lt;- numeric(nrep) # Vector para almacenar el conteo for (i in 1:nrep) { x &lt;- runif(n=n, min=1, max=3) conteo[i] &lt;- sum(x &gt;= 2.5) } conteo # Para obtener el conteo ## [1] 24 37 28 26 30 18 29 23 19 19 2.6.5 Instrucción while La instrucción while es muy útil para repetir un procedimiento siempre que se cumple una condición. A continuación se muestra la estructura básica de uso. while (condición) { operación 1 operación 2 ... operación final } Ejemplo Suponga que se lanza una moneda en la cual el resultado es cara o cruz. Escribir un procedimiento que simule lanzamientos hasta que el número de caras obtenidas sea 5. El procedimiento debe entregar el historial de lanzamientos. Para simular el lanzamiento de una moneda se puede usar la función sample y definiendo el vector resultados con size=1 para simular un lanzamiento, a continuación el código y tres pruebas ilustrativas. resultados &lt;- c(&#39;Cara&#39;, &#39;Cruz&#39;) sample(x=resultados, size=1) # Prueba 1 ## [1] &quot;Cruz&quot; Una vez seamos capaces de simular un lanzamiento podemos escribir el procedimiento para generar tantos lanzamientos hasta que se cumpla la condición. El código mostrado abajo permite hacer lo solicitado. num.lanza &lt;- 0 # Contador de lanzamientos num.caras &lt;- 0 # Contados de caras obtenidas historial &lt;- NULL # Vector vacío para almacenar while (num.caras &lt; 5) { res &lt;- sample(x=resultados, size=1) num.lanza &lt;- num.lanza + 1 historial[num.lanza] &lt;- res if (res == &#39;Cara&#39;) { num.caras &lt;- num.caras + 1 } } historial ## [1] &quot;Cruz&quot; &quot;Cruz&quot; &quot;Cruz&quot; &quot;Cruz&quot; &quot;Cara&quot; &quot;Cara&quot; &quot;Cruz&quot; &quot;Cruz&quot; &quot;Cara&quot; &quot;Cara&quot; ## [11] &quot;Cara&quot; num.lanza ## [1] 11 La instrucción for se usa cuando sabemos el número de veces que se debe repetir el procedimiento, mientras que la instrucción while se usa cuando debemos repetir un procedimiento cuando se cumpla una condición. 2.6.6 Instrucción repeat La instrucción while es muy útil para repetir un procedimiento siempre que se cumple una condición. A continuación se muestra la estructura básica de uso. repeat { operación 1 operación 2 ... operación final if (condición) break } Ejemplo Escribir un procedimiento para ir aumentando de uno en uno el valor de x hasta que x sea igual a siete El procedimiento debe imprimir por pantalla la secuencia de valores de x. x &lt;- 3 # Valor de inicio repeat { print(x) x &lt;- x + 1 if (x == 8) { break } } ## [1] 3 ## [1] 4 ## [1] 5 ## [1] 6 ## [1] 7 La instrucción break sirve para salir de un procedimiento iterativo. 2.7 Guía de estilo Así como en el español existen reglas ortográficas, la escritura de códigos en R también tiene unas reglas que se recomienda seguir para evitar confusiones. Tener una buena guía de estilo es importante para que el código creado por usted sea fácilmente entendido por sus lectores (rpackages?). No existe una única y mejor guía de estilo para escritura en R, sin embargo aquí vamos a mostrar unas sugerencias basadas en la guía llamada The tidyverse style guidee. 2.7.1 Nombres de los archivos Se sugiere que el nombre usado para nombrar un archivo tenga sentido y que termine con extensión “.R”. A continuación dos ejemplos de como nombrar bien y mal un archivo. Bien: \"2020-analisis_exploratorio.R Mal: ju89HR56_74.R 2.7.2 Nombres de los objetos Se recomienda usar los símbolos _ dentro de los nombres de objetos. Para las variables es preferible usar letras minúsculas (pesomaiz o peso_maiz) o utilizar la notación camello iniciando en minúscula (pesoMaiz). Para las funciones se recomienda usar la notación camello iniciando todas la palabras en mayúscula (PlotRes). Para los nombres de las constantes se recomienda que inicien con la letra k (kPrecioBus). 2.7.3 Longitud de una línea de código Se recomienda que cada línea tenga como máximo 80 caracteres. Si una línea es muy larga se debe cortar siempre por una coma. 2.7.4 Espacios Use espacios alrededor de todos los operadores binarios (=, +, -, &lt;-, etc.). Los espacios alrededor del símbolo = son opcionales cuando se usan para ingresar valores dentro de una función. Así como en español, nunca coloque espacio antes de una coma, pero siempre use espacio luego de una coma. A continuación ejemplos de buenas y malas prácticas. tab &lt;- table(df[df$days &lt; 0, 2]) # Bien tot &lt;- sum(x[, 1]) # Bien tot &lt;- sum(x[1, ]) # Bien tab &lt;- table(df[df$days&lt;0, 2]) # Faltan espacios alrededor &#39;&lt;&#39; tab &lt;- table(df[df$days &lt; 0,2]) # Falta espacio luego de coma tab &lt;- table(df[df$days &lt; 0 , 2]) # Sobra espacio antes de coma tab&lt;- table(df[df$days &lt; 0, 2]) # Falta espacio antes de &#39;&lt;-&#39; tab&lt;-table(df[df$days &lt; 0, 2]) # Falta espacio alrededor de &#39;&lt;-&#39; tot &lt;- sum(x[,1]) # Falta espacio luego de coma tot &lt;- sum(x[1,]) # Falta espacio luego de coma Otra buena práctica es colocar espacio antes de un paréntesis excepto cuando se llama una función. if (debug) # Correcto if(debug) # Funciona pero no se recomienda colMeans (x) # Funciona pero no se recomienda Espacios extras pueden ser usados si con esto se mejora la apariencia del código, ver el ejemplo siguiente. plot(x = x.coord, y = data.mat[, MakeColName(metric, ptiles[1], &quot;roiOpt&quot;)], ylim = ylim, xlab = &quot;dates&quot;, ylab = metric, main = (paste(metric, &quot; for 3 samples &quot;, sep = &quot;&quot;))) No coloque espacios alrededor del código que esté dentro de paréntesis ( ) o corchetes [ ], la única excepción es luego de una coma, ver el ejemplo siguiente. if (condicion) # Correcto x[1, ] # Correcto if ( condicion ) # Sobran espacios alrededor de condición x[1,] # Se necesita espacio luego de coma Los signos de agrupación llaves { } se utilizan para agrupar bloques de código y se recomienda que nunca una llave abierta { esté sola en una línea; una llave cerrada } si debe ir sola en su propia línea. Se pueden omitir las llaves cuando el bloque de instrucciones esté formado por una sola línea pero esa línea de código NO debe ir en la misma línea de la condición. A continuación dos ejemplos de lo que se recomienda. if (is.null(ylim)) { # Correcto ylim &lt;- c(0, 0.06) } if (is.null(ylim)) # Correcto ylim &lt;- c(0, 0.06) if (is.null(ylim)) ylim &lt;- c(0, 0.06) # Aceptable if (is.null(ylim)) # No se recomienda { ylim &lt;- c(0, 0.06) } if (is.null(ylim)) {ylim &lt;- c(0, 0.06)} # Frente a la llave { no debe ir nada # la llave de cierre } debe ir sola La sentencia else debe ir siempre entre llaves } {, ver el siguiente ejemplo. if (condition) { one or more lines } else { # Correcto one or more lines } if (condition) { one or more lines } else { # Incorrecto one or more lines } if (condition) one line else # Incorrecto one line 2.7.5 Asignación Para realizar asignaciones se recomienda usar el símbolo &lt;-, el símbolo de igualdad = no se recomienda usarlo para asignaciones. x &lt;- 5 # Correcto x = 5 # No recomendado Para una explicación más detallada sobre el símbolo de asignación se recomienda visitar este enlace. 2.7.6 Punto y coma No se recomienda colocar varias instrucciones separadas por ; en la misma línea, aunque funciona dificulta la revisión del código. n &lt;- 100; y &lt;- rnorm(n, mean=5); hist(y) # No se recomienda n &lt;- 100 # Correcto y &lt;- rnorm(n, mean=5) hist(y) A pesar de la anterior advertencia es posible que en este libro usemos el ; en algunas ocasiones, si lo hacemos es para ahorrar espacio en la presentación del código. "],["lectura-de-datos.html", "Capítulo 3 Lectura de datos 3.1 Archivos csv 3.2 Archivos txt 3.3 Archivos xls y xlsx 3.4 Archivos json 3.5 Bases de Datos", " Capítulo 3 Lectura de datos Usualmente, no creamos los datos desde la sesión de R, sino que a través de un archivo externo o una base de datos se realiza la lectura de datos. Los más comunes son: 3.1 Archivos csv A la hora de importar conjuntos de datos en R, uno de los formatos más habituales en los que hallamos información es en archivos separados por comas (comma separated values), cuya extensión suele ser .csv. En ellos encontramos múltiples líneas que recogen la tabla de interés, y en las cuales los valores aparecen, de manera consecutiva, separados por el carácter ,. Para importar este tipo de archivos en nuestra sesión de R, se utiliza la función read_csv(). Para acceder a su documentación utilizamos el comando ?read_csv. El único argumento que debemos de pasar a esta función de manera obligatoria, es file, el nombre o la ruta completa del archivo que pretendemos importar. library(readr) read_csv( file, col_names = TRUE, col_types = NULL, locale = default_locale(), na = c(&quot;&quot;, &quot;NA&quot;), quoted_na = TRUE, quote = &quot;\\&quot;&quot;, comment = &quot;&quot;) La paquetería readr fue desarrollada recientemente para lidiar con la lectura de archivos grandes rápidamente. El paquete proporciona reemplazos para funciones como read.table(), read.csv() entre otras. Esta paquetería proporciona funciones que suelen ser mucho más rápidas que las funciones base que proporciona R. Ventajas de readr: Por lo general, son mucho más rápidos (~ 10x) que sus funciones equivalentes. Producen tibbles: No convierten vectores de caracteres en factores. No usan nombres de filas ni modifican los nombres de columnas. Reproducibilidad No convierte, automáticamente, las columnas con cadenas de caracteres a factores, como sí hacen por defecto las otras funciones base de R. Reconoce ocho clases diferentes de datos (enteros, lógicos, etc.), dejando el resto como cadenas de caracteres. Veamos un ejemplo: La base de datos llamada AmesHousing contiene un conjunto de datos con información de la Oficina del Tasador de Ames utilizada para calcular los valores tasados para las propiedades residenciales individuales vendidas en Ames, Iowa, de 2006 a 2010. FUENTES: Ames, Oficina del Tasador de Iowa. Pueden descargar los datos para la clase aquí base &lt;- read.csv(&quot;data/ames.csv&quot;) head(base, 2) ## MS_SubClass MS_Zoning Lot_Frontage ## 1 One_Story_1946_and_Newer_All_Styles Residential_Low_Density 141 ## 2 One_Story_1946_and_Newer_All_Styles Residential_High_Density 80 ## Lot_Area Street Alley Lot_Shape Land_Contour Utilities ## 1 31770 Pave No_Alley_Access Slightly_Irregular Lvl AllPub ## 2 11622 Pave No_Alley_Access Regular Lvl AllPub ## Lot_Config Land_Slope Neighborhood Condition_1 Condition_2 Bldg_Type ## 1 Corner Gtl North_Ames Norm Norm OneFam ## 2 Inside Gtl North_Ames Feedr Norm OneFam ## House_Style Overall_Cond Year_Built Year_Remod_Add Roof_Style Roof_Matl ## 1 One_Story Average 1960 1960 Hip CompShg ## 2 One_Story Above_Average 1961 1961 Gable CompShg ## Exterior_1st Exterior_2nd Mas_Vnr_Type Mas_Vnr_Area Exter_Cond Foundation ## 1 BrkFace Plywood Stone 112 Typical CBlock ## 2 VinylSd VinylSd None 0 Typical CBlock ## Bsmt_Cond Bsmt_Exposure BsmtFin_Type_1 BsmtFin_SF_1 BsmtFin_Type_2 ## 1 Good Gd BLQ 2 Unf ## 2 Typical No Rec 6 LwQ ## BsmtFin_SF_2 Bsmt_Unf_SF Total_Bsmt_SF Heating Heating_QC Central_Air ## 1 0 441 1080 GasA Fair Y ## 2 144 270 882 GasA Typical Y ## Electrical First_Flr_SF Second_Flr_SF Gr_Liv_Area Bsmt_Full_Bath ## 1 SBrkr 1656 0 1656 1 ## 2 SBrkr 896 0 896 0 ## Bsmt_Half_Bath Full_Bath Half_Bath Bedroom_AbvGr Kitchen_AbvGr TotRms_AbvGrd ## 1 0 1 0 3 1 7 ## 2 0 1 0 2 1 5 ## Functional Fireplaces Garage_Type Garage_Finish Garage_Cars Garage_Area ## 1 Typ 2 Attchd Fin 2 528 ## 2 Typ 0 Attchd Unf 1 730 ## Garage_Cond Paved_Drive Wood_Deck_SF Open_Porch_SF Enclosed_Porch ## 1 Typical Partial_Pavement 210 62 0 ## 2 Typical Paved 140 0 0 ## Three_season_porch Screen_Porch Pool_Area Pool_QC Fence ## 1 0 0 0 No_Pool No_Fence ## 2 0 120 0 No_Pool Minimum_Privacy ## Misc_Feature Misc_Val Mo_Sold Year_Sold Sale_Type Sale_Condition Sale_Price ## 1 None 0 5 2010 WD Normal 215000 ## 2 None 0 6 2010 WD Normal 105000 ## Longitude Latitude ## 1 -93.61975 42.05403 ## 2 -93.61976 42.05301 tidy &lt;- read_csv(&quot;data/ames.csv&quot;) ## Rows: 2930 Columns: 74 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (40): MS_SubClass, MS_Zoning, Street, Alley, Lot_Shape, Land_Contour, Ut... ## dbl (34): Lot_Frontage, Lot_Area, Year_Built, Year_Remod_Add, Mas_Vnr_Area, ... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. head(tidy, 2) ## # A tibble: 2 × 74 ## MS_SubC…¹ MS_Zo…² Lot_F…³ Lot_A…⁴ Street Alley Lot_S…⁵ Land_…⁶ Utili…⁷ Lot_C…⁸ ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 One_Stor… Reside… 141 31770 Pave No_A… Slight… Lvl AllPub Corner ## 2 One_Stor… Reside… 80 11622 Pave No_A… Regular Lvl AllPub Inside ## # … with 64 more variables: Land_Slope &lt;chr&gt;, Neighborhood &lt;chr&gt;, ## # Condition_1 &lt;chr&gt;, Condition_2 &lt;chr&gt;, Bldg_Type &lt;chr&gt;, House_Style &lt;chr&gt;, ## # Overall_Cond &lt;chr&gt;, Year_Built &lt;dbl&gt;, Year_Remod_Add &lt;dbl&gt;, ## # Roof_Style &lt;chr&gt;, Roof_Matl &lt;chr&gt;, Exterior_1st &lt;chr&gt;, Exterior_2nd &lt;chr&gt;, ## # Mas_Vnr_Type &lt;chr&gt;, Mas_Vnr_Area &lt;dbl&gt;, Exter_Cond &lt;chr&gt;, Foundation &lt;chr&gt;, ## # Bsmt_Cond &lt;chr&gt;, Bsmt_Exposure &lt;chr&gt;, BsmtFin_Type_1 &lt;chr&gt;, ## # BsmtFin_SF_1 &lt;dbl&gt;, BsmtFin_Type_2 &lt;chr&gt;, BsmtFin_SF_2 &lt;dbl&gt;, … ## # ℹ Use `colnames()` to see all variable names ¿Y si el archivo que necesitamos leer esta en excel? 3.2 Archivos txt Uno de los archivos más comunes es el .txt. La librería readr también cuenta con funciones que permiten leer fácilmente los datos contenidos en formato tabular. ames_txt &lt;- read_delim(&quot;data/ames.txt&quot;, delim = &quot;;&quot;, col_names = TRUE) ## Rows: 2930 Columns: 74 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;;&quot; ## chr (40): MS_SubClass, MS_Zoning, Street, Alley, Lot_Shape, Land_Contour, Ut... ## dbl (34): Lot_Frontage, Lot_Area, Year_Built, Year_Remod_Add, Mas_Vnr_Area, ... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. head(ames_txt, 2) ## # A tibble: 2 × 74 ## MS_SubC…¹ MS_Zo…² Lot_F…³ Lot_A…⁴ Street Alley Lot_S…⁵ Land_…⁶ Utili…⁷ Lot_C…⁸ ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 One_Stor… Reside… 141 31770 Pave No_A… Slight… Lvl AllPub Corner ## 2 One_Stor… Reside… 80 11622 Pave No_A… Regular Lvl AllPub Inside ## # … with 64 more variables: Land_Slope &lt;chr&gt;, Neighborhood &lt;chr&gt;, ## # Condition_1 &lt;chr&gt;, Condition_2 &lt;chr&gt;, Bldg_Type &lt;chr&gt;, House_Style &lt;chr&gt;, ## # Overall_Cond &lt;chr&gt;, Year_Built &lt;dbl&gt;, Year_Remod_Add &lt;dbl&gt;, ## # Roof_Style &lt;chr&gt;, Roof_Matl &lt;chr&gt;, Exterior_1st &lt;chr&gt;, Exterior_2nd &lt;chr&gt;, ## # Mas_Vnr_Type &lt;chr&gt;, Mas_Vnr_Area &lt;dbl&gt;, Exter_Cond &lt;chr&gt;, Foundation &lt;chr&gt;, ## # Bsmt_Cond &lt;chr&gt;, Bsmt_Exposure &lt;chr&gt;, BsmtFin_Type_1 &lt;chr&gt;, ## # BsmtFin_SF_1 &lt;dbl&gt;, BsmtFin_Type_2 &lt;chr&gt;, BsmtFin_SF_2 &lt;dbl&gt;, … ## # ℹ Use `colnames()` to see all variable names La función read_delim() funciona para leer archivos con diferentes delimitadores posibles, es decir, es posible especificar si las columnas están separadas por espacios, comas, punto y coma, tabulador o algún otro delimitador (““,”,“,”;“,”, “@”). Adicionalmente, se puede especificar si el archivo contiene encabezado, si existen renglones a saltar, codificación, tipo de variable y muchas más opciones. Todos estos detalles pueden consultarse en la documentación de ayuda. 3.3 Archivos xls y xlsx La paquetería readxl facilita la obtención de datos tabulares de archivos de Excel. Admite tanto el formato .xls heredado como el formato .xlsx moderno basado en XML. Esta paquetería pone a disposición las siguientes funciones: read_xlsx() lee un archivo con extensión xlsx. read_xlsx( path, sheet = NULL, range = NULL, col_names = TRUE, col_types = NULL, na = &quot;&quot;, trim_ws = TRUE, skip = 0, n_max = Inf, guess_max = min(1000, n_max), progress = readxl_progress(), .name_repair = &quot;unique&quot; ) read_xls() lee un archivo con extensión xls. read_xls( path, sheet = NULL, range = NULL, col_names = TRUE, col_types = NULL, na = &quot;&quot;, trim_ws = TRUE, skip = 0, n_max = Inf, guess_max = min(1000, n_max), progress = readxl_progress(), .name_repair = &quot;unique&quot; ) read_excel() determina si el archivo es de tipo xls o xlsx para después llamar a una de las funciones mencionadas anteriormente. read_excel( path, sheet = NULL, range = NULL, col_names = TRUE, col_types = NULL, na = &quot;&quot;, trim_ws = TRUE, skip = 0, n_max = Inf, guess_max = min(1000, n_max), progress = readxl_progress(), .name_repair = &quot;unique&quot; ) EJERCICIO: Leer archivo excel de la carpeta del curso 3.4 Archivos json Se utiliza la función fromJSON de la paquetería jsonlite library(jsonlite) base_json &lt;- jsonlite::fromJSON(&quot;data/ames.json&quot;) head(base_json, 2) ## MS_SubClass MS_Zoning Lot_Frontage ## 1 One_Story_1946_and_Newer_All_Styles Residential_Low_Density 141 ## 2 One_Story_1946_and_Newer_All_Styles Residential_High_Density 80 ## Lot_Area Street Alley Lot_Shape Land_Contour Utilities ## 1 31770 Pave No_Alley_Access Slightly_Irregular Lvl AllPub ## 2 11622 Pave No_Alley_Access Regular Lvl AllPub ## Lot_Config Land_Slope Neighborhood Condition_1 Condition_2 Bldg_Type ## 1 Corner Gtl North_Ames Norm Norm OneFam ## 2 Inside Gtl North_Ames Feedr Norm OneFam ## House_Style Overall_Cond Year_Built Year_Remod_Add Roof_Style Roof_Matl ## 1 One_Story Average 1960 1960 Hip CompShg ## 2 One_Story Above_Average 1961 1961 Gable CompShg ## Exterior_1st Exterior_2nd Mas_Vnr_Type Mas_Vnr_Area Exter_Cond Foundation ## 1 BrkFace Plywood Stone 112 Typical CBlock ## 2 VinylSd VinylSd None 0 Typical CBlock ## Bsmt_Cond Bsmt_Exposure BsmtFin_Type_1 BsmtFin_SF_1 BsmtFin_Type_2 ## 1 Good Gd BLQ 2 Unf ## 2 Typical No Rec 6 LwQ ## BsmtFin_SF_2 Bsmt_Unf_SF Total_Bsmt_SF Heating Heating_QC Central_Air ## 1 0 441 1080 GasA Fair Y ## 2 144 270 882 GasA Typical Y ## Electrical First_Flr_SF Second_Flr_SF Gr_Liv_Area Bsmt_Full_Bath ## 1 SBrkr 1656 0 1656 1 ## 2 SBrkr 896 0 896 0 ## Bsmt_Half_Bath Full_Bath Half_Bath Bedroom_AbvGr Kitchen_AbvGr TotRms_AbvGrd ## 1 0 1 0 3 1 7 ## 2 0 1 0 2 1 5 ## Functional Fireplaces Garage_Type Garage_Finish Garage_Cars Garage_Area ## 1 Typ 2 Attchd Fin 2 528 ## 2 Typ 0 Attchd Unf 1 730 ## Garage_Cond Paved_Drive Wood_Deck_SF Open_Porch_SF Enclosed_Porch ## 1 Typical Partial_Pavement 210 62 0 ## 2 Typical Paved 140 0 0 ## Three_season_porch Screen_Porch Pool_Area Pool_QC Fence ## 1 0 0 0 No_Pool No_Fence ## 2 0 120 0 No_Pool Minimum_Privacy ## Misc_Feature Misc_Val Mo_Sold Year_Sold Sale_Type Sale_Condition Sale_Price ## 1 None 0 5 2010 WD Normal 215000 ## 2 None 0 6 2010 WD Normal 105000 ## Longitude Latitude ## 1 -93.6198 42.054 ## 2 -93.6198 42.053 3.5 Bases de Datos En muchos de los casos la información estará dentro de un Sistema Manejador de Bases de Datos, existen bibliotecas que nos permiten establecer las conexiones con ellas, algunos ejemplos son: ODBC DBI Un ejemplo con un SMBD como SQL Server: 3.5.1 Microsoft SQL Server Referencias Configuración de conexión: Se necesitan seis configuraciones para realizar una conexión: Controlador : consulte la sección Controladores para obtener más información Servidor : una ruta de red al servidor de la base de datos. Base de datos : el nombre de la base de datos. UID : el ID de red del usuario o la cuenta local del servidor PWD : la contraseña de la cuenta Puerto : debe establecerse en 1433 Para establecer la conexión con la base de datos: library(DBI) con &lt;- DBI::dbConnect( odbc::odbc(), Driver = &quot;[your driver&#39;s name]&quot;, Server = &quot;[your server&#39;s path]&quot;, Database = &quot;[your database&#39;s name]&quot;, UID = rstudioapi::askForPassword(&quot;Database user&quot;), PWD = rstudioapi::askForPassword(&quot;Database password&quot;), Port = 1433 ) Información sobre la base de datos: El paquete odbc le brinda herramientas para explorar objetos y columnas en la base de datos. # Top level objects odbcListObjects(con) # Tables in a schema odbcListObjects(con, catalog = &quot;mydb&quot;, schema = &quot;dbo&quot;) # Columns in a table odbcListColumns(con, catalog = &quot;mydb&quot;, schema = &quot;dbo&quot;, table = &quot;cars&quot;) # Database structure odbcListObjectTypes(con) Consultas con SQL: Para consultas interactivas, utilice dbGetQuery() para enviar una consulta y obtener los resultados. Para obtener los resultados por separado, utilice dbSendQuery() y dbFetch(). El argumento n en dbFetch() se puede utilizar para obtener resultados parciales. # Return the results for an arbitrary query dbGetQuery(con, &quot;SELECT speed, dist FROM cars&quot;) # Fetch the first 100 records query &lt;- dbSendQuery(con, &quot;SELECT speed, dist FROM cars&quot;) dbFetch(query, n = 10) dbClearResult(query) Puedes usar los ejemplos anteriores para probar con diferentes consultas y bases de datos. Tengamos un ejemplo de manera local: remotes::install_version(&quot;RSQLite&quot;) library(dplyr) library(dbplyr) library(RSQLite) con &lt;- src_memdb() copy_to(con, storms, overwrite = T) tbl_storms &lt;- tbl(con, &quot;storms&quot;) tbl_storms ## # Source: table&lt;storms&gt; [?? x 13] ## # Database: sqlite 3.39.1 [:memory:] ## name year month day hour lat long status categ…¹ wind press…² ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 Amy 1975 6 27 0 27.5 -79 tropical dep… -1 25 1013 ## 2 Amy 1975 6 27 6 28.5 -79 tropical dep… -1 25 1013 ## 3 Amy 1975 6 27 12 29.5 -79 tropical dep… -1 25 1013 ## 4 Amy 1975 6 27 18 30.5 -79 tropical dep… -1 25 1013 ## 5 Amy 1975 6 28 0 31.5 -78.8 tropical dep… -1 25 1012 ## 6 Amy 1975 6 28 6 32.4 -78.7 tropical dep… -1 25 1012 ## 7 Amy 1975 6 28 12 33.3 -78 tropical dep… -1 25 1011 ## 8 Amy 1975 6 28 18 34 -77 tropical dep… -1 30 1006 ## 9 Amy 1975 6 29 0 34.4 -75.8 tropical sto… 0 35 1004 ## 10 Amy 1975 6 29 6 34 -74.8 tropical sto… 0 40 1002 ## # … with more rows, 2 more variables: tropicalstorm_force_diameter &lt;int&gt;, ## # hurricane_force_diameter &lt;int&gt;, and abbreviated variable names ¹​category, ## # ²​pressure ## # ℹ Use `print(n = ...)` to see more rows, and `colnames()` to see all variable names "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
